
David Baines
The ebible.py script doesn't keep track of the passage of each translation as well as it could.
I wonder what methods would be good for improving the storing and making use of infomation about the actions of the script, so that progress can be stored in order to avoid repeating actions unnecessarily.

We may download and read in the translations.csv file that is stored in the ebible_data/metadata folder. It could be good to hold that in memory and add to each translation information such as: Name of the downloaded file and date stamp for downloading. Name of the unzipped project folder and date stamp of unzipping. Name of the extracted text file which will be <isocode>-<project-folder.name> and the date stamp of extracting - which will be the last modified date of the extracted text file.

This could all be saved as an expanded ebible_translations.csv which would be read each time the code runs so that previously done actions don't need to be done again unless the --force-download flag is set. Or the --max-zip-age-days is exceeded.

Don't write code at this point. Please just discuss ways of doing something like this. First list a few potential solutions with their pros and cons. the folder name where it has been unzipped and the unzipp


Gemini
Okay, let's explore some ways to improve the progress tracking in the ebible.py script. The goal is to store the state of each translation (downloaded, unzipped, extracted) persistently, so the script can efficiently pick up where it left off and avoid redundant work, while still respecting options like --force_download and --max_zip_age_days.

Here are a few potential approaches:

1. Enhanced CSV Status File (Your Suggestion)

Description:
Maintain a separate CSV file (e.g., metadata/ebible_status.csv).
This file would store the translationId along with columns for:
download_path (path to the specific zip file used)
download_timestamp (when it was downloaded or verified as recent)
unzip_path (path to the project folder)
unzip_timestamp (when it was unzipped)
extract_path (path to the final corpus text file)
extract_timestamp (last modified time of the corpus file)
At the start, the script reads translations.csv (the source list) and ebible_status.csv (the current state).
For each translation, it compares the status information against the required steps and flags (--force_download, --max_zip_age_days).
As each step (download, unzip, extract) completes successfully for a translation, the script updates the corresponding information in memory.
The updated status data is written back to ebible_status.csv at the end of the run.
Pros:
Simplicity: Relatively straightforward to implement using existing libraries like csv or pandas.
Readability: The status file is human-readable and can be easily inspected or manually edited if needed.
Persistence: Clearly stores state between script runs.
Alignment: Fits well with the script's current use of CSV data.
Cons:
File I/O: Reading and writing the entire status CSV on every run might become slightly inefficient if the number of translations grows extremely large.
Concurrency: Not inherently safe if multiple instances of the script were run simultaneously against the same status file (though this seems unlikely for this use case).
Merging Logic: Requires logic to handle discrepancies between translations.csv (source) and ebible_status.csv (state) â€“ e.g., new translations added upstream, or translations removed.
2. SQLite Database

Description:
Use a single SQLite database file (e.g., metadata/ebible_status.db) to store the status.
Create a table (e.g., translation_status) with columns similar to the CSV approach (translationId, language_code, is_redistributable, download_path, download_timestamp, unzip_path, unzip_timestamp, extract_path, extract_timestamp).
The script connects to the database at the start.
It queries the database for the status of each translation it intends to process.
Updates are performed using SQL UPDATE statements after each successful step.
New translations found in translations.csv but not in the database are added with an initial status.
Pros:
Robustness: Databases are designed for efficient querying and atomic updates, reducing risks of data corruption compared to rewriting entire files.
Querying: Easier to perform more complex queries on the status data if needed (e.g., "show all translations downloaded but not unzipped").
Scalability: Generally handles large amounts of data more efficiently than reading/writing large flat files.
Built-in Support: Python's standard library includes the sqlite3 module.
Cons:
Complexity: Introduces database concepts (SQL, schema management), slightly increasing the learning curve and code complexity compared to CSV.
Readability: The status data isn't as easily human-readable/editable outside the script without a database tool.
Overhead: Might be slight overkill if the number of translations remains manageable and complex queries aren't needed.
3. Filesystem Markers

Description:
Rely more heavily on the existence and modification times of the actual files and directories produced by each step.
Download: Check for downloads/{id}--YYYY-MM-DD.zip files and their dates (already partially done).
Unzip: Check for the existence of the target project directory (projects/{proj_name} or private_projects/{proj_name}). Potentially add an empty marker file (e.g., .unzipped_ok) inside the directory upon successful completion to distinguish from a partial unzip.
Extract: Check for the existence and modification time of the final corpus file (corpus/{lang}-{proj_name}.txt or private_corpus/...).
The script would perform these filesystem checks for each translation to determine the next required step.
Pros:
Implicit State: The state is directly tied to the artifacts themselves; no separate status file to manage or potentially get out of sync.
Minimal Overhead: Avoids reading/writing a dedicated status file/database. Leverages filesystem operations which are often fast.
Cons:
Less Explicit: Harder to get a quick overview of the status of all translations. Querying specific states (e.g., "downloaded but not unzipped") becomes more complex, requiring iterating through directories.
Ambiguity: Simple directory existence might not guarantee a successful prior step (e.g., unzip might have failed halfway). Marker files help but add clutter.
Timestamp Reliance: Relying solely on modification times can sometimes be fragile.
Tracking Specifics: Doesn't easily track which specific downloaded zip file corresponds to the current unzipped version if multiple downloads exist for the same ID.
Discussion:

The Enhanced CSV Status File seems like the most balanced approach for this scenario. It directly addresses the need for explicit tracking of each step's completion and associated metadata (paths, timestamps) while leveraging the existing familiarity with CSV handling in the script. It provides good readability and persistence with manageable complexity. The key challenge is implementing the logic to correctly read the status, compare it with requirements/flags, perform actions, and reliably update and save the status. Using a separate ebible_status.csv avoids modifying the downloaded translations.csv and simplifies handling updates.

The SQLite approach is a strong runner-up, offering more robustness and scalability, but might add unnecessary complexity unless the tracking requirements become significantly more intricate.

The Filesystem Markers approach is the least explicit and potentially the most fragile, making it harder to reliably determine the exact state and debug issues.

Therefore, proceeding with the idea of a separate ebible_status.csv file appears to be a practical and effective way forward.

Rate this answer: 
Context Sources (1)

David Baines
Would you propose a solution based on the "Enhanced CSV Status File" system. Please ensure that the status file includes all the data from the translations.csv file. It should maintain all the columns in the translations.csv file: "languageCode","translationId","languageName","languageNameInEnglish","dialect","homeDomain","title","description","Redistributable","Copyright","UpdateDate","publicationURL","OTbooks","OTchapters","OTverses","NTbooks","NTchapters","NTverses","DCbooks","DCchapters","DCverses","FCBHID","Certified","inScript","swordName","rodCode","textDirection","downloadable","font","shortTitle","PODISBN","script","sourceDate" Any new columns that it adds should have unique names that don't use these names. Please also include columns for keeping track of the licence data. The data from the licence files should be added to the ebible_status.csv file along with a data stamp for when that data was read from the licence file and added to the ebible_status.csv.

Let's rename the --max_zip_age_days argument to --max-age-days and use that one value to control what is considered out of date and what is considered up to date. Maybe storing only the date of the last update is sufficient since we don't have a use for the time. Are there pros and cons to putting that in the .env file?

We should prefix the licence column names with "licence_" so that it is clear where that information came from. This is important because these columns sometimes have very similar names to the translation.csv columns but the data is not always consistent. This will help a human to see where the licence data is inconsistent with the translations.csv data. The licence column names would be: "licence_ID", "licence_File", "licence_Language", "licence_Dialect", "licence_Vernacular Title", "licence_Licence Type", "licence_Licence Version", "licence_CC Licence Link", "licence_Copyright Holder", "licence_Copyright Years", "licence_Translation by" And the date of reading the data: "licence_date_read"

How does this sound and are there things that could improve it?

ebible.py: 13 lines selected

Gemini
Okay, let's flesh out the "Enhanced CSV Status File" approach based on your suggestions. This sounds like a solid plan to make the script more robust and efficient.

Here's a breakdown of how it could work:

1. The ebible_status.csv File

Location: metadata/ebible_status.csv (relative to the base folder).
Purpose: This file becomes the central hub for tracking the state of each translation processed by the script. It persists information between runs.
Initialization:
On the first run (or if metadata/ebible_status.csv is missing), the script reads metadata/translations.csv.
It creates ebible_status.csv by copying all columns and rows from translations.csv.
It then adds the new status and licence columns, initially populated with empty values or appropriate defaults.
Subsequent Runs:
The script reads the existing ebible_status.csv into memory (likely using pandas).
It also reads metadata/translations.csv to check for upstream changes (new translations added, maybe metadata updates, although handling metadata updates might be complex).
It merges information: New translations from translations.csv are added as new rows to the status DataFrame. Existing rows are potentially updated if key metadata from translations.csv changes (this needs careful consideration â€“ maybe only add new ones and leave existing ones?).
Columns: The file would contain:
All original columns from translations.csv: languageCode, translationId, languageName, languageNameInEnglish, dialect, homeDomain, title, description, Redistributable, Copyright, UpdateDate, publicationURL, OTbooks, OTchapters, OTverses, NTbooks, NTchapters, NTverses, DCbooks, DCchapters, DCverses, FCBHID, Certified, inScript, swordName, rodCode, textDirection, downloadable, font, shortTitle, PODISBN, script, sourceDate
New Status Tracking Columns:
status_download_path: (String) Full path to the specific zip file used.
status_download_date: (String, YYYY-MM-DD) Date the zip was downloaded or last verified.
status_unzip_path: (String) Full path to the unzipped project folder.
status_unzip_date: (String, YYYY-MM-DD) Date the project folder was successfully unzipped/created.
status_extract_path: (String) Expected full path to the final corpus text file.
status_extract_date: (String, YYYY-MM-DD) Last modified date of the corpus text file (if it exists). Note: Since extraction is external, this reflects when the script last checked the file.
New Licence Tracking Columns (Prefixed):
licence_ID: (String) The ID found in the licence file (often the project folder name).
licence_File: (String) Path to the copr.htm file checked.
licence_Language: (String) Language from the licence file.
licence_Dialect: (String) Dialect from the licence file.
licence_Vernacular_Title: (String) Vernacular Title from the licence file. (Underscore used for easier column naming).
licence_Licence_Type: (String) Licence Type from the licence file.
licence_Licence_Version: (String) Licence Version from the licence file.
licence_CC_Licence_Link: (String) CC Licence Link from the licence file.
licence_Copyright_Holder: (String) Copyright Holder from the licence file.
licence_Copyright_Years: (String) Copyright Years from the licence file.
licence_Translation_by: (String) Translation by info from the licence file.
licence_date_read: (String, YYYY-MM-DD) Date the licence information was extracted and added/updated in this status file.
2. Workflow Integration

Initialization: Read/create/update ebible_status.csv as described above.
Filtering: Apply the --filter to the status DataFrame.
Determine Actions: Iterate through the rows (translations) in the status DataFrame. For each translation:
Check Download:
If --force_download is set, mark for download.
Else, check status_download_date. If it's missing or older than --max-age-days, mark for download.
Else, verify status_download_path still exists. If not, mark for download. Otherwise, skip download.
Check Unzip:
If marked for download (implying a new zip will be used), mark for unzip.
If --force_download is set, mark for unzip.
Else, check status_unzip_date. If it's missing or older than --max-age-days, mark for unzip.
Else, verify status_unzip_path still exists. If not, mark for unzip. Otherwise, skip unzip.
Check Licence:
If marked for unzip (implying project files might change), mark for licence check.
If --force_download is set, mark for licence check.
Else, check licence_date_read. If it's missing or older than --max-age-days, mark for licence check. Otherwise, skip licence check.
(Check Extract: This step remains manual, but the script could potentially check if status_extract_path exists and update status_extract_date if the user wants that info.)
Perform Actions:
Download: Perform downloads for marked translations. Update status_download_path and status_download_date in the DataFrame upon success.
Unzip: Perform unzipping for marked translations. Update status_unzip_path and status_unzip_date. Run rename_usfm and write_settings_file as part of this step.
Licence Extraction: Modify get_licence_details to take the status DataFrame and the specific project_dir as input. It should find the relevant row in the DataFrame and update the licence_* columns and licence_date_read directly. The separate licences.tsv file and write_licence_file function become obsolete.
Save Status: Write the entire updated status DataFrame back to metadata/ebible_status.csv at the end of the script run.
3. Argument Renaming: --max-age-days

Yes, renaming --max_zip_age_days to --max-age-days makes perfect sense. It generalizes the concept, as this single value will now control the freshness check for downloads, unzips, and licence data extraction. This simplifies configuration.
4. Date vs. Timestamp

Using just the date (YYYY-MM-DD) for status_download_date, status_unzip_date, and licence_date_read seems sufficient and simpler.
Pros: Easier to parse, compare, and less verbose in the CSV. Avoids timezone complexities if the script runs in different environments. The --max-age-days comparison works perfectly with just dates.
Cons: Loses granularity. If the script were run multiple times a day with --max-age-days=0, using only the date wouldn't distinguish between runs on the same day. However, given the nature of this script (downloading large files, processing potentially taking time), checking freshness based on days seems appropriate.
Recommendation: Stick with YYYY-MM-DD dates.
5. --max-age-days in .env File

Pros:
Configuration: Allows users to set a persistent default value without modifying the script or command-line arguments every time. Good for user-specific or project-specific settings.
Consistency: Ensures the same default max age is used unless overridden by the command line.
Separation of Concerns: Keeps configuration separate from code.
Cons:
Discovery: Users need to know to look in the .env file for this setting.
Complexity: Adds another place to manage configuration (though .env is already used).
Precedence: The script needs clear logic for precedence (e.g., command-line argument overrides .env value, which overrides a hardcoded default in the script).
Recommendation: It's a reasonable place to put the default value. The script should:
Look for the --max-age-days command-line argument.
If not provided, look for a MAX_AGE_DAYS variable in the .env file.
If not found there, use a hardcoded default (e.g., 365) within the script. This provides flexibility while having sensible fallbacks.
6. Licence Data Integration

Integrating the licence data directly into ebible_status.csv is a good improvement.
Prefixing: Using licence_ as a prefix is crucial, as you noted, to avoid ambiguity with columns from translations.csv (like Copyright, Language, Dialect). It clearly marks the data source.
Redundancy: This eliminates the need for the separate licences.tsv file and the write_licence_file function, simplifying the overall process and keeping all relevant state in one place.
Modification: get_licence_details will need refactoring. Instead of returning a list of dictionaries, it should accept the main status DataFrame and the path to the specific project being processed. It will locate the correct row in the DataFrame (based on translationId or project_path) and update the licence_* columns and licence_date_read for that row directly.
Overall Assessment & Potential Improvements

Soundness: This approach is robust and significantly improves the script's ability to manage state and avoid redundant work. Using pandas for CSV manipulation will handle the large number of columns efficiently.
Clarity: Storing all state information in one place makes it easier to understand the status of the entire process. The licence_ prefix adds necessary clarity.
Efficiency: Avoids unnecessary downloads, unzipping, and licence parsing based on configurable age limits.
Potential Improvements/Considerations:
Error Handling: How should errors during download, unzip, or licence parsing be recorded in the status file? Add a status_last_error column?
Upstream Metadata Changes: How strictly should changes in translations.csv (e.g., an updated Copyright field) trigger reprocessing or updates in ebible_status.csv? For simplicity, initially, maybe only focus on adding new translations found in translations.csv. Updating existing rows based on upstream changes could add significant complexity.
File Format: While CSV is good for readability, for a very large number of translations and columns, a more performant format like Parquet could eventually be considered, but CSV with pandas is likely fine for the foreseeable future.
Schema Versioning: If the structure of ebible_status.csv changes in the future (e.g., adding new status columns), how will older status files be handled? (Might be overkill for now).
Cleanup: The script doesn't currently clean up old zip files or project directories that are no longer needed (e.g., if a translation is removed upstream or if multiple dated zips exist for one ID). This status file could potentially facilitate such cleanup logic in the future.
This enhanced CSV approach looks like the right direction, providing a good balance of functionality, readability, and implementation complexity.

Rate this answer: 
Context Sources (1)

David Baines
Can you write a complete script that would use this system? Let's call it ebible_status.py then I can test the solution before renaming it as ebible.py once any bugs are resolved.

ebible.py: 13 lines selected

Gemini
Okay, let's create the ebible_status.py script incorporating the enhanced CSV status file system.

This script will:

Maintain an ebible_status.csv file in the metadata folder.
Initialize or update this status file based on translations.csv.
Use the status file to track download, unzip, and licence extraction dates.
Use the --max-age-days argument (with .env fallback) to determine when actions need re-running.
Integrate licence data directly into the status file, removing the need for licences.tsv.
Update relevant functions to work with the pandas DataFrame holding the status.
diff
-0
+826
The code change produced by Gemini could not be fully parsed. Can't automatically accept changes.
"""ebible_status.py contains functions for downloading and processing data from eBible.org.

This version uses a status file (ebible_status.csv) to track progress and avoid
re-running steps unnecessarily.

Workflow:
1. Read or initialize ebible_status.csv, merging info from translations.csv.
2. Filter translations based on command-line arguments.
3. Determine required actions (download, unzip, licence check) based on status
   dates, --max-age-days, and --force_download.
4. Perform downloads, updating status.
5. Perform unzipping, renaming, settings file creation, and licence extraction,
   updating status.
6. Save the updated ebible_status.csv.
7. Print commands for bulk extraction using SILNLP.
"""

import argparse
import os
import shutil
import sys
from csv import DictReader  # Keep for potential future use? Maybe remove.
from datetime import date, datetime, timedelta, timezone
from pathlib import Path
from random import randint
from time import sleep, strftime
from typing import Dict, List, Optional, Set

import numpy as np
import pandas as pd
import regex
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from rename_usfm import get_destination_file_from_book
from settings_file import get_vrs_diffs, write_settings_file
from tqdm import tqdm

global headers
headers: Dict[str, str] = {
    "Accept-Encoding": "gzip, deflate",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
    "User-Agent": "Mozilla/5.0",
}

# --- Configuration ---
TODAY_STR = datetime.now(timezone.utc).date().isoformat()
STATUS_FILENAME = "ebible_status.csv"
TRANSLATIONS_FILENAME = "translations.csv"

# Define the columns for the status file
# Start with original columns from translations.csv
ORIGINAL_COLUMNS = [
    "languageCode", "translationId", "languageName", "languageNameInEnglish",
    "dialect", "homeDomain", "title", "description", "Redistributable",
    "Copyright", "UpdateDate", "publicationURL", "OTbooks", "OTchapters",
    "OTverses", "NTbooks", "NTchapters", "NTverses", "DCbooks", "DCchapters",
    "DCverses", "FCBHID", "Certified", "inScript", "swordName", "rodCode",
    "textDirection", "downloadable", "font", "shortTitle", "PODISBN", "script",
    "sourceDate"
]

# Add new status tracking columns
STATUS_COLUMNS = [
    "status_download_path", "status_download_date", "status_unzip_path",
    "status_unzip_date", "status_extract_path", "status_extract_date", # extract path/date might be less useful now
    "status_last_error" # Added for basic error tracking
]

# Add new licence tracking columns
LICENCE_COLUMNS = [
    "licence_ID", "licence_File", "licence_Language", "licence_Dialect",
    "licence_Vernacular_Title", "licence_Licence_Type", "licence_Licence_Version",
    "licence_CC_Licence_Link", "licence_Copyright_Holder", "licence_Copyright_Years",
    "licence_Translation_by", "licence_date_read"
]

ALL_STATUS_COLUMNS = ORIGINAL_COLUMNS + STATUS_COLUMNS + LICENCE_COLUMNS

# --- Utility Functions ---

def log_and_print(file: Path, messages, log_type="Info") -> None:
    """Logs messages to a file and prints them to the console."""
    now_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    try:
        with open(file, "a", encoding='utf-8') as log:
            if isinstance(messages, str):
                log.write(f"{log_type}: {now_str} {messages}\n")
                print(messages)
            elif isinstance(messages, list):
                for message in messages:
                    log.write(f"{log_type}: {now_str} {message}\n")
                    print(message)
            elif isinstance(messages, pd.Series): # Handle pandas Series (like value_counts)
                msg_str = messages.to_string()
                log.write(f"{log_type}: {now_str}\n{msg_str}\n")
                print(msg_str)
            else: # Handle other types like value_counts() output
                 msg_str = str(messages)
                 log.write(f"{log_type}: {now_str}\n{msg_str}\n")
                 print(msg_str)

    except Exception as e:
        print(f"Error writing to log file {file}: {e}")
        # Also print the original message to console if logging failed
        if isinstance(messages, str):
            print(messages)
        elif isinstance(messages, list):
            for message in messages:
                print(message)
        else:
            print(str(messages))


def make_directories(dirs_to_create: List[Path]) -> None:
    """Creates directories if they don't exist."""
    for dir_to_create in dirs_to_create:
        dir_to_create.mkdir(parents=True, exist_ok=True)


def download_url_to_file(url: str, file: Path, headers: Dict = headers) -> bool:
    """Downloads a URL to a local file, returns True on success."""
    try:
        r = requests.get(url, headers=headers, timeout=60) # Added timeout
        r.raise_for_status() # Raise an exception for bad status codes
        with open(file, "wb") as out_file:
            out_file.write(r.content)
        return True
    except requests.exceptions.RequestException as e:
        print(f"Error downloading {url}: {e}")
        # Clean up potentially incomplete file
        if file.exists():
            try:
                file.unlink()
            except OSError as unlink_e:
                 print(f"Error removing incomplete download {file}: {unlink_e}")
        return False


def build_zip_filename(translation_id: str, date_str: str) -> str:
    """Builds the standard zip filename."""
    # Ensure date_str is in YYYY-MM-DD format if it's a date object
    if isinstance(date_str, date):
        date_str = date_str.isoformat()
    return f"{translation_id}--{date_str}.zip"


def create_project_name(language_code: str, translation_id: str) -> str:
    """Creates the destination project name."""
    if regex.match(f"^{regex.escape(language_code)}[_\\-]", translation_id):
        return translation_id[len(language_code) + 1 :].replace("-", "_")
    else:
        return translation_id


def is_date_older_than(date_str: Optional[str], max_days: int) -> bool:
    """Checks if a date string (YYYY-MM-DD) is older than max_days ago."""
    if pd.isna(date_str) or not isinstance(date_str, str) or not date_str:
        return True # Treat missing or invalid dates as old
    try:
        record_date = date.fromisoformat(date_str)
        cutoff_date = datetime.now(timezone.utc).date() - timedelta(days=max_days)
        return record_date < cutoff_date
    except ValueError:
        return True # Treat parse errors as old


def choose_yes_no(prompt: str) -> bool:
    """Prompts user for Y/N input."""
    choice: str = " "
    while choice not in ["n", "y"]:
        try:
            choice = input(prompt).strip()[0].lower()
        except IndexError:
            pass # Handle empty input
    return choice == "y"


def check_folders_exist(folders: list, base: Path, logfile: Path):
    """Checks if required folders exist, prompts to create if missing."""
    missing_folders: List[Path] = [folder for folder in folders if not folder.is_dir()]

    print(f"The base folder is : {base}")

    if missing_folders:
        print(
            f"\nThe following {len(missing_folders)} folders are required but missing:"
        )
        for folder in missing_folders:
            print(folder)

        print(f"\nBase folder check:    {base} ")
        if choose_yes_no("Create missing folders and continue? (Y/N): "):
            make_directories(missing_folders)
            log_and_print(logfile, f"Created required folders within {base}\n")
        else:
            print("Aborting script.")
            sys.exit() # Use sys.exit for clarity
    else:
        # Only log if the log folder itself exists (which it should after make_directories)
        if logfile.parent.exists():
             log_and_print(logfile, f"All required folders exist in {base}")
        else:
             print(f"Log folder {logfile.parent} does not exist, cannot log folder check.")

# --- Core Logic Functions ---

def initialize_or_load_status(status_path: Path, translations_path: Path, logfile: Path) -> pd.DataFrame:
    """Loads the status CSV, or creates it from the translations CSV if it doesn't exist."""
    if status_path.exists():
        log_and_print(logfile, f"Loading existing status file: {status_path}")
        try:
            status_df = pd.read_csv(status_path, keep_default_na=False, na_values=['']) # Treat empty strings as NA
            # Verify essential columns exist
            if not 'translationId' in status_df.columns:
                 raise ValueError("Status file missing 'translationId' column.")
            # Add any missing columns with default NaN values
            for col in ALL_STATUS_COLUMNS:
                if col not in status_df.columns:
                    log_and_print(logfile, f"Adding missing column '{col}' to status DataFrame.")
                    status_df[col] = np.nan
            # Ensure correct order
            status_df = status_df[ALL_STATUS_COLUMNS]

        except Exception as e:
            log_and_print(logfile, f"Error loading status file {status_path}: {e}. Attempting to rebuild.", log_type="Error")
            status_path.unlink(missing_ok=True) # Remove corrupted file
            return initialize_or_load_status(status_path, translations_path, logfile) # Recurse to rebuild

    else:
        log_and_print(logfile, f"Status file not found. Creating new one: {status_path}")
        if not translations_path.exists():
             log_and_print(logfile, f"Error: translations file missing at {translations_path}. Cannot create status file.", log_type="Critical")
             sys.exit(1)
        try:
            # Read translations, ensuring 'translationId' is string
            trans_df = pd.read_csv(translations_path, dtype={'translationId': str}, keep_default_na=False, na_values=[''])
            if 'translationId' not in trans_df.columns:
                 raise ValueError("Translations file missing 'translationId' column.")

            # Create status DataFrame with all columns
            status_df = pd.DataFrame(columns=ALL_STATUS_COLUMNS)

            # Copy data from translations_df for matching columns
            for col in ORIGINAL_COLUMNS:
                 if col in trans_df.columns:
                      status_df[col] = trans_df[col]
                 else:
                      log_and_print(logfile, f"Warning: Column '{col}' not found in {translations_path}", log_type="Warn")
                      status_df[col] = np.nan # Add as empty column if missing

            # Initialize new status/licence columns with NaN
            for col in STATUS_COLUMNS + LICENCE_COLUMNS:
                status_df[col] = np.nan

            # Ensure translationId is the index for easier merging later
            # status_df.set_index('translationId', inplace=True) # Let's keep it as a column for now

        except Exception as e:
            log_and_print(logfile, f"Error creating status file from {translations_path}: {e}", log_type="Critical")
            sys.exit(1)

    # --- Merge upstream changes (optional but recommended) ---
    # This adds new translations from translations.csv to status.csv
    # It does NOT update existing rows in status.csv from translations.csv by default
    try:
        trans_df = pd.read_csv(translations_path, dtype={'translationId': str}, keep_default_na=False, na_values=[''])
        if 'translationId' not in trans_df.columns:
             raise ValueError("Translations file missing 'translationId' column during merge check.")

        existing_ids = set(status_df['translationId'].astype(str))
        upstream_ids = set(trans_df['translationId'].astype(str))
        new_ids = list(upstream_ids - existing_ids)

        if new_ids:
            log_and_print(logfile, f"Found {len(new_ids)} new translations in {translations_path}. Adding to status.")
            new_rows_df = trans_df[trans_df['translationId'].isin(new_ids)].copy()

            # Prepare new rows with all status columns, initializing non-original ones
            full_new_rows = pd.DataFrame(columns=ALL_STATUS_COLUMNS)
            for col in ORIGINAL_COLUMNS:
                 if col in new_rows_df.columns:
                      full_new_rows[col] = new_rows_df[col]
                 else:
                      full_new_rows[col] = np.nan
            for col in STATUS_COLUMNS + LICENCE_COLUMNS:
                 full_new_rows[col] = np.nan

            status_df = pd.concat([status_df, full_new_rows], ignore_index=True)
            # Consider saving immediately after adding new rows?
            # status_df.to_csv(status_path, index=False)

        removed_ids = list(existing_ids - upstream_ids)
        if removed_ids:
             log_and_print(logfile, f"Warning: {len(removed_ids)} translations exist in status but not in upstream {translations_path}. They will be kept but may be outdated.", log_type="Warn")
             # Optionally, mark them as inactive or remove them:
             # status_df = status_df[~status_df['translationId'].isin(removed_ids)]

    except Exception as e:
        log_and_print(logfile, f"Error merging upstream changes from {translations_path}: {e}", log_type="Error")

    # Ensure data types are reasonable (especially for boolean checks later)
    status_df['Redistributable'] = status_df['Redistributable'].astype(str).str.lower() == 'true'
    status_df['downloadable'] = status_df['downloadable'].astype(str).str.lower() == 'true'
    # Convert verse counts safely to numeric, coercing errors to NaN, then fill NaN with 0
    for col in ['OTverses', 'NTverses']:
        status_df[col] = pd.to_numeric(status_df[col], errors='coerce').fillna(0).astype(int)

    return status_df


def filter_translations(df: pd.DataFrame, allow_non_redistributable: bool, verse_threshold: int, regex_filter: Optional[str], logfile: Path) -> pd.DataFrame:
    """Filters the DataFrame based on criteria."""
    initial_count = len(df)
    log_and_print(logfile, f"Initial translations in status file: {initial_count}")

    # 1. Filter by downloadable flag
    df = df[df['downloadable'] == True]
    log_and_print(logfile, f"Translations after 'downloadable' filter: {len(df)}")

    # 2. Filter by redistributable flag (if applicable)
    if not allow_non_redistributable:
        df = df[df['Redistributable'] == True]
        log_and_print(logfile, f"Translations after 'Redistributable' filter: {len(df)}")

    # 3. Filter by verse count
    df = df[(df['OTverses'] + df['NTverses']) >= verse_threshold]
    log_and_print(logfile, f"Translations after verse count filter (>= {verse_threshold}): {len(df)}")

    # 4. Apply regex filter (if provided)
    if regex_filter:
        try:
            df = df[df['translationId'].astype(str).str.match(regex_filter, na=False)]
            log_and_print(logfile, f"Translations after regex filter ('{regex_filter}'): {len(df)}")
        except regex.error as e:
            log_and_print(logfile, f"Invalid regex filter '{regex_filter}': {e}. Skipping filter.", log_type="Error")

    final_count = len(df)
    log_and_print(logfile, f"Filtered down to {final_count} translations to process.")
    return df


def determine_actions(df: pd.DataFrame, max_age_days: int, force_download: bool, downloads_folder: Path, projects_folder: Path, private_projects_folder: Path) -> pd.DataFrame:
    """Adds boolean columns indicating required actions."""

    df['action_needed_download'] = False
    df['action_needed_unzip'] = False
    df['action_needed_licence'] = False

    for index, row in df.iterrows():
        # --- Download Check ---
        needs_download = False
        if force_download:
            needs_download = True
        elif is_date_older_than(row['status_download_date'], max_age_days):
            needs_download = True
        elif pd.isna(row['status_download_path']) or not Path(row['status_download_path']).exists():
             # Check if file exists only if date is recent
             needs_download = True

        df.loc[index, 'action_needed_download'] = needs_download

        # --- Unzip Check ---
        needs_unzip = False
        if needs_download: # If downloading, must unzip
            needs_unzip = True
        elif force_download: # Force implies re-unzip too
            needs_unzip = True
        elif is_date_older_than(row['status_unzip_date'], max_age_days):
            needs_unzip = True
        elif pd.isna(row['status_unzip_path']) or not Path(row['status_unzip_path']).exists():
             # Check if dir exists only if date is recent
             needs_unzip = True

        df.loc[index, 'action_needed_unzip'] = needs_unzip

        # --- Licence Check ---
        needs_licence = False
        if needs_unzip: # If unzipping, must re-check licence
            needs_licence = True
        elif force_download: # Force implies re-check
            needs_licence = True
        elif is_date_older_than(row['licence_date_read'], max_age_days):
            needs_licence = True
        # No path check needed here, as licence data is in the status file itself

        df.loc[index, 'action_needed_licence'] = needs_licence

    return df


def download_required_files(df: pd.DataFrame, base_url: str, folder: Path, logfile: Path) -> pd.DataFrame:
    """Downloads files marked with action_needed_download."""
    translations_to_download = df[df['action_needed_download']]
    count = len(translations_to_download)
    log_and_print(logfile, f"Attempting to download {count} zip files...")

    downloaded_count = 0
    for index, row in tqdm(translations_to_download.iterrows(), total=count, desc="Downloading"):
        translation_id = row['translationId']
        url = f"{base_url}{translation_id}_usfm.zip"
        # Always use today's date for new downloads
        local_filename = build_zip_filename(translation_id, TODAY_STR)
        local_path = folder / local_filename

        log_and_print(logfile, f"Downloading {url} to {local_path}")
        if download_url_to_file(url, local_path):
            df.loc[index, 'status_download_path'] = str(local_path.resolve())
            df.loc[index, 'status_download_date'] = TODAY_STR
            df.loc[index, 'status_last_error'] = np.nan # Clear previous error on success
            downloaded_count += 1
            log_and_print(logfile, f"Success: Saved {url} as {local_path}")
            sleep(randint(1, 3000) / 1000) # Shorter sleep?
        else:
            df.loc[index, 'status_download_path'] = np.nan # Clear path on failure
            df.loc[index, 'status_download_date'] = np.nan # Clear date on failure
            df.loc[index, 'status_last_error'] = f"Download failed: {url}"
            log_and_print(logfile, f"Failed: Could not download {url}", log_type="Error")

    log_and_print(logfile, f"Finished downloading. Successfully downloaded {downloaded_count}/{count} files.")
    return df


def unzip_and_process_files(df: pd.DataFrame, downloads_folder: Path, projects_folder: Path, private_projects_folder: Path, vrs_diffs_data: Dict, logfile: Path) -> pd.DataFrame:
    """Unzips, renames, creates settings, and extracts licence for required projects."""
    translations_to_unzip = df[df['action_needed_unzip']]
    count = len(translations_to_unzip)
    log_and_print(logfile, f"Attempting to unzip and process {count} projects...")

    processed_count = 0
    for index, row in tqdm(translations_to_unzip.iterrows(), total=count, desc="Unzipping/Processing"):
        translation_id = row['translationId']
        lang_code = row['languageCode']
        is_redist = row['Redistributable']
        download_path_str = row['status_download_path']

        if pd.isna(download_path_str):
            log_and_print(logfile, f"Skipping unzip for {translation_id}: No valid download path found.", log_type="Warn")
            df.loc[index, 'status_last_error'] = "Unzip skipped: Missing download path"
            continue

        download_path = Path(download_path_str)
        if not download_path.exists():
             log_and_print(logfile, f"Skipping unzip for {translation_id}: Download path {download_path} not found.", log_type="Warn")
             df.loc[index, 'status_last_error'] = f"Unzip skipped: Download not found at {download_path}"
             continue

        unzip_base_dir = projects_folder if is_redist else private_projects_folder
        proj_name = create_project_name(lang_code, translation_id)
        project_dir = unzip_base_dir / proj_name

        log_and_print(logfile, f"Processing {translation_id}: Unzipping {download_path} to {project_dir}")

        # --- Unzip ---
        try:
            # Clean existing directory before unzipping
            if project_dir.exists():
                log_and_print(logfile, f"Removing existing directory: {project_dir}")
                shutil.rmtree(project_dir)
            project_dir.mkdir(parents=True, exist_ok=True)

            shutil.unpack_archive(download_path, project_dir)
            df.loc[index, 'status_unzip_path'] = str(project_dir.resolve())
            df.loc[index, 'status_unzip_date'] = TODAY_STR
            df.loc[index, 'status_last_error'] = np.nan # Clear error on successful unzip
            log_and_print(logfile, f"Successfully unzipped to {project_dir}")

            # --- Post-Unzip Processing ---
            # Rename USFM files
            rename_usfm(project_dir, logfile)

            # Write Settings.xml
            write_settings_file(project_dir, lang_code, translation_id, vrs_diffs_data)

            # Extract Licence Details (only if needed or forced)
            if row['action_needed_licence']:
                 df = get_and_update_licence_details(df, index, project_dir, logfile)

            processed_count += 1

        except (shutil.ReadError, FileNotFoundError, OSError, Exception) as e:
            log_and_print(logfile, f"Error processing {translation_id} at {project_dir}: {e}", log_type="Error")
            df.loc[index, 'status_unzip_path'] = np.nan
            df.loc[index, 'status_unzip_date'] = np.nan
            df.loc[index, 'status_last_error'] = f"Processing error: {e}"
            # Clean up potentially corrupted unzip dir
            if project_dir.exists():
                 try:
                      shutil.rmtree(project_dir)
                 except OSError as rm_e:
                      log_and_print(logfile, f"Could not remove failed unzip dir {project_dir}: {rm_e}", log_type="Warn")

    log_and_print(logfile, f"Finished processing. Successfully processed {processed_count}/{count} projects.")
    return df


def rename_usfm(project_dir: Path, logfile: Path):
    """Renames USFM files within the project directory."""
    log_and_print(logfile, f"Renaming USFM files in {project_dir}")
    renamed_count = 0
    try:
        usfm_paths = list(project_dir.glob("*.usfm"))
        for old_usfm_path in usfm_paths:
            new_sfm_path = get_destination_file_from_book(old_usfm_path)
            if new_sfm_path == old_usfm_path:
                continue
            if new_sfm_path.is_file():
                new_sfm_path.unlink() # Remove existing target

            # log_and_print(logfile, f"Renaming {old_usfm_path.name} to {new_sfm_path.name}")
            old_usfm_path.rename(new_sfm_path)
            renamed_count += 1
        if renamed_count > 0:
             log_and_print(logfile, f"Renamed {renamed_count} USFM files.")
    except Exception as e:
        log_and_print(logfile, f"Error renaming USFM files in {project_dir}: {e}", log_type="Error")


def get_and_update_licence_details(df: pd.DataFrame, index, project_dir: Path, logfile: Path) -> pd.DataFrame:
    """Extracts licence details from copr.htm and updates the DataFrame row."""
    copyright_path = project_dir / "copr.htm"
    log_and_print(logfile, f"Extracting licence info for {project_dir.name} from {copyright_path}")

    # Clear previous licence data for this row first
    for col in LICENCE_COLUMNS:
        if col != 'licence_date_read': # Keep date read until success
             df.loc[index, col] = np.nan

    if not copyright_path.exists():
        log_and_print(logfile, f"Unable to find {copyright_path}", log_type="Warn")
        df.loc[index, 'status_last_error'] = f"Licence check failed: copr.htm not found"
        df.loc[index, 'licence_date_read'] = TODAY_STR # Mark as checked today, even if failed
        return df

    entry = {} # Use a temporary dict
    entry["licence_ID"] = project_dir.name
    entry["licence_File"] = str(copyright_path.resolve())

    try:
        with open(copyright_path, "r", encoding="utf-8") as copr:
            html = copr.read()
            soup = BeautifulSoup(html, "lxml")

        cclink = soup.find(href=regex.compile("creativecommons"))
        if cclink:
            ref = cclink.get("href")
            if ref:
                entry["licence_CC_Licence_Link"] = ref
                # More robust CC parsing
                cc_match = regex.search(r"/licenses/([a-z\-]+)/([\d\.]+)", ref)
                if cc_match:
                    entry["licence_Licence_Type"] = cc_match.group(1)
                    entry["licence_Licence_Version"] = cc_match.group(2)
                else: # Handle simpler cases like /by/4.0/
                     cc_match_simple = regex.search(r"/licenses/([a-z\-]+)/?", ref)
                     if cc_match_simple:
                          entry["licence_Licence_Type"] = cc_match_simple.group(1)
                          # Try to find version elsewhere if needed

        titlelink = soup.find(href=regex.compile(f"https://ebible.org/{entry['licence_ID']}"))
        if titlelink and titlelink.string:
            entry["licence_Vernacular_Title"] = titlelink.string.strip()

        # Extract text, handle potential missing <p> or body
        body_tag = soup.body
        if body_tag and body_tag.p:
             copy_strings = [s.strip() for s in body_tag.p.stripped_strings if s.strip()]
        elif body_tag:
             copy_strings = [s.strip() for s in body_tag.stripped_strings if s.strip()]
        else:
             copy_strings = []
             log_and_print(logfile, f"Warning: No body or paragraph tag found in {copyright_path}", log_type="Warn")


        # Simpler text parsing logic
        is_public_domain = False
        for i, text in enumerate(copy_strings):
            if "public domain" in text.lower():
                is_public_domain = True
                break # Assume PD overrides other info
            elif "copyright Â©" in text.lower():
                 entry["licence_Copyright_Years"] = text # Keep full string for now
                 if i + 1 < len(copy_strings):
                      entry["licence_Copyright_Holder"] = copy_strings[i+1]
            elif text.lower().startswith("language:"):
                 if i + 1 < len(copy_strings):
                      entry["licence_Language"] = copy_strings[i+1]
            elif text.lower().startswith("dialect"): # Handles "Dialect:" or "Dialect (if applicable):"
                 # Take rest of string after colon, or the next string if current is just "Dialect:"
                 parts = text.split(":", 1)
                 if len(parts) > 1 and parts[1].strip():
                      entry["licence_Dialect"] = parts[1].strip()
                 elif i + 1 < len(copy_strings):
                      entry["licence_Dialect"] = copy_strings[i+1]
            elif "translation by" in text.lower():
                 entry["licence_Translation_by"] = text # Keep full string

        if is_public_domain:
            entry["licence_Copyright_Holder"] = "Public Domain"
            entry["licence_Licence_Type"] = "Public Domain" # Standardize
            entry["licence_Copyright_Years"] = "" # Clear years for PD

        # --- Data Cleaning/Defaults ---
        if pd.isna(entry.get("licence_Licence_Type")):
             if "Public Domain" == entry.get("licence_Copyright_Holder"):
                  entry["licence_Licence_Type"] = "Public Domain"
             elif entry.get("licence_CC_Licence_Link"):
                  entry["licence_Licence_Type"] = "CC (Unknown Version)" # Indicate CC link exists but type/version parse failed
             else:
                  entry["licence_Licence_Type"] = "Unknown" # Default if no other info

        # Apply specific known fixes (example)
        if entry["licence_ID"] in ["engwmb", "engwmbb"]:
             entry["licence_Copyright_Holder"] = "Public Domain"
             entry["licence_Licence_Type"] = "Public Domain"

        # Update DataFrame row
        for col_suffix, value in entry.items():
             # col_name = f"licence_{col_suffix}" # Prefix already included in entry keys
             if col_suffix in df.columns:
                  df.loc[index, col_suffix] = value
             else:
                  log_and_print(logfile, f"Warning: Licence key '{col_suffix}' not a column in DataFrame.", log_type="Warn")

        df.loc[index, 'licence_date_read'] = TODAY_STR
        df.loc[index, 'status_last_error'] = np.nan # Clear error on success
        log_and_print(logfile, f"Successfully extracted licence info for {project_dir.name}")

    except Exception as e:
        log_and_print(logfile, f"Error parsing licence file {copyright_path}: {e}", log_type="Error")
        df.loc[index, 'status_last_error'] = f"Licence parse error: {e}"
        df.loc[index, 'licence_date_read'] = TODAY_STR # Mark as checked today, even if failed

    return df


# --- Main Execution ---

def main() -> None:
    load_dotenv()

    parser: argparse.ArgumentParser = argparse.ArgumentParser(
        description="Download, unzip and process eBible translations using a status file."
    )
    parser.add_argument(
        "-f", "--filter", default=None,
        help="Regex filter for translationId (e.g., 'eng-.*' or '^(fra|spa)')",
    )
    parser.add_argument(
        "-d", "--force_download", default=False, action="store_true",
        help="Force download, unzip, and licence check for all filtered translations.",
    )
    parser.add_argument(
        "--allow_non_redistributable", default=False, action="store_true",
        help="Include non-redistributable (private) translations.",
    )
    # --download_only might need rethinking with status file, maybe remove or adapt?
    # parser.add_argument(
    #     "--download_only", default=False, action="store_true",
    #     help="Stop after downloading zip files.",
    # )
    parser.add_argument(
        "--max-age-days", default=None, type=int, # Default handled later
        help="Max age in days for downloaded/unzipped files before re-processing. Overrides .env.",
    )
    parser.add_argument(
        "--base-folder", default=None,
        help="Override base folder location (defaults to EBIBLE_DATA_DIR from .env or './_ebible_data').",
    )
    parser.add_argument(
        "--verse-threshold", default=400, type=int,
        help="Minimum total OT+NT verses required for a translation to be processed.",
    )
    args: argparse.Namespace = parser.parse_args()

    # --- Determine Base Path ---
    if args.base_folder:
        base = Path(args.base_folder).resolve()
        print(f"Using base folder from command line: {base}")
    elif os.getenv("EBIBLE_DATA_DIR"):
        base = Path(os.getenv("EBIBLE_DATA_DIR")).resolve()
        print(f"Using base folder from EBIBLE_DATA_DIR env var: {base}")
    else:
        # Default relative to the script's location might be safer
        base = (Path(__file__).parent / "_ebible_data").resolve()
        # repo_root = Path(__file__).parent.parent.parent.parent # Old assumption
        # base = repo_root / "_ebible_data"
        print(f"Using default base folder: {base}")

    # --- Define Paths ---
    corpus_folder: Path = base / "corpus"
    downloads_folder: Path = base / "downloads"
    private_corpus_folder: Path = base / "private_corpus"
    private_projects_folder: Path = base / "private_projects"
    projects_folder: Path = base / "projects"
    metadata_folder: Path = base / "metadata"
    logs_folder: Path = base / "logs"

    # --- Setup Logging ---
    logs_folder.mkdir(parents=True, exist_ok=True) # Ensure log dir exists first
    year, month, day, hour, minute = map(int, strftime("%Y %m %d %H %M").split())
    log_suffix: str = f"_{year}_{month:02d}_{day:02d}-{hour:02d}_{minute:02d}.log"
    log_filename: str = "ebible_status" + log_suffix
    logfile: Path = logs_folder / log_filename
    print(f"Logging to: {logfile}")

    # --- Check Folders ---
    required_folders = [
        corpus_folder, downloads_folder, private_corpus_folder,
        private_projects_folder, projects_folder, metadata_folder, logs_folder
    ]
    check_folders_exist(required_folders, base, logfile)

    # --- Determine Max Age ---
    max_age_days = args.max_age_days
    if max_age_days is None:
        env_max_age = os.getenv("MAX_AGE_DAYS")
        if env_max_age and env_max_age.isdigit():
            max_age_days = int(env_max_age)
            log_and_print(logfile, f"Using MAX_AGE_DAYS={max_age_days} from .env file.")
        else:
            max_age_days = 365 # Hardcoded default
            log_and_print(logfile, f"Using default max_age_days={max_age_days}.")
    else:
        log_and_print(logfile, f"Using --max-age-days={max_age_days} from command line.")

    # --- Download translations.csv if needed ---
    translations_csv_url: str = r"https://ebible.org/Scriptures/translations.csv"
    translations_csv: Path = metadata_folder / TRANSLATIONS_FILENAME
    if not translations_csv.is_file() or args.force_download:
        log_and_print(logfile, f"Downloading {translations_csv_url} to {translations_csv}")
        if not download_url_to_file(translations_csv_url, translations_csv):
            log_and_print(logfile, f"Critical: Failed to download {translations_csv}. Aborting.", log_type="Critical")
            sys.exit(1)
    else:
        log_and_print(logfile, f"{translations_csv} already exists.")

    # --- Load or Initialize Status ---
    status_path = metadata_folder / STATUS_FILENAME
    status_df = initialize_or_load_status(status_path, translations_csv, logfile)

    # --- Filter Translations ---
    filtered_df = filter_translations(
        status_df,
        args.allow_non_redistributable,
        args.verse_threshold,
        args.filter,
        logfile
    )

    if filtered_df.empty:
        log_and_print(logfile, "No translations match the specified filters. Exiting.")
        # Save status file even if empty? Maybe not necessary.
        # filtered_df.to_csv(status_path, index=False)
        sys.exit(0)

    # --- Determine Actions ---
    actions_df = determine_actions(
        filtered_df, max_age_days, args.force_download,
        downloads_folder, projects_folder, private_projects_folder
    )

    # --- Execute Actions ---
    eBible_url: str = r"https://ebible.org/Scriptures/"

    # Download
    actions_df = download_required_files(actions_df, eBible_url, downloads_folder, logfile)

    # # Option to stop after download (re-evaluate if needed)
    # if args.download_only:
    #     log_and_print(logfile, "Stopping after download phase as requested.")
    #     # Save status now
    #     status_df.update(actions_df) # Update the main df with changes
    #     status_df.to_csv(status_path, index=False)
    #     log_and_print(logfile, f"Saved updated status to {status_path}")
    #     sys.exit(0)

    # Unzip, Rename, Settings, Licence
    vrs_diffs_data = get_vrs_diffs()
    actions_df = unzip_and_process_files(
        actions_df, downloads_folder, projects_folder,
        private_projects_folder, vrs_diffs_data, logfile
    )

    # --- Update Main Status DataFrame and Save ---
    # Use update() which aligns on index (translationId if set, otherwise row number)
    # Ensure index is set correctly if needed, or update based on 'translationId' column
    status_df.set_index('translationId', inplace=True, drop=False) # Set index temporarily
    actions_df.set_index('translationId', inplace=True, drop=False)
    status_df.update(actions_df)
    status_df.reset_index(drop=True, inplace=True) # Remove index before saving

    try:
        status_df.to_csv(status_path, index=False)
        log_and_print(logfile, f"\nSaved updated status for {len(status_df)} translations to {status_path}")
    except Exception as e:
        log_and_print(logfile, f"Error saving status file {status_path}: {e}", log_type="Error")

    # --- Final Info and Commands ---
    log_and_print(logfile, "\nLicence Type Summary (Processed Translations):")
    # Filter actions_df for successfully processed ones if needed, or show all filtered
    log_and_print(logfile, actions_df['licence_Licence_Type'].value_counts(dropna=False))

    # Define extract log paths using the same suffix
    public_extract_log: Path = logs_folder / ("extract_public" + log_suffix)
    private_extract_log: Path = logs_folder / ("extract_private" + log_suffix)

    log_and_print(
        logfile,
        [
            "\n--- Next Step: Bulk Extraction ---",
            "Use SILNLP's bulk_extract_corpora tool.",
            "Ensure you have SILNLP installed and configured (e.g., via poetry).",
            "\nCommand for PRIVATE projects:",
            f"poetry run python -m silnlp.common.bulk_extract_corpora --input \"{private_projects_folder}\" --output \"{private_corpus_folder}\" --error-log \"{private_extract_log}\"",
            "\nCommand for PUBLIC projects:",
            f"poetry run python -m silnlp.common.bulk_extract_corpora --input \"{projects_folder}\" --output \"{corpus_folder}\" --error-log \"{public_extract_log}\"",
            "\n---------------------------------"
        ],
    )


if __name__ == "__main__":
    main()
```

**Explanation of Key Changes:**

1.  **File Renaming:** The script is saved as `ebible_status.py`.
2.  **Status File Handling (`initialize_or_load_status`):**
   *   Defines `STATUS_FILENAME` (`ebible_status.csv`).
   *   Defines all expected columns (`ALL_STATUS_COLUMNS`).
   *   Tries to load `ebible_status.csv` using pandas.
   *   If it exists, it checks for missing columns and adds them.
   *   If it *doesn't* exist (or fails to load), it reads `translations.csv`, creates a new DataFrame with all columns, populates original data, and initializes new columns with `NaN`.
   *   **Merging Upstream Changes:** It compares `translationId`s between the loaded status and the current `translations.csv`. New IDs found upstream are added as new rows to the status DataFrame. It logs a warning for IDs present in status but missing upstream.
   *   Ensures basic data types (`Redistributable`, `downloadable`, verse counts) are correct.
3.  **Argument Parsing:**
   *   `--max_zip_age_days` is renamed to `--max-age-days`.
   *   The default for `--max-age-days` is set to `None`, and logic is added in `main` to check the `.env` variable `MAX_AGE_DAYS` if the argument isn't provided, falling back to 365.
   *   `--verse-threshold` argument added.
4.  **Filtering (`filter_translations`):**
   *   This function now takes the status DataFrame and applies filters based on `downloadable`, `Redistributable`, verse count (`--verse-threshold`), and the optional `--filter` regex.
5.  **Determining Actions (`determine_actions`):**
   *   This new function iterates through the filtered DataFrame.
   *   For each row, it checks `status_download_date`, `status_unzip_date`, `licence_date_read` against `max_age_days`.
   *   It also considers `force_download` and checks if required paths (`status_download_path`, `status_unzip_path`) actually exist if the dates are recent.
   *   It adds boolean columns (`action_needed_download`, `action_needed_unzip`, `action_needed_licence`) to the DataFrame.
6.  **Downloading (`download_required_files`):**
   *   Takes the DataFrame with action flags.
   *   Filters for rows where `action_needed_download` is True.
   *   Iterates and downloads, updating `status_download_path`, `status_download_date`, and `status_last_error` directly in the DataFrame upon success or failure.
7.  **Unzipping and Processing (`unzip_and_process_files`):**
   *   Filters for rows where `action_needed_unzip` is True.
   *   Checks if the required `status_download_path` exists.
   *   Removes the existing project directory before unzipping to ensure freshness.
   *   Unzips the archive.
   *   Updates `status_unzip_path` and `status_unzip_date`.
   *   Calls `rename_usfm`.
   *   Calls `write_settings_file`.
   *   Calls `get_and_update_licence_details` **only if** `action_needed_licence` is True for that row.
   *   Updates `status_last_error` on failure.
8.  **Licence Extraction (`get_and_update_licence_details`):**
   *   This function is significantly refactored.
   *   It now takes the main DataFrame and the specific `index` of the row to update.
   *   It clears previous licence data for that row.
   *   It parses `copr.htm`.
   *   It updates the `licence_*` columns and `licence_date_read` directly in the DataFrame at the given `index`.
   *   Includes basic data cleaning and handling for "Public Domain".
   *   The separate `licences.tsv` file and `write_licence_file` function are removed.
9.  **Saving Status:**
   *   At the end of `main`, the potentially modified `status_df` (updated using `df.update()`) is saved back to `ebible_status.csv`. `df.update()` is used to merge the changes from the processed subset (`actions_df`) back into the main DataFrame.
10. **Helper Functions:**
   *   `get_translations` is removed as its logic is integrated into `initialize_or_load_status` and `filter_translations`.
   *   `find_recent_download` is removed as the download path is now directly tracked in the status file.
   *   `write_licence_file` is removed.
11. **Logging:** Logging messages are updated to reflect the new workflow.
12. **Error Handling:** Basic error handling is added (e.g., `status_last_error` column, try/except blocks around file operations).

This script should provide the desired stateful processing, avoiding redundant work based on the status file and the `--max-age-days` setting. Remember to install pandas (`pip install pandas`) if you haven't already.




David Baines
The ebible.py script doesn't keep track of the passage of each translation as well as it could.
I wonder what methods would be good for improving the storing and making use of infomation about the actions of the script, so that progress can be stored in order to avoid repeating actions unnecessarily.

We may download and read in the translations.csv file that is stored in the ebible_data/metadata folder. It could be good to hold that in memory and add to each translation information such as: Name of the downloaded file and date stamp for downloading. Name of the unzipped project folder and date stamp of unzipping. Name of the extracted text file which will be <isocode>-<project-folder.name> and the date stamp of extracting - which will be the last modified date of the extracted text file.

This could all be saved as an expanded ebible_translations.csv which would be read each time the code runs so that previously done actions don't need to be done again unless the --force-download flag is set. Or the --max-zip-age-days is exceeded.

Don't write code at this point. Please just discuss ways of doing something like this. First list a few potential solutions with their pros and cons. the folder name where it has been unzipped and the unzipp


Gemini
Okay, let's explore some ways to improve the progress tracking in the ebible.py script. The goal is to store the state of each translation (downloaded, unzipped, extracted) persistently, so the script can efficiently pick up where it left off and avoid redundant work, while still respecting options like --force_download and --max_zip_age_days.

Here are a few potential approaches:

1. Enhanced CSV Status File (Your Suggestion)

Description:
Maintain a separate CSV file (e.g., metadata/ebible_status.csv).
This file would store the translationId along with columns for:
download_path (path to the specific zip file used)
download_timestamp (when it was downloaded or verified as recent)
unzip_path (path to the project folder)
unzip_timestamp (when it was unzipped)
extract_path (path to the final corpus text file)
extract_timestamp (last modified time of the corpus file)
At the start, the script reads translations.csv (the source list) and ebible_status.csv (the current state).
For each translation, it compares the status information against the required steps and flags (--force_download, --max_zip_age_days).
As each step (download, unzip, extract) completes successfully for a translation, the script updates the corresponding information in memory.
The updated status data is written back to ebible_status.csv at the end of the run.
Pros:
Simplicity: Relatively straightforward to implement using existing libraries like csv or pandas.
Readability: The status file is human-readable and can be easily inspected or manually edited if needed.
Persistence: Clearly stores state between script runs.
Alignment: Fits well with the script's current use of CSV data.
Cons:
File I/O: Reading and writing the entire status CSV on every run might become slightly inefficient if the number of translations grows extremely large.
Concurrency: Not inherently safe if multiple instances of the script were run simultaneously against the same status file (though this seems unlikely for this use case).
Merging Logic: Requires logic to handle discrepancies between translations.csv (source) and ebible_status.csv (state) â€“ e.g., new translations added upstream, or translations removed.
2. SQLite Database

Description:
Use a single SQLite database file (e.g., metadata/ebible_status.db) to store the status.
Create a table (e.g., translation_status) with columns similar to the CSV approach (translationId, language_code, is_redistributable, download_path, download_timestamp, unzip_path, unzip_timestamp, extract_path, extract_timestamp).
The script connects to the database at the start.
It queries the database for the status of each translation it intends to process.
Updates are performed using SQL UPDATE statements after each successful step.
New translations found in translations.csv but not in the database are added with an initial status.
Pros:
Robustness: Databases are designed for efficient querying and atomic updates, reducing risks of data corruption compared to rewriting entire files.
Querying: Easier to perform more complex queries on the status data if needed (e.g., "show all translations downloaded but not unzipped").
Scalability: Generally handles large amounts of data more efficiently than reading/writing large flat files.
Built-in Support: Python's standard library includes the sqlite3 module.
Cons:
Complexity: Introduces database concepts (SQL, schema management), slightly increasing the learning curve and code complexity compared to CSV.
Readability: The status data isn't as easily human-readable/editable outside the script without a database tool.
Overhead: Might be slight overkill if the number of translations remains manageable and complex queries aren't needed.
3. Filesystem Markers

Description:
Rely more heavily on the existence and modification times of the actual files and directories produced by each step.
Download: Check for downloads/{id}--YYYY-MM-DD.zip files and their dates (already partially done).
Unzip: Check for the existence of the target project directory (projects/{proj_name} or private_projects/{proj_name}). Potentially add an empty marker file (e.g., .unzipped_ok) inside the directory upon successful completion to distinguish from a partial unzip.
Extract: Check for the existence and modification time of the final corpus file (corpus/{lang}-{proj_name}.txt or private_corpus/...).
The script would perform these filesystem checks for each translation to determine the next required step.
Pros:
Implicit State: The state is directly tied to the artifacts themselves; no separate status file to manage or potentially get out of sync.
Minimal Overhead: Avoids reading/writing a dedicated status file/database. Leverages filesystem operations which are often fast.
Cons:
Less Explicit: Harder to get a quick overview of the status of all translations. Querying specific states (e.g., "downloaded but not unzipped") becomes more complex, requiring iterating through directories.
Ambiguity: Simple directory existence might not guarantee a successful prior step (e.g., unzip might have failed halfway). Marker files help but add clutter.
Timestamp Reliance: Relying solely on modification times can sometimes be fragile.
Tracking Specifics: Doesn't easily track which specific downloaded zip file corresponds to the current unzipped version if multiple downloads exist for the same ID.
Discussion:

The Enhanced CSV Status File seems like the most balanced approach for this scenario. It directly addresses the need for explicit tracking of each step's completion and associated metadata (paths, timestamps) while leveraging the existing familiarity with CSV handling in the script. It provides good readability and persistence with manageable complexity. The key challenge is implementing the logic to correctly read the status, compare it with requirements/flags, perform actions, and reliably update and save the status. Using a separate ebible_status.csv avoids modifying the downloaded translations.csv and simplifies handling updates.

The SQLite approach is a strong runner-up, offering more robustness and scalability, but might add unnecessary complexity unless the tracking requirements become significantly more intricate.

The Filesystem Markers approach is the least explicit and potentially the most fragile, making it harder to reliably determine the exact state and debug issues.

Therefore, proceeding with the idea of a separate ebible_status.csv file appears to be a practical and effective way forward.

Rate this answer: 
Context Sources (1)

David Baines
Would you propose a solution based on the "Enhanced CSV Status File" system. Please ensure that the status file includes all the data from the translations.csv file. It should maintain all the columns in the translations.csv file: "languageCode","translationId","languageName","languageNameInEnglish","dialect","homeDomain","title","description","Redistributable","Copyright","UpdateDate","publicationURL","OTbooks","OTchapters","OTverses","NTbooks","NTchapters","NTverses","DCbooks","DCchapters","DCverses","FCBHID","Certified","inScript","swordName","rodCode","textDirection","downloadable","font","shortTitle","PODISBN","script","sourceDate" Any new columns that it adds should have unique names that don't use these names. Please also include columns for keeping track of the licence data. The data from the licence files should be added to the ebible_status.csv file along with a data stamp for when that data was read from the licence file and added to the ebible_status.csv.

Let's rename the --max_zip_age_days argument to --max-age-days and use that one value to control what is considered out of date and what is considered up to date. Maybe storing only the date of the last update is sufficient since we don't have a use for the time. Are there pros and cons to putting that in the .env file?

We should prefix the licence column names with "licence_" so that it is clear where that information came from. This is important because these columns sometimes have very similar names to the translation.csv columns but the data is not always consistent. This will help a human to see where the licence data is inconsistent with the translations.csv data. The licence column names would be: "licence_ID", "licence_File", "licence_Language", "licence_Dialect", "licence_Vernacular Title", "licence_Licence Type", "licence_Licence Version", "licence_CC Licence Link", "licence_Copyright Holder", "licence_Copyright Years", "licence_Translation by" And the date of reading the data: "licence_date_read"

How does this sound and are there things that could improve it?

ebible.py: 13 lines selected

Gemini
Okay, let's flesh out the "Enhanced CSV Status File" approach based on your suggestions. This sounds like a solid plan to make the script more robust and efficient.

Here's a breakdown of how it could work:

1. The ebible_status.csv File

Location: metadata/ebible_status.csv (relative to the base folder).
Purpose: This file becomes the central hub for tracking the state of each translation processed by the script. It persists information between runs.
Initialization:
On the first run (or if metadata/ebible_status.csv is missing), the script reads metadata/translations.csv.
It creates ebible_status.csv by copying all columns and rows from translations.csv.
It then adds the new status and licence columns, initially populated with empty values or appropriate defaults.
Subsequent Runs:
The script reads the existing ebible_status.csv into memory (likely using pandas).
It also reads metadata/translations.csv to check for upstream changes (new translations added, maybe metadata updates, although handling metadata updates might be complex).
It merges information: New translations from translations.csv are added as new rows to the status DataFrame. Existing rows are potentially updated if key metadata from translations.csv changes (this needs careful consideration â€“ maybe only add new ones and leave existing ones?).
Columns: The file would contain:
All original columns from translations.csv: languageCode, translationId, languageName, languageNameInEnglish, dialect, homeDomain, title, description, Redistributable, Copyright, UpdateDate, publicationURL, OTbooks, OTchapters, OTverses, NTbooks, NTchapters, NTverses, DCbooks, DCchapters, DCverses, FCBHID, Certified, inScript, swordName, rodCode, textDirection, downloadable, font, shortTitle, PODISBN, script, sourceDate
New Status Tracking Columns:
status_download_path: (String) Full path to the specific zip file used.
status_download_date: (String, YYYY-MM-DD) Date the zip was downloaded or last verified.
status_unzip_path: (String) Full path to the unzipped project folder.
status_unzip_date: (String, YYYY-MM-DD) Date the project folder was successfully unzipped/created.
status_extract_path: (String) Expected full path to the final corpus text file.
status_extract_date: (String, YYYY-MM-DD) Last modified date of the corpus text file (if it exists). Note: Since extraction is external, this reflects when the script last checked the file.
New Licence Tracking Columns (Prefixed):
licence_ID: (String) The ID found in the licence file (often the project folder name).
licence_File: (String) Path to the copr.htm file checked.
licence_Language: (String) Language from the licence file.
licence_Dialect: (String) Dialect from the licence file.
licence_Vernacular_Title: (String) Vernacular Title from the licence file. (Underscore used for easier column naming).
licence_Licence_Type: (String) Licence Type from the licence file.
licence_Licence_Version: (String) Licence Version from the licence file.
licence_CC_Licence_Link: (String) CC Licence Link from the licence file.
licence_Copyright_Holder: (String) Copyright Holder from the licence file.
licence_Copyright_Years: (String) Copyright Years from the licence file.
licence_Translation_by: (String) Translation by info from the licence file.
licence_date_read: (String, YYYY-MM-DD) Date the licence information was extracted and added/updated in this status file.
2. Workflow Integration

Initialization: Read/create/update ebible_status.csv as described above.
Filtering: Apply the --filter to the status DataFrame.
Determine Actions: Iterate through the rows (translations) in the status DataFrame. For each translation:
Check Download:
If --force_download is set, mark for download.
Else, check status_download_date. If it's missing or older than --max-age-days, mark for download.
Else, verify status_download_path still exists. If not, mark for download. Otherwise, skip download.
Check Unzip:
If marked for download (implying a new zip will be used), mark for unzip.
If --force_download is set, mark for unzip.
Else, check status_unzip_date. If it's missing or older than --max-age-days, mark for unzip.
Else, verify status_unzip_path still exists. If not, mark for unzip. Otherwise, skip unzip.
Check Licence:
If marked for unzip (implying project files might change), mark for licence check.
If --force_download is set, mark for licence check.
Else, check licence_date_read. If it's missing or older than --max-age-days, mark for licence check. Otherwise, skip licence check.
(Check Extract: This step remains manual, but the script could potentially check if status_extract_path exists and update status_extract_date if the user wants that info.)
Perform Actions:
Download: Perform downloads for marked translations. Update status_download_path and status_download_date in the DataFrame upon success.
Unzip: Perform unzipping for marked translations. Update status_unzip_path and status_unzip_date. Run rename_usfm and write_settings_file as part of this step.
Licence Extraction: Modify get_licence_details to take the status DataFrame and the specific project_dir as input. It should find the relevant row in the DataFrame and update the licence_* columns and licence_date_read directly. The separate licences.tsv file and write_licence_file function become obsolete.
Save Status: Write the entire updated status DataFrame back to metadata/ebible_status.csv at the end of the script run.
3. Argument Renaming: --max-age-days

Yes, renaming --max_zip_age_days to --max-age-days makes perfect sense. It generalizes the concept, as this single value will now control the freshness check for downloads, unzips, and licence data extraction. This simplifies configuration.
4. Date vs. Timestamp

Using just the date (YYYY-MM-DD) for status_download_date, status_unzip_date, and licence_date_read seems sufficient and simpler.
Pros: Easier to parse, compare, and less verbose in the CSV. Avoids timezone complexities if the script runs in different environments. The --max-age-days comparison works perfectly with just dates.
Cons: Loses granularity. If the script were run multiple times a day with --max-age-days=0, using only the date wouldn't distinguish between runs on the same day. However, given the nature of this script (downloading large files, processing potentially taking time), checking freshness based on days seems appropriate.
Recommendation: Stick with YYYY-MM-DD dates.
5. --max-age-days in .env File

Pros:
Configuration: Allows users to set a persistent default value without modifying the script or command-line arguments every time. Good for user-specific or project-specific settings.
Consistency: Ensures the same default max age is used unless overridden by the command line.
Separation of Concerns: Keeps configuration separate from code.
Cons:
Discovery: Users need to know to look in the .env file for this setting.
Complexity: Adds another place to manage configuration (though .env is already used).
Precedence: The script needs clear logic for precedence (e.g., command-line argument overrides .env value, which overrides a hardcoded default in the script).
Recommendation: It's a reasonable place to put the default value. The script should:
Look for the --max-age-days command-line argument.
If not provided, look for a MAX_AGE_DAYS variable in the .env file.
If not found there, use a hardcoded default (e.g., 365) within the script. This provides flexibility while having sensible fallbacks.
6. Licence Data Integration

Integrating the licence data directly into ebible_status.csv is a good improvement.
Prefixing: Using licence_ as a prefix is crucial, as you noted, to avoid ambiguity with columns from translations.csv (like Copyright, Language, Dialect). It clearly marks the data source.
Redundancy: This eliminates the need for the separate licences.tsv file and the write_licence_file function, simplifying the overall process and keeping all relevant state in one place.
Modification: get_licence_details will need refactoring. Instead of returning a list of dictionaries, it should accept the main status DataFrame and the path to the specific project being processed. It will locate the correct row in the DataFrame (based on translationId or project_path) and update the licence_* columns and licence_date_read for that row directly.
Overall Assessment & Potential Improvements

Soundness: This approach is robust and significantly improves the script's ability to manage state and avoid redundant work. Using pandas for CSV manipulation will handle the large number of columns efficiently.
Clarity: Storing all state information in one place makes it easier to understand the status of the entire process. The licence_ prefix adds necessary clarity.
Efficiency: Avoids unnecessary downloads, unzipping, and licence parsing based on configurable age limits.
Potential Improvements/Considerations:
Error Handling: How should errors during download, unzip, or licence parsing be recorded in the status file? Add a status_last_error column?
Upstream Metadata Changes: How strictly should changes in translations.csv (e.g., an updated Copyright field) trigger reprocessing or updates in ebible_status.csv? For simplicity, initially, maybe only focus on adding new translations found in translations.csv. Updating existing rows based on upstream changes could add significant complexity.
File Format: While CSV is good for readability, for a very large number of translations and columns, a more performant format like Parquet could eventually be considered, but CSV with pandas is likely fine for the foreseeable future.
Schema Versioning: If the structure of ebible_status.csv changes in the future (e.g., adding new status columns), how will older status files be handled? (Might be overkill for now).
Cleanup: The script doesn't currently clean up old zip files or project directories that are no longer needed (e.g., if a translation is removed upstream or if multiple dated zips exist for one ID). This status file could potentially facilitate such cleanup logic in the future.
This enhanced CSV approach looks like the right direction, providing a good balance of functionality, readability, and implementation complexity.

Rate this answer: 
Context Sources (1)

David Baines
Can you write a complete script that would use this system? Let's call it ebible_status.py then I can test the solution before renaming it as ebible.py once any bugs are resolved.

ebible.py: 13 lines selected

Gemini
Okay, let's create the ebible_status.py script incorporating the enhanced CSV status file system.

This script will:

Maintain an ebible_status.csv file in the metadata folder.
Initialize or update this status file based on translations.csv.
Use the status file to track download, unzip, and licence extraction dates.
Use the --max-age-days argument (with .env fallback) to determine when actions need re-running.
Integrate licence data directly into the status file, removing the need for licences.tsv.
Update relevant functions to work with the pandas DataFrame holding the status.
diff
-0
+826
The code change produced by Gemini could not be fully parsed. Can't automatically accept changes.
"""ebible_status.py contains functions for downloading and processing data from eBible.org.

This version uses a status file (ebible_status.csv) to track progress and avoid
re-running steps unnecessarily.

Workflow:
1. Read or initialize ebible_status.csv, merging info from translations.csv.
2. Filter translations based on command-line arguments.
3. Determine required actions (download, unzip, licence check) based on status
   dates, --max-age-days, and --force_download.
4. Perform downloads, updating status.
5. Perform unzipping, renaming, settings file creation, and licence extraction,
   updating status.
6. Save the updated ebible_status.csv.
7. Print commands for bulk extraction using SILNLP.
"""

import argparse
import os
import shutil
import sys
from csv import DictReader  # Keep for potential future use? Maybe remove.
from datetime import date, datetime, timedelta, timezone
from pathlib import Path
from random import randint
from time import sleep, strftime
from typing import Dict, List, Optional, Set

import numpy as np
import pandas as pd
import regex
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from rename_usfm import get_destination_file_from_book
from settings_file import get_vrs_diffs, write_settings_file
from tqdm import tqdm

global headers
headers: Dict[str, str] = {
    "Accept-Encoding": "gzip, deflate",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
    "User-Agent": "Mozilla/5.0",
}

# --- Configuration ---
TODAY_STR = datetime.now(timezone.utc).date().isoformat()
STATUS_FILENAME = "ebible_status.csv"
TRANSLATIONS_FILENAME = "translations.csv"

# Define the columns for the status file
# Start with original columns from translations.csv
ORIGINAL_COLUMNS = [
    "languageCode", "translationId", "languageName", "languageNameInEnglish",
    "dialect", "homeDomain", "title", "description", "Redistributable",
    "Copyright", "UpdateDate", "publicationURL", "OTbooks", "OTchapters",
    "OTverses", "NTbooks", "NTchapters", "NTverses", "DCbooks", "DCchapters",
    "DCverses", "FCBHID", "Certified", "inScript", "swordName", "rodCode",
    "textDirection", "downloadable", "font", "shortTitle", "PODISBN", "script",
    "sourceDate"
]

# Add new status tracking columns
STATUS_COLUMNS = [
    "status_download_path", "status_download_date", "status_unzip_path",
    "status_unzip_date", "status_extract_path", "status_extract_date", # extract path/date might be less useful now
    "status_last_error" # Added for basic error tracking
]

# Add new licence tracking columns
LICENCE_COLUMNS = [
    "licence_ID", "licence_File", "licence_Language", "licence_Dialect",
    "licence_Vernacular_Title", "licence_Licence_Type", "licence_Licence_Version",
    "licence_CC_Licence_Link", "licence_Copyright_Holder", "licence_Copyright_Years",
    "licence_Translation_by", "licence_date_read"
]

ALL_STATUS_COLUMNS = ORIGINAL_COLUMNS + STATUS_COLUMNS + LICENCE_COLUMNS

# --- Utility Functions ---

def log_and_print(file: Path, messages, log_type="Info") -> None:
    """Logs messages to a file and prints them to the console."""
    now_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    try:
        with open(file, "a", encoding='utf-8') as log:
            if isinstance(messages, str):
                log.write(f"{log_type}: {now_str} {messages}\n")
                print(messages)
            elif isinstance(messages, list):
                for message in messages:
                    log.write(f"{log_type}: {now_str} {message}\n")
                    print(message)
            elif isinstance(messages, pd.Series): # Handle pandas Series (like value_counts)
                msg_str = messages.to_string()
                log.write(f"{log_type}: {now_str}\n{msg_str}\n")
                print(msg_str)
            else: # Handle other types like value_counts() output
                 msg_str = str(messages)
                 log.write(f"{log_type}: {now_str}\n{msg_str}\n")
                 print(msg_str)

    except Exception as e:
        print(f"Error writing to log file {file}: {e}")
        # Also print the original message to console if logging failed
        if isinstance(messages, str):
            print(messages)
        elif isinstance(messages, list):
            for message in messages:
                print(message)
        else:
            print(str(messages))


def make_directories(dirs_to_create: List[Path]) -> None:
    """Creates directories if they don't exist."""
    for dir_to_create in dirs_to_create:
        dir_to_create.mkdir(parents=True, exist_ok=True)


def download_url_to_file(url: str, file: Path, headers: Dict = headers) -> bool:
    """Downloads a URL to a local file, returns True on success."""
    try:
        r = requests.get(url, headers=headers, timeout=60) # Added timeout
        r.raise_for_status() # Raise an exception for bad status codes
        with open(file, "wb") as out_file:
            out_file.write(r.content)
        return True
    except requests.exceptions.RequestException as e:
        print(f"Error downloading {url}: {e}")
        # Clean up potentially incomplete file
        if file.exists():
            try:
                file.unlink()
            except OSError as unlink_e:
                 print(f"Error removing incomplete download {file}: {unlink_e}")
        return False


def build_zip_filename(translation_id: str, date_str: str) -> str:
    """Builds the standard zip filename."""
    # Ensure date_str is in YYYY-MM-DD format if it's a date object
    if isinstance(date_str, date):
        date_str = date_str.isoformat()
    return f"{translation_id}--{date_str}.zip"


def create_project_name(language_code: str, translation_id: str) -> str:
    """Creates the destination project name."""
    if regex.match(f"^{regex.escape(language_code)}[_\\-]", translation_id):
        return translation_id[len(language_code) + 1 :].replace("-", "_")
    else:
        return translation_id


def is_date_older_than(date_str: Optional[str], max_days: int) -> bool:
    """Checks if a date string (YYYY-MM-DD) is older than max_days ago."""
    if pd.isna(date_str) or not isinstance(date_str, str) or not date_str:
        return True # Treat missing or invalid dates as old
    try:
        record_date = date.fromisoformat(date_str)
        cutoff_date = datetime.now(timezone.utc).date() - timedelta(days=max_days)
        return record_date < cutoff_date
    except ValueError:
        return True # Treat parse errors as old


def choose_yes_no(prompt: str) -> bool:
    """Prompts user for Y/N input."""
    choice: str = " "
    while choice not in ["n", "y"]:
        try:
            choice = input(prompt).strip()[0].lower()
        except IndexError:
            pass # Handle empty input
    return choice == "y"


def check_folders_exist(folders: list, base: Path, logfile: Path):
    """Checks if required folders exist, prompts to create if missing."""
    missing_folders: List[Path] = [folder for folder in folders if not folder.is_dir()]

    print(f"The base folder is : {base}")

    if missing_folders:
        print(
            f"\nThe following {len(missing_folders)} folders are required but missing:"
        )
        for folder in missing_folders:
            print(folder)

        print(f"\nBase folder check:    {base} ")
        if choose_yes_no("Create missing folders and continue? (Y/N): "):
            make_directories(missing_folders)
            log_and_print(logfile, f"Created required folders within {base}\n")
        else:
            print("Aborting script.")
            sys.exit() # Use sys.exit for clarity
    else:
        # Only log if the log folder itself exists (which it should after make_directories)
        if logfile.parent.exists():
             log_and_print(logfile, f"All required folders exist in {base}")
        else:
             print(f"Log folder {logfile.parent} does not exist, cannot log folder check.")

# --- Core Logic Functions ---

def initialize_or_load_status(status_path: Path, translations_path: Path, logfile: Path) -> pd.DataFrame:
    """Loads the status CSV, or creates it from the translations CSV if it doesn't exist."""
    if status_path.exists():
        log_and_print(logfile, f"Loading existing status file: {status_path}")
        try:
            status_df = pd.read_csv(status_path, keep_default_na=False, na_values=['']) # Treat empty strings as NA
            # Verify essential columns exist
            if not 'translationId' in status_df.columns:
                 raise ValueError("Status file missing 'translationId' column.")
            # Add any missing columns with default NaN values
            for col in ALL_STATUS_COLUMNS:
                if col not in status_df.columns:
                    log_and_print(logfile, f"Adding missing column '{col}' to status DataFrame.")
                    status_df[col] = np.nan
            # Ensure correct order
            status_df = status_df[ALL_STATUS_COLUMNS]

        except Exception as e:
            log_and_print(logfile, f"Error loading status file {status_path}: {e}. Attempting to rebuild.", log_type="Error")
            status_path.unlink(missing_ok=True) # Remove corrupted file
            return initialize_or_load_status(status_path, translations_path, logfile) # Recurse to rebuild

    else:
        log_and_print(logfile, f"Status file not found. Creating new one: {status_path}")
        if not translations_path.exists():
             log_and_print(logfile, f"Error: translations file missing at {translations_path}. Cannot create status file.", log_type="Critical")
             sys.exit(1)
        try:
            # Read translations, ensuring 'translationId' is string
            trans_df = pd.read_csv(translations_path, dtype={'translationId': str}, keep_default_na=False, na_values=[''])
            if 'translationId' not in trans_df.columns:
                 raise ValueError("Translations file missing 'translationId' column.")

            # Create status DataFrame with all columns
            status_df = pd.DataFrame(columns=ALL_STATUS_COLUMNS)

            # Copy data from translations_df for matching columns
            for col in ORIGINAL_COLUMNS:
                 if col in trans_df.columns:
                      status_df[col] = trans_df[col]
                 else:
                      log_and_print(logfile, f"Warning: Column '{col}' not found in {translations_path}", log_type="Warn")
                      status_df[col] = np.nan # Add as empty column if missing

            # Initialize new status/licence columns with NaN
            for col in STATUS_COLUMNS + LICENCE_COLUMNS:
                status_df[col] = np.nan

            # Ensure translationId is the index for easier merging later
            # status_df.set_index('translationId', inplace=True) # Let's keep it as a column for now

        except Exception as e:
            log_and_print(logfile, f"Error creating status file from {translations_path}: {e}", log_type="Critical")
            sys.exit(1)

    # --- Merge upstream changes (optional but recommended) ---
    # This adds new translations from translations.csv to status.csv
    # It does NOT update existing rows in status.csv from translations.csv by default
    try:
        trans_df = pd.read_csv(translations_path, dtype={'translationId': str}, keep_default_na=False, na_values=[''])
        if 'translationId' not in trans_df.columns:
             raise ValueError("Translations file missing 'translationId' column during merge check.")

        existing_ids = set(status_df['translationId'].astype(str))
        upstream_ids = set(trans_df['translationId'].astype(str))
        new_ids = list(upstream_ids - existing_ids)

        if new_ids:
            log_and_print(logfile, f"Found {len(new_ids)} new translations in {translations_path}. Adding to status.")
            new_rows_df = trans_df[trans_df['translationId'].isin(new_ids)].copy()

            # Prepare new rows with all status columns, initializing non-original ones
            full_new_rows = pd.DataFrame(columns=ALL_STATUS_COLUMNS)
            for col in ORIGINAL_COLUMNS:
                 if col in new_rows_df.columns:
                      full_new_rows[col] = new_rows_df[col]
                 else:
                      full_new_rows[col] = np.nan
            for col in STATUS_COLUMNS + LICENCE_COLUMNS:
                 full_new_rows[col] = np.nan

            status_df = pd.concat([status_df, full_new_rows], ignore_index=True)
            # Consider saving immediately after adding new rows?
            # status_df.to_csv(status_path, index=False)

        removed_ids = list(existing_ids - upstream_ids)
        if removed_ids:
             log_and_print(logfile, f"Warning: {len(removed_ids)} translations exist in status but not in upstream {translations_path}. They will be kept but may be outdated.", log_type="Warn")
             # Optionally, mark them as inactive or remove them:
             # status_df = status_df[~status_df['translationId'].isin(removed_ids)]

    except Exception as e:
        log_and_print(logfile, f"Error merging upstream changes from {translations_path}: {e}", log_type="Error")

    # Ensure data types are reasonable (especially for boolean checks later)
    status_df['Redistributable'] = status_df['Redistributable'].astype(str).str.lower() == 'true'
    status_df['downloadable'] = status_df['downloadable'].astype(str).str.lower() == 'true'
    # Convert verse counts safely to numeric, coercing errors to NaN, then fill NaN with 0
    for col in ['OTverses', 'NTverses']:
        status_df[col] = pd.to_numeric(status_df[col], errors='coerce').fillna(0).astype(int)

    return status_df


def filter_translations(df: pd.DataFrame, allow_non_redistributable: bool, verse_threshold: int, regex_filter: Optional[str], logfile: Path) -> pd.DataFrame:
    """Filters the DataFrame based on criteria."""
    initial_count = len(df)
    log_and_print(logfile, f"Initial translations in status file: {initial_count}")

    # 1. Filter by downloadable flag
    df = df[df['downloadable'] == True]
    log_and_print(logfile, f"Translations after 'downloadable' filter: {len(df)}")

    # 2. Filter by redistributable flag (if applicable)
    if not allow_non_redistributable:
        df = df[df['Redistributable'] == True]
        log_and_print(logfile, f"Translations after 'Redistributable' filter: {len(df)}")

    # 3. Filter by verse count
    df = df[(df['OTverses'] + df['NTverses']) >= verse_threshold]
    log_and_print(logfile, f"Translations after verse count filter (>= {verse_threshold}): {len(df)}")

    # 4. Apply regex filter (if provided)
    if regex_filter:
        try:
            df = df[df['translationId'].astype(str).str.match(regex_filter, na=False)]
            log_and_print(logfile, f"Translations after regex filter ('{regex_filter}'): {len(df)}")
        except regex.error as e:
            log_and_print(logfile, f"Invalid regex filter '{regex_filter}': {e}. Skipping filter.", log_type="Error")

    final_count = len(df)
    log_and_print(logfile, f"Filtered down to {final_count} translations to process.")
    return df


def determine_actions(df: pd.DataFrame, max_age_days: int, force_download: bool, downloads_folder: Path, projects_folder: Path, private_projects_folder: Path) -> pd.DataFrame:
    """Adds boolean columns indicating required actions."""

    df['action_needed_download'] = False
    df['action_needed_unzip'] = False
    df['action_needed_licence'] = False

    for index, row in df.iterrows():
        # --- Download Check ---
        needs_download = False
        if force_download:
            needs_download = True
        elif is_date_older_than(row['status_download_date'], max_age_days):
            needs_download = True
        elif pd.isna(row['status_download_path']) or not Path(row['status_download_path']).exists():
             # Check if file exists only if date is recent
             needs_download = True

        df.loc[index, 'action_needed_download'] = needs_download

        # --- Unzip Check ---
        needs_unzip = False
        if needs_download: # If downloading, must unzip
            needs_unzip = True
        elif force_download: # Force implies re-unzip too
            needs_unzip = True
        elif is_date_older_than(row['status_unzip_date'], max_age_days):
            needs_unzip = True
        elif pd.isna(row['status_unzip_path']) or not Path(row['status_unzip_path']).exists():
             # Check if dir exists only if date is recent
             needs_unzip = True

        df.loc[index, 'action_needed_unzip'] = needs_unzip

        # --- Licence Check ---
        needs_licence = False
        if needs_unzip: # If unzipping, must re-check licence
            needs_licence = True
        elif force_download: # Force implies re-check
            needs_licence = True
        elif is_date_older_than(row['licence_date_read'], max_age_days):
            needs_licence = True
        # No path check needed here, as licence data is in the status file itself

        df.loc[index, 'action_needed_licence'] = needs_licence

    return df


def download_required_files(df: pd.DataFrame, base_url: str, folder: Path, logfile: Path) -> pd.DataFrame:
    """Downloads files marked with action_needed_download."""
    translations_to_download = df[df['action_needed_download']]
    count = len(translations_to_download)
    log_and_print(logfile, f"Attempting to download {count} zip files...")

    downloaded_count = 0
    for index, row in tqdm(translations_to_download.iterrows(), total=count, desc="Downloading"):
        translation_id = row['translationId']
        url = f"{base_url}{translation_id}_usfm.zip"
        # Always use today's date for new downloads
        local_filename = build_zip_filename(translation_id, TODAY_STR)
        local_path = folder / local_filename

        log_and_print(logfile, f"Downloading {url} to {local_path}")
        if download_url_to_file(url, local_path):
            df.loc[index, 'status_download_path'] = str(local_path.resolve())
            df.loc[index, 'status_download_date'] = TODAY_STR
            df.loc[index, 'status_last_error'] = np.nan # Clear previous error on success
            downloaded_count += 1
            log_and_print(logfile, f"Success: Saved {url} as {local_path}")
            sleep(randint(1, 3000) / 1000) # Shorter sleep?
        else:
            df.loc[index, 'status_download_path'] = np.nan # Clear path on failure
            df.loc[index, 'status_download_date'] = np.nan # Clear date on failure
            df.loc[index, 'status_last_error'] = f"Download failed: {url}"
            log_and_print(logfile, f"Failed: Could not download {url}", log_type="Error")

    log_and_print(logfile, f"Finished downloading. Successfully downloaded {downloaded_count}/{count} files.")
    return df


def unzip_and_process_files(df: pd.DataFrame, downloads_folder: Path, projects_folder: Path, private_projects_folder: Path, vrs_diffs_data: Dict, logfile: Path) -> pd.DataFrame:
    """Unzips, renames, creates settings, and extracts licence for required projects."""
    translations_to_unzip = df[df['action_needed_unzip']]
    count = len(translations_to_unzip)
    log_and_print(logfile, f"Attempting to unzip and process {count} projects...")

    processed_count = 0
    for index, row in tqdm(translations_to_unzip.iterrows(), total=count, desc="Unzipping/Processing"):
        translation_id = row['translationId']
        lang_code = row['languageCode']
        is_redist = row['Redistributable']
        download_path_str = row['status_download_path']

        if pd.isna(download_path_str):
            log_and_print(logfile, f"Skipping unzip for {translation_id}: No valid download path found.", log_type="Warn")
            df.loc[index, 'status_last_error'] = "Unzip skipped: Missing download path"
            continue

        download_path = Path(download_path_str)
        if not download_path.exists():
             log_and_print(logfile, f"Skipping unzip for {translation_id}: Download path {download_path} not found.", log_type="Warn")
             df.loc[index, 'status_last_error'] = f"Unzip skipped: Download not found at {download_path}"
             continue

        unzip_base_dir = projects_folder if is_redist else private_projects_folder
        proj_name = create_project_name(lang_code, translation_id)
        project_dir = unzip_base_dir / proj_name

        log_and_print(logfile, f"Processing {translation_id}: Unzipping {download_path} to {project_dir}")

        # --- Unzip ---
        try:
            # Clean existing directory before unzipping
            if project_dir.exists():
                log_and_print(logfile, f"Removing existing directory: {project_dir}")
                shutil.rmtree(project_dir)
            project_dir.mkdir(parents=True, exist_ok=True)

            shutil.unpack_archive(download_path, project_dir)
            df.loc[index, 'status_unzip_path'] = str(project_dir.resolve())
            df.loc[index, 'status_unzip_date'] = TODAY_STR
            df.loc[index, 'status_last_error'] = np.nan # Clear error on successful unzip
            log_and_print(logfile, f"Successfully unzipped to {project_dir}")

            # --- Post-Unzip Processing ---
            # Rename USFM files
            rename_usfm(project_dir, logfile)

            # Write Settings.xml
            write_settings_file(project_dir, lang_code, translation_id, vrs_diffs_data)

            # Extract Licence Details (only if needed or forced)
            if row['action_needed_licence']:
                 df = get_and_update_licence_details(df, index, project_dir, logfile)

            processed_count += 1

        except (shutil.ReadError, FileNotFoundError, OSError, Exception) as e:
            log_and_print(logfile, f"Error processing {translation_id} at {project_dir}: {e}", log_type="Error")
            df.loc[index, 'status_unzip_path'] = np.nan
            df.loc[index, 'status_unzip_date'] = np.nan
            df.loc[index, 'status_last_error'] = f"Processing error: {e}"
            # Clean up potentially corrupted unzip dir
            if project_dir.exists():
                 try:
                      shutil.rmtree(project_dir)
                 except OSError as rm_e:
                      log_and_print(logfile, f"Could not remove failed unzip dir {project_dir}: {rm_e}", log_type="Warn")

    log_and_print(logfile, f"Finished processing. Successfully processed {processed_count}/{count} projects.")
    return df


def rename_usfm(project_dir: Path, logfile: Path):
    """Renames USFM files within the project directory."""
    log_and_print(logfile, f"Renaming USFM files in {project_dir}")
    renamed_count = 0
    try:
        usfm_paths = list(project_dir.glob("*.usfm"))
        for old_usfm_path in usfm_paths:
            new_sfm_path = get_destination_file_from_book(old_usfm_path)
            if new_sfm_path == old_usfm_path:
                continue
            if new_sfm_path.is_file():
                new_sfm_path.unlink() # Remove existing target

            # log_and_print(logfile, f"Renaming {old_usfm_path.name} to {new_sfm_path.name}")
            old_usfm_path.rename(new_sfm_path)
            renamed_count += 1
        if renamed_count > 0:
             log_and_print(logfile, f"Renamed {renamed_count} USFM files.")
    except Exception as e:
        log_and_print(logfile, f"Error renaming USFM files in {project_dir}: {e}", log_type="Error")


def get_and_update_licence_details(df: pd.DataFrame, index, project_dir: Path, logfile: Path) -> pd.DataFrame:
    """Extracts licence details from copr.htm and updates the DataFrame row."""
    copyright_path = project_dir / "copr.htm"
    log_and_print(logfile, f"Extracting licence info for {project_dir.name} from {copyright_path}")

    # Clear previous licence data for this row first
    for col in LICENCE_COLUMNS:
        if col != 'licence_date_read': # Keep date read until success
             df.loc[index, col] = np.nan

    if not copyright_path.exists():
        log_and_print(logfile, f"Unable to find {copyright_path}", log_type="Warn")
        df.loc[index, 'status_last_error'] = f"Licence check failed: copr.htm not found"
        df.loc[index, 'licence_date_read'] = TODAY_STR # Mark as checked today, even if failed
        return df

    entry = {} # Use a temporary dict
    entry["licence_ID"] = project_dir.name
    entry["licence_File"] = str(copyright_path.resolve())

    try:
        with open(copyright_path, "r", encoding="utf-8") as copr:
            html = copr.read()
            soup = BeautifulSoup(html, "lxml")

        cclink = soup.find(href=regex.compile("creativecommons"))
        if cclink:
            ref = cclink.get("href")
            if ref:
                entry["licence_CC_Licence_Link"] = ref
                # More robust CC parsing
                cc_match = regex.search(r"/licenses/([a-z\-]+)/([\d\.]+)", ref)
                if cc_match:
                    entry["licence_Licence_Type"] = cc_match.group(1)
                    entry["licence_Licence_Version"] = cc_match.group(2)
                else: # Handle simpler cases like /by/4.0/
                     cc_match_simple = regex.search(r"/licenses/([a-z\-]+)/?", ref)
                     if cc_match_simple:
                          entry["licence_Licence_Type"] = cc_match_simple.group(1)
                          # Try to find version elsewhere if needed

        titlelink = soup.find(href=regex.compile(f"https://ebible.org/{entry['licence_ID']}"))
        if titlelink and titlelink.string:
            entry["licence_Vernacular_Title"] = titlelink.string.strip()

        # Extract text, handle potential missing <p> or body
        body_tag = soup.body
        if body_tag and body_tag.p:
             copy_strings = [s.strip() for s in body_tag.p.stripped_strings if s.strip()]
        elif body_tag:
             copy_strings = [s.strip() for s in body_tag.stripped_strings if s.strip()]
        else:
             copy_strings = []
             log_and_print(logfile, f"Warning: No body or paragraph tag found in {copyright_path}", log_type="Warn")


        # Simpler text parsing logic
        is_public_domain = False
        for i, text in enumerate(copy_strings):
            if "public domain" in text.lower():
                is_public_domain = True
                break # Assume PD overrides other info
            elif "copyright Â©" in text.lower():
                 entry["licence_Copyright_Years"] = text # Keep full string for now
                 if i + 1 < len(copy_strings):
                      entry["licence_Copyright_Holder"] = copy_strings[i+1]
            elif text.lower().startswith("language:"):
                 if i + 1 < len(copy_strings):
                      entry["licence_Language"] = copy_strings[i+1]
            elif text.lower().startswith("dialect"): # Handles "Dialect:" or "Dialect (if applicable):"
                 # Take rest of string after colon, or the next string if current is just "Dialect:"
                 parts = text.split(":", 1)
                 if len(parts) > 1 and parts[1].strip():
                      entry["licence_Dialect"] = parts[1].strip()
                 elif i + 1 < len(copy_strings):
                      entry["licence_Dialect"] = copy_strings[i+1]
            elif "translation by" in text.lower():
                 entry["licence_Translation_by"] = text # Keep full string

        if is_public_domain:
            entry["licence_Copyright_Holder"] = "Public Domain"
            entry["licence_Licence_Type"] = "Public Domain" # Standardize
            entry["licence_Copyright_Years"] = "" # Clear years for PD

        # --- Data Cleaning/Defaults ---
        if pd.isna(entry.get("licence_Licence_Type")):
             if "Public Domain" == entry.get("licence_Copyright_Holder"):
                  entry["licence_Licence_Type"] = "Public Domain"
             elif entry.get("licence_CC_Licence_Link"):
                  entry["licence_Licence_Type"] = "CC (Unknown Version)" # Indicate CC link exists but type/version parse failed
             else:
                  entry["licence_Licence_Type"] = "Unknown" # Default if no other info

        # Apply specific known fixes (example)
        if entry["licence_ID"] in ["engwmb", "engwmbb"]:
             entry["licence_Copyright_Holder"] = "Public Domain"
             entry["licence_Licence_Type"] = "Public Domain"

        # Update DataFrame row
        for col_suffix, value in entry.items():
             # col_name = f"licence_{col_suffix}" # Prefix already included in entry keys
             if col_suffix in df.columns:
                  df.loc[index, col_suffix] = value
             else:
                  log_and_print(logfile, f"Warning: Licence key '{col_suffix}' not a column in DataFrame.", log_type="Warn")

        df.loc[index, 'licence_date_read'] = TODAY_STR
        df.loc[index, 'status_last_error'] = np.nan # Clear error on success
        log_and_print(logfile, f"Successfully extracted licence info for {project_dir.name}")

    except Exception as e:
        log_and_print(logfile, f"Error parsing licence file {copyright_path}: {e}", log_type="Error")
        df.loc[index, 'status_last_error'] = f"Licence parse error: {e}"
        df.loc[index, 'licence_date_read'] = TODAY_STR # Mark as checked today, even if failed

    return df


# --- Main Execution ---

def main() -> None:
    load_dotenv()

    parser: argparse.ArgumentParser = argparse.ArgumentParser(
        description="Download, unzip and process eBible translations using a status file."
    )
    parser.add_argument(
        "-f", "--filter", default=None,
        help="Regex filter for translationId (e.g., 'eng-.*' or '^(fra|spa)')",
    )
    parser.add_argument(
        "-d", "--force_download", default=False, action="store_true",
        help="Force download, unzip, and licence check for all filtered translations.",
    )
    parser.add_argument(
        "--allow_non_redistributable", default=False, action="store_true",
        help="Include non-redistributable (private) translations.",
    )
    # --download_only might need rethinking with status file, maybe remove or adapt?
    # parser.add_argument(
    #     "--download_only", default=False, action="store_true",
    #     help="Stop after downloading zip files.",
    # )
    parser.add_argument(
        "--max-age-days", default=None, type=int, # Default handled later
        help="Max age in days for downloaded/unzipped files before re-processing. Overrides .env.",
    )
    parser.add_argument(
        "--base-folder", default=None,
        help="Override base folder location (defaults to EBIBLE_DATA_DIR from .env or './_ebible_data').",
    )
    parser.add_argument(
        "--verse-threshold", default=400, type=int,
        help="Minimum total OT+NT verses required for a translation to be processed.",
    )
    args: argparse.Namespace = parser.parse_args()

    # --- Determine Base Path ---
    if args.base_folder:
        base = Path(args.base_folder).resolve()
        print(f"Using base folder from command line: {base}")
    elif os.getenv("EBIBLE_DATA_DIR"):
        base = Path(os.getenv("EBIBLE_DATA_DIR")).resolve()
        print(f"Using base folder from EBIBLE_DATA_DIR env var: {base}")
    else:
        # Default relative to the script's location might be safer
        base = (Path(__file__).parent / "_ebible_data").resolve()
        # repo_root = Path(__file__).parent.parent.parent.parent # Old assumption
        # base = repo_root / "_ebible_data"
        print(f"Using default base folder: {base}")

    # --- Define Paths ---
    corpus_folder: Path = base / "corpus"
    downloads_folder: Path = base / "downloads"
    private_corpus_folder: Path = base / "private_corpus"
    private_projects_folder: Path = base / "private_projects"
    projects_folder: Path = base / "projects"
    metadata_folder: Path = base / "metadata"
    logs_folder: Path = base / "logs"

    # --- Setup Logging ---
    logs_folder.mkdir(parents=True, exist_ok=True) # Ensure log dir exists first
    year, month, day, hour, minute = map(int, strftime("%Y %m %d %H %M").split())
    log_suffix: str = f"_{year}_{month:02d}_{day:02d}-{hour:02d}_{minute:02d}.log"
    log_filename: str = "ebible_status" + log_suffix
    logfile: Path = logs_folder / log_filename
    print(f"Logging to: {logfile}")

    # --- Check Folders ---
    required_folders = [
        corpus_folder, downloads_folder, private_corpus_folder,
        private_projects_folder, projects_folder, metadata_folder, logs_folder
    ]
    check_folders_exist(required_folders, base, logfile)

    # --- Determine Max Age ---
    max_age_days = args.max_age_days
    if max_age_days is None:
        env_max_age = os.getenv("MAX_AGE_DAYS")
        if env_max_age and env_max_age.isdigit():
            max_age_days = int(env_max_age)
            log_and_print(logfile, f"Using MAX_AGE_DAYS={max_age_days} from .env file.")
        else:
            max_age_days = 365 # Hardcoded default
            log_and_print(logfile, f"Using default max_age_days={max_age_days}.")
    else:
        log_and_print(logfile, f"Using --max-age-days={max_age_days} from command line.")

    # --- Download translations.csv if needed ---
    translations_csv_url: str = r"https://ebible.org/Scriptures/translations.csv"
    translations_csv: Path = metadata_folder / TRANSLATIONS_FILENAME
    if not translations_csv.is_file() or args.force_download:
        log_and_print(logfile, f"Downloading {translations_csv_url} to {translations_csv}")
        if not download_url_to_file(translations_csv_url, translations_csv):
            log_and_print(logfile, f"Critical: Failed to download {translations_csv}. Aborting.", log_type="Critical")
            sys.exit(1)
    else:
        log_and_print(logfile, f"{translations_csv} already exists.")

    # --- Load or Initialize Status ---
    status_path = metadata_folder / STATUS_FILENAME
    status_df = initialize_or_load_status(status_path, translations_csv, logfile)

    # --- Filter Translations ---
    filtered_df = filter_translations(
        status_df,
        args.allow_non_redistributable,
        args.verse_threshold,
        args.filter,
        logfile
    )

    if filtered_df.empty:
        log_and_print(logfile, "No translations match the specified filters. Exiting.")
        # Save status file even if empty? Maybe not necessary.
        # filtered_df.to_csv(status_path, index=False)
        sys.exit(0)

    # --- Determine Actions ---
    actions_df = determine_actions(
        filtered_df, max_age_days, args.force_download,
        downloads_folder, projects_folder, private_projects_folder
    )

    # --- Execute Actions ---
    eBible_url: str = r"https://ebible.org/Scriptures/"

    # Download
    actions_df = download_required_files(actions_df, eBible_url, downloads_folder, logfile)

    # # Option to stop after download (re-evaluate if needed)
    # if args.download_only:
    #     log_and_print(logfile, "Stopping after download phase as requested.")
    #     # Save status now
    #     status_df.update(actions_df) # Update the main df with changes
    #     status_df.to_csv(status_path, index=False)
    #     log_and_print(logfile, f"Saved updated status to {status_path}")
    #     sys.exit(0)

    # Unzip, Rename, Settings, Licence
    vrs_diffs_data = get_vrs_diffs()
    actions_df = unzip_and_process_files(
        actions_df, downloads_folder, projects_folder,
        private_projects_folder, vrs_diffs_data, logfile
    )

    # --- Update Main Status DataFrame and Save ---
    # Use update() which aligns on index (translationId if set, otherwise row number)
    # Ensure index is set correctly if needed, or update based on 'translationId' column
    status_df.set_index('translationId', inplace=True, drop=False) # Set index temporarily
    actions_df.set_index('translationId', inplace=True, drop=False)
    status_df.update(actions_df)
    status_df.reset_index(drop=True, inplace=True) # Remove index before saving

    try:
        status_df.to_csv(status_path, index=False)
        log_and_print(logfile, f"\nSaved updated status for {len(status_df)} translations to {status_path}")
    except Exception as e:
        log_and_print(logfile, f"Error saving status file {status_path}: {e}", log_type="Error")

    # --- Final Info and Commands ---
    log_and_print(logfile, "\nLicence Type Summary (Processed Translations):")
    # Filter actions_df for successfully processed ones if needed, or show all filtered
    log_and_print(logfile, actions_df['licence_Licence_Type'].value_counts(dropna=False))

    # Define extract log paths using the same suffix
    public_extract_log: Path = logs_folder / ("extract_public" + log_suffix)
    private_extract_log: Path = logs_folder / ("extract_private" + log_suffix)

    log_and_print(
        logfile,
        [
            "\n--- Next Step: Bulk Extraction ---",
            "Use SILNLP's bulk_extract_corpora tool.",
            "Ensure you have SILNLP installed and configured (e.g., via poetry).",
            "\nCommand for PRIVATE projects:",
            f"poetry run python -m silnlp.common.bulk_extract_corpora --input \"{private_projects_folder}\" --output \"{private_corpus_folder}\" --error-log \"{private_extract_log}\"",
            "\nCommand for PUBLIC projects:",
            f"poetry run python -m silnlp.common.bulk_extract_corpora --input \"{projects_folder}\" --output \"{corpus_folder}\" --error-log \"{public_extract_log}\"",
            "\n---------------------------------"
        ],
    )


if __name__ == "__main__":
    main()
```

**Explanation of Key Changes:**

1.  **File Renaming:** The script is saved as `ebible_status.py`.
2.  **Status File Handling (`initialize_or_load_status`):**
   *   Defines `STATUS_FILENAME` (`ebible_status.csv`).
   *   Defines all expected columns (`ALL_STATUS_COLUMNS`).
   *   Tries to load `ebible_status.csv` using pandas.
   *   If it exists, it checks for missing columns and adds them.
   *   If it *doesn't* exist (or fails to load), it reads `translations.csv`, creates a new DataFrame with all columns, populates original data, and initializes new columns with `NaN`.
   *   **Merging Upstream Changes:** It compares `translationId`s between the loaded status and the current `translations.csv`. New IDs found upstream are added as new rows to the status DataFrame. It logs a warning for IDs present in status but missing upstream.
   *   Ensures basic data types (`Redistributable`, `downloadable`, verse counts) are correct.
3.  **Argument Parsing:**
   *   `--max_zip_age_days` is renamed to `--max-age-days`.
   *   The default for `--max-age-days` is set to `None`, and logic is added in `main` to check the `.env` variable `MAX_AGE_DAYS` if the argument isn't provided, falling back to 365.
   *   `--verse-threshold` argument added.
4.  **Filtering (`filter_translations`):**
   *   This function now takes the status DataFrame and applies filters based on `downloadable`, `Redistributable`, verse count (`--verse-threshold`), and the optional `--filter` regex.
5.  **Determining Actions (`determine_actions`):**
   *   This new function iterates through the filtered DataFrame.
   *   For each row, it checks `status_download_date`, `status_unzip_date`, `licence_date_read` against `max_age_days`.
   *   It also considers `force_download` and checks if required paths (`status_download_path`, `status_unzip_path`) actually exist if the dates are recent.
   *   It adds boolean columns (`action_needed_download`, `action_needed_unzip`, `action_needed_licence`) to the DataFrame.
6.  **Downloading (`download_required_files`):**
   *   Takes the DataFrame with action flags.
   *   Filters for rows where `action_needed_download` is True.
   *   Iterates and downloads, updating `status_download_path`, `status_download_date`, and `status_last_error` directly in the DataFrame upon success or failure.
7.  **Unzipping and Processing (`unzip_and_process_files`):**
   *   Filters for rows where `action_needed_unzip` is True.
   *   Checks if the required `status_download_path` exists.
   *   Removes the existing project directory before unzipping to ensure freshness.
   *   Unzips the archive.
   *   Updates `status_unzip_path` and `status_unzip_date`.
   *   Calls `rename_usfm`.
   *   Calls `write_settings_file`.
   *   Calls `get_and_update_licence_details` **only if** `action_needed_licence` is True for that row.
   *   Updates `status_last_error` on failure.
8.  **Licence Extraction (`get_and_update_licence_details`):**
   *   This function is significantly refactored.
   *   It now takes the main DataFrame and the specific `index` of the row to update.
   *   It clears previous licence data for that row.
   *   It parses `copr.htm`.
   *   It updates the `licence_*` columns and `licence_date_read` directly in the DataFrame at the given `index`.
   *   Includes basic data cleaning and handling for "Public Domain".
   *   The separate `licences.tsv` file and `write_licence_file` function are removed.
9.  **Saving Status:**
   *   At the end of `main`, the potentially modified `status_df` (updated using `df.update()`) is saved back to `ebible_status.csv`. `df.update()` is used to merge the changes from the processed subset (`actions_df`) back into the main DataFrame.
10. **Helper Functions:**
   *   `get_translations` is removed as its logic is integrated into `initialize_or_load_status` and `filter_translations`.
   *   `find_recent_download` is removed as the download path is now directly tracked in the status file.
   *   `write_licence_file` is removed.
11. **Logging:** Logging messages are updated to reflect the new workflow.
12. **Error Handling:** Basic error handling is added (e.g., `status_last_error` column, try/except blocks around file operations).

This script should provide the desired stateful processing, avoiding redundant work based on the status file and the `--max-age-days` setting. Remember to install pandas (`pip install pandas`) if you haven't already.
Use code with care. Learn more
Rate this answer: 
External Sources (1)
Context Sources (2)

David Baines
It would be ideal to add logic that will update the ebible_status.csv file from the files that already exist in the ebible_data folders. It will be much quicker to check for the data of a downloaded file than to re-download it. We can look for expected files in the ebible_data\downloads folder and update their download date based on the file modified date.

Similarly for the ebible_data\projects that contains the unzipped files. Though there we probably want to use the file modified date of the majority of files in the folder. That should be a robust enough approach.

Also for the extracted files that are in ebible_data/corpus we can use the file modified date as the date of the last extraction.

Can you update the script to do these checks to cope with cases where the ebible_data folders already exist and contain data, while keeping the fall back for cases where those don't exist and all the steps need to be run?


Gemini
Okay, that's a great idea! Scanning the existing data folders to populate or update the ebible_status.csv makes the script much more resilient, especially when run on a system where data might already exist but the status file is missing or incomplete.

Here's how we can modify ebible_status.py to incorporate this scanning logic:

Create Helper Scan Functions: We'll add functions to check the downloads, projects/private_projects, and corpus/private_corpus folders for existing artifacts and their modification dates.
Integrate Scanning into initialize_or_load_status: After the status DataFrame is loaded or initially created, we'll iterate through it. For any translation missing status information (like download date), we'll call the appropriate scan function to try and populate it from the filesystem.
Pass Folder Paths: We need to pass the relevant folder paths (downloads_folder, projects_folder, etc.) into initialize_or_load_status.
Let's implement these changes.

diff
-0
+123
The code change produced by Gemini could not be fully parsed. Can't automatically accept changes.
        else:
             print(f"Log folder {logfile.parent} does not exist, cannot log folder check.")

# --- Scanning Functions for Existing Data ---

def scan_download_folder(translation_id: str, downloads_folder: Path) -> Optional[tuple[str, str]]:
    """Scans downloads folder for the latest zip file for a translationId."""
    potential_zips: List[Path] = sorted(
        downloads_folder.glob(f"{translation_id}--*-*-*.zip"),
        key=os.path.getmtime, # Sort by modification time
        reverse=True # Latest first
    )

    if not potential_zips:
        return None

    latest_zip = potential_zips[0]
    try:
        # Extract date from filename if possible, otherwise use file mtime
        match = regex.search(r"--(\d{4}-\d{2}-\d{2})\.zip$", latest_zip.name)
        if match:
            date_str = match.group(1)
        else:
            # Fallback to file modification time
            mtime = latest_zip.stat().st_mtime
            date_str = datetime.fromtimestamp(mtime, tz=timezone.utc).date().isoformat()
        return str(latest_zip.resolve()), date_str
    except Exception:
        # If anything goes wrong, fallback to mtime
        mtime = latest_zip.stat().st_mtime
        date_str = datetime.fromtimestamp(mtime, tz=timezone.utc).date().isoformat()
        return str(latest_zip.resolve()), date_str

def scan_project_folder(project_path: Path) -> Optional[tuple[str, str]]:
    """Checks if a project folder exists and returns its path and modification date."""
    if project_path.is_dir():
        try:
            # Use the folder's modification time as a proxy for unzip date
            mtime = project_path.stat().st_mtime
            date_str = datetime.fromtimestamp(mtime, tz=timezone.utc).date().isoformat()
            return str(project_path.resolve()), date_str
        except Exception:
            return None # Error getting stat
    return None

def scan_corpus_file(extract_path: Path) -> Optional[tuple[str, str]]:
    """Checks if an extracted corpus file exists and returns its path and modification date."""
    if extract_path.is_file():
        try:
            mtime = extract_path.stat().st_mtime
            date_str = datetime.fromtimestamp(mtime, tz=timezone.utc).date().isoformat()
            return str(extract_path.resolve()), date_str
        except Exception:
            return None # Error getting stat
    return None
# --- Core Logic Functions ---

def initialize_or_load_status(status_path: Path, translations_path: Path, logfile: Path) -> pd.DataFrame:
Unchanged lines
    status_df['downloadable'] = status_df['downloadable'].astype(str).str.lower() == 'true'
    # Convert verse counts safely to numeric, coercing errors to NaN, then fill NaN with 0
    for col in ['OTverses', 'NTverses']:
        status_df[col] = pd.to_numeric(status_df[col], errors='coerce').fillna(0).astype(int)

    return status_df


def scan_and_update_status(
    status_df: pd.DataFrame,
    downloads_folder: Path,
    projects_folder: Path,
    private_projects_folder: Path,
    corpus_folder: Path,
    private_corpus_folder: Path,
    logfile: Path
) -> pd.DataFrame:
    """Scans data folders to update status DataFrame for entries with missing info."""
    log_and_print(logfile, "Scanning existing data folders to update status file...")
    updated_count = 0
    for index, row in tqdm(status_df.iterrows(), total=len(status_df), desc="Scanning Folders"):
        translation_id = row['translationId']
        lang_code = row['languageCode']
        is_redist = row['Redistributable'] # Assumes this column is correctly typed bool

        # Scan Downloads
        if pd.isna(row['status_download_date']):
            scan_result = scan_download_folder(translation_id, downloads_folder)
            if scan_result:
                status_df.loc[index, 'status_download_path'] = scan_result[0]
                status_df.loc[index, 'status_download_date'] = scan_result[1]
                updated_count += 1

        # Scan Projects (Unzipped)
        if pd.isna(row['status_unzip_date']):
            proj_base = projects_folder if is_redist else private_projects_folder
            proj_name = create_project_name(lang_code, translation_id)
            project_dir = proj_base / proj_name
            scan_result = scan_project_folder(project_dir)
            if scan_result:
                status_df.loc[index, 'status_unzip_path'] = scan_result[0]
                status_df.loc[index, 'status_unzip_date'] = scan_result[1]
                updated_count += 1

        # Scan Corpus (Extracted) - Note: Extract path is less certain, depends on SILNLP output format
        # This part might need adjustment if SILNLP naming changes. Assuming standard {lang}-{proj_name}.txt
        # Also, status_extract_path/date might be less critical now, but we can scan for it.
        if pd.isna(row['status_extract_date']):
            proj_name = create_project_name(lang_code, translation_id) # Recalculate needed here
            corpus_base = corpus_folder if is_redist else private_corpus_folder
            # Construct expected extract filename - THIS IS AN ASSUMPTION
            expected_extract_filename = f"{lang_code}-{proj_name}.txt"
            extract_path = corpus_base / expected_extract_filename
            scan_result = scan_corpus_file(extract_path)
            if scan_result:
                status_df.loc[index, 'status_extract_path'] = scan_result[0]
                status_df.loc[index, 'status_extract_date'] = scan_result[1]
                updated_count += 1

    if updated_count > 0:
        log_and_print(logfile, f"Scan complete. Updated status for {updated_count} entries based on existing files.")
    else:
        log_and_print(logfile, "Scan complete. No missing status information updated from existing files.")

    return status_df


Unchanged lines
    status_path = metadata_folder / STATUS_FILENAME
    status_df = initialize_or_load_status(status_path, translations_csv, logfile)

    # --- Scan existing folders to potentially update status ---
    status_df = scan_and_update_status(
        status_df,
        downloads_folder,
        projects_folder,
        private_projects_folder,
        corpus_folder,
        private_corpus_folder,
        logfile
    )
    # --- Filter Translations ---
    filtered_df = filter_translations(
        status_df,
Changes Made:

scan_download_folder function:
Takes translation_id and downloads_folder.
Uses glob to find all matching zip files ({id}--*.zip).
Sorts them by modification time (latest first).
If found, it tries to parse the date from the filename (--YYYY-MM-DD). If that fails, it falls back to using the file's modification timestamp.
Returns the resolved path and the date string (YYYY-MM-DD) or None.
scan_project_folder function:
Takes the full potential project_path.
Checks if the directory exists (is_dir()).
If it exists, it gets the directory's modification time (stat().st_mtime) and converts it to a YYYY-MM-DD date string.
Returns the resolved path and the date string or None.
scan_corpus_file function:
Takes the full potential extract_path.
Checks if the file exists (is_file()).
If it exists, it gets the file's modification time and converts it to a YYYY-MM-DD date string.
Returns the resolved path and the date string or None.
scan_and_update_status function:
This new function orchestrates the scanning.
It takes the status_df and all the relevant data folder paths.
It iterates through the DataFrame using tqdm for progress.
For each row, it checks if status_download_date, status_unzip_date, or status_extract_date is NaN.
If a date is missing, it calls the corresponding scan function (scan_download_folder, scan_project_folder, scan_corpus_file).
It reconstructs the expected project_dir and extract_path based on the row's data (languageCode, translationId, Redistributable). Note: The extract_path construction assumes the standard SILNLP output format ({lang}-{proj_name}.txt).
If a scan function returns a result, it updates the corresponding _path and _date columns in the DataFrame for that row using .loc.
Logs how many entries were updated.
main function:
After initialize_or_load_status is called, the new scan_and_update_status function is called, passing the status_df and the folder paths. The returned (potentially updated) DataFrame replaces the old one.
Now, when the script runs, it will first load or create the status file, then attempt to fill in any missing status dates by looking at the actual files and folders in your _ebible_data directory before proceeding with determining which actions (download, unzip, etc.) are needed.

Rate this answer: 
Context Sources (3)

David Baines
This is really a great improvement. Now we need to check the logic to ensure that all the steps are completed.

After the scan and update of the status. Then the code would need to look at which files still need to be downloaded and download and unzip them and update the status. If the licence_date_read is empty then the licence file should be read so that the licence_* fields can be updated.

Lastly it would be good to report which extract files are missing, these are the ones with no status_extract_date. The status_extract_path should be calculated and completed in the ebible_status.csv


Gemini
Okay, let's refine the ebible_status.py script to ensure the logic flows correctly after the scan, explicitly handle the licence check trigger, and add reporting for missing extracted files.

1. Ensuring status_extract_path is Populated:

We'll add a step after loading/initializing and scanning the status file to calculate and populate the status_extract_path for all relevant entries. This makes the path readily available for reporting later.

2. Reporting Missing Extracts:

At the end of the main function, after saving the updated status, we'll add logic to filter the DataFrame and report translations where the status_extract_date is still missing (indicating the .txt file wasn't found or hasn't been created).

Here are the code changes:

diff
-0
+38
The code change produced by Gemini could not be fully parsed. Can't automatically accept changes.
    return status_df


def ensure_extract_paths(
    status_df: pd.DataFrame,
    corpus_folder: Path,
    private_corpus_folder: Path,
    logfile: Path
) -> pd.DataFrame:
    """Calculates and fills the status_extract_path column if missing."""
    log_and_print(logfile, "Ensuring status_extract_path is populated...")
    for index, row in status_df.iterrows():
        if pd.isna(row['status_extract_path']):
            lang_code = row['languageCode']
            translation_id = row['translationId']
            is_redist = row['Redistributable'] # Assumes bool
            proj_name = create_project_name(lang_code, translation_id)
            corpus_base = corpus_folder if is_redist else private_corpus_folder
            expected_extract_filename = f"{lang_code}-{proj_name}.txt"
            status_df.loc[index, 'status_extract_path'] = str((corpus_base / expected_extract_filename).resolve())
    return status_df

def filter_translations(df: pd.DataFrame, allow_non_redistributable: bool, verse_threshold: int, regex_filter: Optional[str], logfile: Path) -> pd.DataFrame:
    """Filters the DataFrame based on criteria."""
    initial_count = len(df)
Unchanged lines
        logfile
    )

    # --- Ensure extract paths are calculated for reporting ---
    status_df = ensure_extract_paths(
        status_df,
        corpus_folder,
        private_corpus_folder,
        logfile
    )
    # --- Filter Translations ---
    filtered_df = filter_translations(
        status_df,
Unchanged lines
    except Exception as e:
        log_and_print(logfile, f"Error saving status file {status_path}: {e}", log_type="Error")

    # --- Report Missing Extracts ---
    # Re-scan corpus folders to update extract dates one last time before reporting
    status_df = scan_and_update_status(status_df, downloads_folder, projects_folder, private_projects_folder, corpus_folder, private_corpus_folder, logfile)

    missing_extracts_df = status_df[status_df['status_extract_date'].isna() & status_df['downloadable'] & ((status_df['OTverses'] + status_df['NTverses']) >= args.verse_threshold)]
    # Apply filters again if needed, or assume we only care about potentially processable ones
    # missing_extracts_df = filter_translations(missing_extracts_df, args.allow_non_redistributable, args.verse_threshold, args.filter, logfile) # Re-filter if strict reporting needed

    if not missing_extracts_df.empty:
        log_and_print(logfile, f"\nWarning: {len(missing_extracts_df)} translations appear to be missing extracted corpus files (.txt):", log_type="Warn")
        for index, row in missing_extracts_df.iterrows():
            log_and_print(logfile, f"  - {row['translationId']}: Expected at {row['status_extract_path']}", log_type="Warn")
    # --- Final Info and Commands ---
    log_and_print(logfile, "\nLicence Type Summary (Processed Translations):")
    # Filter actions_df for successfully processed ones if needed, or show all filtered

Summary of Changes:

ensure_extract_paths Function:
A new function is added to iterate through the status DataFrame.
If status_extract_path is empty for a row, it calculates the expected path based on languageCode, translationId, Redistributable, and the corpus folder paths.
It updates the DataFrame with the calculated path.
Call ensure_extract_paths in main: This function is called after scan_and_update_status to make sure all relevant rows have an expected extract path defined.
Report Missing Extracts in main:
Added a section near the end, after saving the status file.
It re-runs scan_and_update_status one last time. This ensures that if the user ran the SILNLP extraction after the main processing but before this script finished, the status file reflects the newly created .txt files.
It filters the final status_df to find rows where status_extract_date is still null/NaN. It also reapplies basic filters (downloadable, verse threshold) to avoid reporting on translations that wouldn't have been processed anyway.
If missing extracts are found, it logs a warning message listing the count and the translationId / expected path for each missing file.
With these additions, the script now ensures the expected extract path is recorded and provides a clear report at the end about which corpus files seem to be missing, guiding the user on potential next steps (like running the SILNLP extraction). The core logic for triggering downloads, unzips, and licence checks based on dates and flags remains as previously implemented, ensuring those steps are performed when needed.

Rate this answer: 
Context Sources (3)

David Baines
At the moment none of the licence_* fields contain any data in the ebible_status.csv file.
Can you help debug the cause for that?


Gemini
Okay, let's figure out why those licence_* fields aren't getting populated in your ebible_status.csv.

Looking through the code (ebible_status.py), the function get_and_update_licence_details is responsible for parsing the copr.htm file and putting the data into the DataFrame.

The key part is when this function gets called. It's currently located inside the unzip_and_process_files function, specifically within the loop that processes translations marked for unzipping (action_needed_unzip == True). It's further guarded by an if row['action_needed_licence']: check.

python
# Inside unzip_and_process_files loop:
        try:
            # ... (unzip, rename, write_settings) ...

            # Extract Licence Details (only if needed or forced)
            if row['action_needed_licence']:
                 df = get_and_update_licence_details(df, index, project_dir, logfile) # <--- HERE

            processed_count += 1
        # ... (except block) ...
The problem is: if a project's unzipped folder (status_unzip_path) already exists and its date (status_unzip_date) is considered recent (not older than --max-age-days), then action_needed_unzip will be False (unless --force_download is used). Consequently, the entire loop body inside unzip_and_process_files for that translation is skipped, and get_and_update_licence_details is never called, even if action_needed_licence was determined to be True (because licence_date_read was missing or old).

The Fix:

We need to ensure the licence check happens even if the unzipping step is skipped, provided the project directory exists and the licence data is indeed missing or outdated.

We can add a separate step after unzip_and_process_files to specifically handle these cases.

Here's the proposed change:

Create a new function check_and_update_licences: This function will iterate through translations marked for a licence check (action_needed_licence == True) but not marked for unzipping (action_needed_unzip == False). It will verify the project directory exists and then call get_and_update_licence_details.
Call this function in main: Add a call to check_and_update_licences after the call to unzip_and_process_files.
diff
-0
+32
The code change produced by Gemini could not be fully parsed. Can't automatically accept changes.
    return df


def check_and_update_licences(df: pd.DataFrame, logfile: Path) -> pd.DataFrame:
    """Checks and updates licence details for projects that weren't re-unzipped but need a licence check."""
    # Filter for rows needing licence check BUT NOT unzip (as unzip handles its own licence check)
    licence_check_candidates = df[df['action_needed_licence'] & ~df['action_needed_unzip']]
    count = len(licence_check_candidates)

    if count == 0:
        log_and_print(logfile, "No existing projects require a separate licence check.")
        return df

    log_and_print(logfile, f"Performing licence check for {count} existing projects...")
    checked_count = 0
    for index, row in tqdm(licence_check_candidates.iterrows(), total=count, desc="Checking Licences"):
        project_path_str = row['status_unzip_path']

        if pd.isna(project_path_str):
            log_and_print(logfile, f"Skipping licence check for {row['translationId']}: Missing unzip path.", log_type="Warn")
            continue

        project_dir = Path(project_path_str)
        if project_dir.is_dir():
            df = get_and_update_licence_details(df, index, project_dir, logfile)
            checked_count += 1
        else:
            log_and_print(logfile, f"Skipping licence check for {row['translationId']}: Project directory {project_dir} not found.", log_type="Warn")

    log_and_print(logfile, f"Finished separate licence check. Updated {checked_count}/{count} projects.")
    return df

# --- Main Execution ---

def main() -> None:
Unchanged lines
        private_projects_folder, vrs_diffs_data, logfile
    )

    # Perform licence checks for existing projects if needed
    actions_df = check_and_update_licences(actions_df, logfile)

    # --- Update Main Status DataFrame and Save ---
    # Use update() which aligns on index (translationId if set, otherwise row number)
    # Ensure index is set correctly if needed, or update based on 'translationId' column

With this change, the script will first handle downloads and unzips (including licence checks for newly unzipped projects). Then, the check_and_update_licences function will specifically look for projects that weren't unzipped but still need their licence information updated, ensuring that the licence_* fields get populated correctly based on the licence_date_read status and --max-age-days.

Rate this answer: 
Context Sources (3)

David Baines
This looks like it is working well for updating the licence information. Now I need to debug some of the files that don't seem to be getting unzipped or not to the expected folder . Examples are those with translation_ids abt-maprik and abt-wosera. The ebible_status.csv says that the expected unzipped folder name is maprik and wosera respectively. However I think these should be unzipped to abt-maprik and abt-wosera respectively.

The downloaded zipped files don't appear to have been unzipped. Not to abt-maprik or to maprik or to abt (which would be an error as there might be other files with the name abt)

Can you diagnose the reason that some downloaded files have not been unzipped and propose a solution?


Gemini
Okay, let's investigate why those files like abt-maprik might not be getting unzipped correctly.

Based on your description and the code, here's the likely sequence of events and potential points of failure:

Project Name Calculation: The function create_project_name('abt', 'abt-maprik') is called. Because abt-maprik starts with abt-, the function strips this prefix and returns maprik. This is the intended behavior based on the logic inherited from the original ebible.py script, designed to work with how silnlp.common.bulk_extract_corpora handles naming.
Target Directory: The script determines the target directory for unzipping as _ebible_data/projects/maprik (assuming it's redistributable).
Unzip Action Trigger: The determine_actions function decides if unzipping is needed (action_needed_unzip = True). This happens if:
The file was just downloaded.
--force_download is used.
The status_unzip_date is missing or older than --max-age-days.
The target directory (_ebible_data/projects/maprik) doesn't exist, even if the date is recent.
Unzipping Process (unzip_and_process_files): If action_needed_unzip is True, the script attempts to:
Remove the existing _ebible_data/projects/maprik directory if it exists.
Create an empty _ebible_data/projects/maprik directory.
Call shutil.unpack_archive to extract the contents of the downloaded zip (e.g., _ebible_data/downloads/abt-maprik--YYYY-MM-DD.zip) into _ebible_data/projects/maprik.
Perform post-unzip steps (rename_usfm, write_settings_file, get_and_update_licence_details).
Possible Reasons for Failure:

Error During unpack_archive: The most common reason is an error during the actual unzipping. This could be due to:
A corrupted download zip file.
Permissions issues preventing writing to the _ebible_data/projects/maprik folder.
shutil having trouble with the specific zip format (less likely but possible).
Running out of disk space.
Error During Post-Unzip Steps: An error in rename_usfm, write_settings_file, or get_and_update_licence_details after a successful unzip could leave the process incomplete. The try...except block should catch this.
action_needed_unzip Incorrectly False: Although the logic seems sound, there might be an edge case where the status file has a recent status_unzip_date and a status_unzip_path pointing to _ebible_data/projects/maprik, but that directory is actually empty or incomplete from a previous failed run. The scan_project_folder only checks is_dir(), not content.
Debugging and Solution:

Let's improve the error logging within the unzip_and_process_files function to pinpoint exactly where it might be failing. We can track the specific step within the try block.

diff
-1
+7
The code change produced by Gemini could not be fully parsed. Can't automatically accept changes.
        # --- Unzip ---
        try:
            # Clean existing directory before unzipping
            step = "clean_existing"
            if project_dir.exists():
                log_and_print(logfile, f"Removing existing directory: {project_dir}")
                shutil.rmtree(project_dir)
            project_dir.mkdir(parents=True, exist_ok=True)

            shutil.unpack_archive(download_path, project_dir)
            step = "unpack_archive"
            shutil.unpack_archive(download_path, project_dir, format='zip') # Explicitly specify format
            df.loc[index, 'status_unzip_path'] = str(project_dir.resolve())
            df.loc[index, 'status_unzip_date'] = TODAY_STR
            df.loc[index, 'status_last_error'] = np.nan # Clear error on successful unzip
Unchanged lines

            # --- Post-Unzip Processing ---
            # Rename USFM files
            step = "rename_usfm"
            rename_usfm(project_dir, logfile)

            # Write Settings.xml
            step = "write_settings"
            write_settings_file(project_dir, lang_code, translation_id, vrs_diffs_data)

            # Extract Licence Details (only if needed or forced)
            step = "get_licence"
            if row['action_needed_licence']:
                 df = get_and_update_licence_details(df, index, project_dir, logfile)

            processed_count += 1

        except (shutil.ReadError, FileNotFoundError, OSError, Exception) as e:
            error_message = f"Processing error during step '{step}': {e}"
            log_and_print(logfile, f"Error processing {translation_id} at {project_dir}: {e}", log_type="Error")
            df.loc[index, 'status_unzip_path'] = np.nan
            df.loc[index, 'status_unzip_date'] = np.nan

Explanation of Changes:

step Variable: Introduced a step variable to keep track of which operation (clean_existing, unpack_archive, rename_usfm, etc.) is currently being attempted within the try block.
Explicit Format: Added format='zip' to shutil.unpack_archive. While shutil usually detects the format, being explicit can sometimes help avoid ambiguity.
Enhanced Error Message: Modified the except block to include the value of the step variable in the error_message that gets stored in the status_last_error column and logged.
How to Use This:

Apply the diff above to your ebible_status.py file.
Delete the existing _ebible_data/projects/maprik and _ebible_data/projects/wosera folders if they exist (especially if they might be empty or incomplete).
Delete the corresponding rows for abt-maprik and abt-wosera from your metadata/ebible_status.csv file OR just clear the status_unzip_path, status_unzip_date, and status_last_error columns for those rows to force the unzip action again. Using --force_download --filter "abt-" might be the easiest way to trigger reprocessing for just these.
Run the script again: python ebible_status.py --filter "abt-" (add --force_download if you didn't modify the CSV).
Check the log file (logs/ebible_status_...log) and the ebible_status.csv file again for abt-maprik and abt-wosera.
If an error occurred, the log and the status_last_error column should now indicate which step failed (e.g., "Processing error during step 'unpack_archive': [Specific Error Message]"). This will tell you if it's the unzipping itself, the renaming, etc.
If it succeeds, the _ebible_data/projects/maprik folder should now exist and contain the unzipped files.
This improved logging should give us a much clearer picture of why the unzipping isn't completing successfully for these specific translations. Remember, the target folder name maprik (not abt-maprik) is expected by the current script logic.

Rate this answer: 
Context Sources (3)

David Baines
Yes that's correct the current logic is working and I have found the corpus/maprik folder now.
I now think that the problem is with the external code.


Gemini
Great! I'm glad to hear that the unzipping logic within the script is working as expected now and you've located the corpus/maprik folder.

It sounds like you're right, if the script is correctly downloading and unzipping the files to the designated project folders (projects/maprik, projects/wosera, etc.), then any subsequent issues with processing those folders (like text extraction) would likely stem from the external tool, which in this workflow is the SILNLP bulk_extract_corpora command.

Rate this answer: 
Context Sources (1)

David Baines
OK - I've considered this further and think that a possible solution here would be to change the logic. The external code isn't as simple to change as there's a large team working on it and many processes. Also it isn't considered to be incorrect - the folder name of the projects are considered to be the problem.

I believe a relatively simple solution would be as follows:

Download and unzip the projects as we currently do.
Rename the usfm files to SFM files as we currently do. This will include the folder name in the filenames.
Write the Settings.xml file according to the current logic - this wil include the translation_id in the PostPart of the filename identifier. Which I think is the current logic. Then print the commands for the external code and extract the files.
Return to this code and have this code rename the extracted files (only one per project). The logic for this may now be much simpler because the name of the extracted needs to be <translation_id>.txt The problem the external code causes is that it adds the language code to the filename so abt-maprik becomes abt-abt-maprik abc-m becomes abc-abc-m amo becomes amo-amo.txt In the past we have simply accepted amo-amo.txt as a filename it's still short and wasn't causing any problems. Now we should re-think that so that the logic is as simple as possible so that there are fewer ways of introducing bugs.
Does this sound like a workable solution? Can you recommend a way to maintain the state so that code does what is necessary without the user having to know. Users wont necessarily know the state of the ebible_data repo that they receive so a robust solution to this would be useful. Could we safely omit the date stamp on the downloaded file since that is stored in the ebible_status.csv? Are there other simplifications to the logic we can make?


Gemini
Okay, let's break down this proposed solution and discuss its implications.

The core idea is to accept the current project folder naming convention (e.g., abt-maprik -> projects/maprik) and the resulting SILNLP output (e.g., corpus/abt-abt-maprik.txt), and then add a final step within your script to rename the extracted file to the desired format (corpus/abt-maprik.txt).

Evaluating the Proposed Workflow:

Download/Unzip/Rename USFM/Write Settings.xml: These steps remain largely unchanged, leveraging the existing logic.
Run SILNLP (External): This remains a manual step for the user.
Rename Extracted File (New Step): This is the key addition. Your script would need to run again after the user has run SILNLP. This second run would identify the incorrectly named .txt files produced by SILNLP and rename them.
Workability:

Yes, this is technically workable. Adding a renaming step after the external process completes can achieve the desired final filename (<translation_id>.txt).
Pros:

Minimal Change to Existing Logic: It avoids altering the create_project_name logic or the interaction with Settings.xml that was designed to work around SILNLP's perceived behavior.
No Change to External Tool: It doesn't require modifications to the SILNLP tool.
Cons:

Two-Step Process: Requires the user to run your script, then SILNLP, then your script again to perform the renaming. This is less streamlined.
Dependency on SILNLP Output: The renaming step becomes dependent on knowing the exact (and potentially inconsistent or changing) naming pattern SILNLP uses for its output (e.g., lang-lang-project.txt, lang-project.txt). If SILNLP's output naming changes, the renaming step in your script will break.
Increased Complexity: Adds another state to manage (extracted but not yet renamed).
State Management for Renaming:

To make this robust, the script needs to track the renaming state, especially since it requires a second run. The ebible_status.csv file is the perfect place for this:

Add New Column: Introduce a column like status_extract_renamed_date (String, YYYY-MM-DD) to ebible_status.csv.
Implement Renaming Logic:
Create a new function (e.g., rename_extracted_files).
This function runs every time the script executes (likely near the end).
It iterates through the status_df.
It looks for rows where status_extract_date is NOT NULL (meaning an extract file was detected by the scan) but status_extract_renamed_date IS NULL.
For such rows:
It needs to determine the actual path of the file created by SILNLP. The status_extract_path currently stores the expected final path. The scan logic (scan_corpus_file) might need adjustment, or this renaming function might need to predict the SILNLP output pattern (e.g., f"{lang}-{lang}-{proj_name}.txt" or f"{lang}-{proj_name}.txt") and check if that file exists. Let's assume for now it can find the incorrectly named file.
Calculate the target path (e.g., corpus/{translation_id}.txt).
Verify the source (incorrectly named) file exists.
Verify the target file doesn't already exist (to avoid errors or accidental overwrites).
Use os.rename() or Path.rename().
On success, update status_extract_renamed_date in the DataFrame and potentially update status_extract_path to the new, correct path.
Log success or failure.
Call the Function: Call rename_extracted_files within main() after other processing and status saving, but before the final reporting.
This way, whenever the script runs, it checks if any previously extracted files are pending renaming and handles them automatically, making it robust even if the user forgets the second run or receives a partially processed dataset.

Simplifications:

Omitting Date Stamp on Downloaded File:
Can we? Yes. The status_download_date in ebible_status.csv tracks this.
Should we? Probably not. Keeping the date in the filename ({id}--{YYYY-MM-DD}.zip) provides redundancy and makes manual identification/debugging much easier if the status file is lost or you're just browsing the downloads folder. The storage overhead is negligible. Recommendation: Keep the date stamp.
Other Simplifications:
The proposed post-rename step inherently adds a layer of complexity compared to a workflow where the correct filename is generated directly.
Alternative Revisit: Is it absolutely certain that SILNLP always produces the lang-lang-project.txt format even with the current projects/maprik structure? If there's a chance that reverting create_project_name (unzipping abt-maprik to projects/abt-maprik) and adjusting rename_usfm would lead to SILNLP producing the correct abt-maprik.txt directly, that would be a significantly cleaner and simpler solution overall, eliminating the need for the post-rename step entirely. It might be worth a small test to confirm SILNLP's behavior with a project folder named like abt-maprik.
Conclusion:

Yes, the proposed solution with a post-extraction renaming step is workable and can be made robust using the status file. However, it adds complexity and a dependency on SILNLP's specific output format.

Recommendation:

Confirm SILNLP Behavior: Strongly recommend performing a quick test: Manually create a project folder _ebible_data/projects/abt-maprik, place the necessary USFM files (renamed appropriately, e.g., INTabt-maprik.SFM) and a corresponding Settings.xml inside, and run bulk_extract_corpora. Observe the exact output filename SILNLP creates in the corpus folder.
If SILNLP produces abt-maprik.txt: Modify create_project_name to not strip the prefix, update rename_usfm logic, and remove the need for the post-rename step. This is the preferred, simpler solution.
If SILNLP produces abt-abt-maprik.txt (or similar): Proceed with implementing the post-rename step using the state management (status_extract_renamed_date column) described above. This addresses the issue without modifying the external tool.
Rate this answer: 
Context Sources (2)

David Baines
No this first statement is, I think, incorrect: The core idea is to accept the current project folder naming convention (e.g., abt-maprik -> projects/maprik) It should be that we have a simpler project folder naming convention: The core idea is to accept the current project folder naming convention (e.g., abt-maprik -> projects/abt-maprik)

The second part is correct, we accept the resulting SILNLP output (e.g., corpus/abt-abt-maprik.txt) and then rename that to abt-maprik.txt. That's the part that I hope allows some simplification of the code. At the moment passing the corpus/maprik folder to SILNLP is returning the empty files (empty except for newlines).


Gemini
Okay, thank you for the clarification! My apologies for misunderstanding the desired project folder naming.

Let's proceed with this corrected plan:

Project Folder Naming: Unzip abt-maprik to projects/abt-maprik.
SILNLP Input: Use projects/abt-maprik as input for SILNLP.
SILNLP Output: Expect SILNLP to produce something like corpus/abt-abt-maprik.txt.
Post-Processing: Rename corpus/abt-abt-maprik.txt to corpus/abt-maprik.txt.
This approach indeed simplifies the project folder naming significantly. The complexity now lies in ensuring the rename_usfm and write_settings_file functions handle this correctly, and implementing the post-extraction renaming step.

Here are the necessary changes:

1. Simplify create_project_name

This function becomes much simpler, essentially just returning the original translation ID.

diff
-5
+2
The code change produced by Gemini could not be fully parsed. Can't automatically accept changes.

def create_project_name(language_code: str, translation_id: str) -> str:
    """Creates the destination project name."""
    if regex.match(f"^{regex.escape(language_code)}[_\\-]", translation_id):
        return translation_id[len(language_code) + 1 :].replace("-", "_")
    else:
        return translation_id

    # Simply use the translation_id as the project folder name.
    return translation_id

def is_date_older_than(date_str: Optional[str], max_days: int) -> bool:
    """Checks if a date string (YYYY-MM-DD) is older than max_days ago."""

2. Adjust rename_usfm (via get_destination_file_from_book)

The helper function get_destination_file_from_book (in rename_usfm.py) needs to construct the new filename based on the book code and the full translation_id. Assuming the desired format is {BOOK_CODE}{translation_id}.SFM:

(Note: This change is in rename_usfm.py, not ebible_status.py)

diff
-4
+5
The code change produced by Gemini could not be fully parsed. Can't automatically accept changes.
        # Extract the base part of the filename after the book code/number prefix
        # Example: 01-GENeng-web.usfm -> eng-web
        # Example: 41MATheb-wlc.usfm -> heb-wlc
        # Example: PSAafr-afr83.usfm -> afr-afr83 (assuming book_code is PSA)
        # Example: PSAafr-afr83.usfm -> afr-afr83

        # This assumes the project name is embedded after the book code
        project_name_part = filename_stem[len(book_code) :]
        # The project name is the name of the parent directory
        project_name = old_usfm_path.parent.name

        # Construct the new filename
        # Example: GEN + eng-web -> GENeng-web.SFM
        new_filename = f"{book_code}{project_name_part}.SFM"
        new_filename = f"{book_code}{project_name}.SFM"
        new_sfm_path = old_usfm_path.with_name(new_filename)
        return new_sfm_path
    else:

3. Adjust write_settings_file

The PostPart in Settings.xml needs to match the new SFM filename format created by the adjusted rename_usfm.

(Note: This change is in settings_file.py, not ebible_status.py)

diff
-1
+1
The code change produced by Gemini could not be fully parsed. Can't automatically accept changes.
            f"""\
            <ScriptureText>
                <Versification>{vrs_num}</Versification>
                <LanguageIsoCode>{language_code}:::</LanguageIsoCode>
                <LanguageIsoCode>{language_code}</LanguageIsoCode>
                <Naming BookNameForm="41MAT" PostPart="{translation_id}.SFM" PrePart="" />
            </ScriptureText>"""
        )

(Self-correction: Also removed the trailing ::: from LanguageIsoCode as it's likely unnecessary).

4. Implement Post-Extraction Renaming

This requires adding the state column and the renaming function to ebible_status.py as discussed previously.

diff
-1
+69
The code change produced by Gemini could not be fully parsed. Can't automatically accept changes.
    "licence_ID", "licence_File", "licence_Language", "licence_Dialect",
    "licence_Vernacular_Title", "licence_Licence_Type", "licence_Licence_Version",
    "licence_CC_Licence_Link", "licence_Copyright_Holder", "licence_Copyright_Years",
    "licence_Translation_by", "licence_date_read",
    "status_extract_renamed_date" # New column for tracking rename
]

ALL_STATUS_COLUMNS = ORIGINAL_COLUMNS + STATUS_COLUMNS + LICENCE_COLUMNS
Unchanged lines
    return df


def rename_extracted_files(
    df: pd.DataFrame,
    corpus_folder: Path,
    private_corpus_folder: Path,
    logfile: Path
) -> pd.DataFrame:
    """Renames extracted files from SILNLP output format to {translation_id}.txt."""
    # Filter for rows where extract exists but rename hasn't happened
    rename_candidates = df[df['status_extract_date'].notna() & df['status_extract_renamed_date'].isna()]
    count = len(rename_candidates)

    if count == 0:
        # log_and_print(logfile, "No extracted files require renaming.") # Reduce noise
        return df

    log_and_print(logfile, f"Attempting to rename {count} extracted files...")
    renamed_count = 0
    for index, row in tqdm(rename_candidates.iterrows(), total=count, desc="Renaming Extracts"):
        lang_code = row['languageCode']
        translation_id = row['translationId']
        is_redist = row['Redistributable']
        corpus_base = corpus_folder if is_redist else private_corpus_folder

        # --- Determine potential SILNLP output filenames ---
        # Pattern 1: lang-lang-id.txt (e.g., abt-abt-maprik.txt)
        silnlp_output_name1 = f"{lang_code}-{translation_id}.txt"
        # Pattern 2: lang-id.txt (e.g., aoj-aoj.txt - SILNLP might handle simple cases differently)
        # This might be the same as translation_id.txt if lang==id prefix
        silnlp_output_name2 = f"{lang_code}-{translation_id}.txt" # Corrected: This was same as pattern 1, should be simpler
        # Let's assume the most likely pattern is lang-translation_id.txt based on input project name
        silnlp_output_path = corpus_base / silnlp_output_name1

        target_name = f"{translation_id}.txt"
        target_path = corpus_base / target_name

        if silnlp_output_path.exists() and silnlp_output_path != target_path:
            if target_path.exists():
                log_and_print(logfile, f"Warning: Target rename path {target_path} already exists. Skipping rename for {translation_id}.", log_type="Warn")
            else:
                try:
                    silnlp_output_path.rename(target_path)
                    df.loc[index, 'status_extract_renamed_date'] = TODAY_STR
                    df.loc[index, 'status_extract_path'] = str(target_path.resolve()) # Update path to correct one
                    renamed_count += 1
                    log_and_print(logfile, f"Renamed {silnlp_output_path.name} to {target_path.name}")
                except OSError as e:
                    log_and_print(logfile, f"Error renaming {silnlp_output_path} to {target_path}: {e}", log_type="Error")
                    df.loc[index, 'status_last_error'] = f"Extract rename failed: {e}"
        elif target_path.exists():
             # If the target already exists, assume it was renamed previously or SILNLP produced correct name
             if pd.isna(row['status_extract_renamed_date']):
                  df.loc[index, 'status_extract_renamed_date'] = TODAY_STR # Mark as done
                  log_and_print(logfile, f"Target file {target_path.name} already exists, marking rename as complete.")
        # else: # File not found with expected SILNLP pattern
            # log_and_print(logfile, f"Could not find expected SILNLP output file {silnlp_output_path.name} for {translation_id}", log_type="Warn")

    if renamed_count > 0:
        log_and_print(logfile, f"Finished renaming. Renamed {renamed_count} files.")
    return df

# --- Main Execution ---

def main() -> None:
Unchanged lines
    # Perform licence checks for existing projects if needed
    actions_df = check_and_update_licences(actions_df, logfile)

    # --- Perform post-extraction renaming ---
    # Run this *before* saving status, so rename date gets saved
    actions_df = rename_extracted_files(actions_df, corpus_folder, private_corpus_folder, logfile)
    # --- Update Main Status DataFrame and Save ---
    # Use update() which aligns on index (translationId if set, otherwise row number)
    # Ensure index is set correctly if needed, or update based on 'translationId' column
Unchanged lines
    except Exception as e:
        log_and_print(logfile, f"Error saving status file {status_path}: {e}", log_type="Error")

    # --- Perform post-extraction renaming (Run again after save? Maybe not needed if run before save) ---
    # status_df = rename_extracted_files(status_df, corpus_folder, private_corpus_folder, logfile)
    # status_df.to_csv(status_path, index=False) # Save again if run after initial save

    # --- Report Missing Extracts ---
    # Re-scan corpus folders to update extract dates one last time before reporting
    status_df = scan_and_update_status(status_df, downloads_folder, projects_folder, private_projects_folder, corpus_folder, private_corpus_folder, logfile)

Summary of Renaming Logic (rename_extracted_files):

Identify Candidates: Filters the DataFrame for rows where status_extract_date is set (meaning a .txt file was found by the scan) but status_extract_renamed_date is not set.
Predict SILNLP Output: It constructs the most likely SILNLP output filename based on the pattern f"{lang_code}-{translation_id}.txt". This is an assumption about SILNLP's behavior given the input project name.
Define Target: Defines the desired final filename as f"{translation_id}.txt".
Check and Rename:
If the predicted SILNLP output file exists and is different from the target path:
It checks if the target path already exists (logs a warning if so).
It attempts to rename the file using Path.rename().
On success, it updates status_extract_renamed_date and status_extract_path in the DataFrame.
If the target path already exists (meaning SILNLP might have produced the correct name, or it was renamed previously), it just updates status_extract_renamed_date to mark it as done.
Call Location: The function is called in main before the final status save, ensuring the rename status is persisted.
This revised approach simplifies the project naming while adding the necessary post-processing step to achieve the desired final .txt filenames. Remember to test the assumption about SILNLP's output naming convention (lang-id.txt) when the input project folder is named like the translation_id.

