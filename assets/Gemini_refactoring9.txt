
David Baines
There are several things that I would like to work on for this repo.

The main aim of the repo is to produce a parallel dataset for machine learning from the Bibles available for downloading from eBible.org.

The code uses SILNLP and the machine library to do this and there are several steps involved:

Download the translations.csv file from ebible.org to find the list of translations.
Create ebible_status.csv with the information from the translations.csv as a starting point. This file keeps track of the progress of each translation through the 'pipeline'.
Download the zipped translations and unzip them.
Can you continue this list from the logic of the code in ebible.py This could be used to check and maybe update README.md


Gemini
Okay, I can help continue that list based on the workflow in ebible.py. This detailed breakdown should indeed be useful for checking and potentially updating your README.md.

Here's a continuation of your list, detailing the subsequent steps:

Your initial steps:

Download the translations.csv file from ebible.org to find the list of translations.
Create ebible_status.csv (if it doesn't exist) by initializing it with information from translations.csv. This status file is subsequently loaded and updated to track the progress and state of each translation through the pipeline.
Download the zipped USFM Bible translations from ebible.org and unzip them into their respective project folders (e.g., in projects/ or private_projects/).
Continuing the list based on ebible.py logic:

Prepare Individual Project Folders (First Major Pass of ebible.py): For each newly unzipped or reprocessed translation, the script performs several setup tasks within its project folder: a. Rename USFM Files: It standardizes the names of the USFM files (e.g., A01OSPSFM.usfm might become 01GEN.SFM or a similar convention, according to the logic in rename_usfm.py) to ensure consistency for subsequent processing. b. Generate Versification (.vrs) File: A project-specific versification file (e.g., <translationId>.vrs) is generated. This file is created by analyzing the USFM content to determine the Bible's versification scheme. c. Create Settings.xml: A Settings.xml file is written into the project directory. This configuration file includes important metadata for SIL tools, such as the language code and, crucially, the inferred versification system (which references the .vrs file generated in the previous step). d. Extract License Information: The script parses the copr.htm file (typically found within the unzipped translation archive) to extract copyright statements, Creative Commons license details (if any), and other related metadata. This extracted information is then saved into the corresponding row for the translation in ebible_status.csv.

Text Extraction using SILNLP (Manual Intermediary Step): After the project folders are prepared, ebible.py prints out the necessary commands for the user to run the bulk_extract_corpora.py script, which is part of the SILNLP toolkit. This external script is responsible for:

Iterating through the prepared project folders.
Extracting the actual verse text from the USFM files.
Saving this text into plain text files, usually one verse per line. These output files are typically placed in the corpus/ (for public data) or private_corpus/ (for private data) directories and are named by SILNLP in a format like languageCode-translationId.txt (e.g., eng-KJV.txt).
Finalize Extracted Files and Update Status (Second Major Pass/Run of ebible.py): Once the SILNLP bulk_extract_corpora.py script has completed, ebible.py is run again (or this part of its logic is executed if it's processing files from a previous SILNLP run). This pass focuses on the files generated by SILNLP: a. Rename Extracted Text Files: The script scans the corpus/ and private_corpus/ directories for the text files output by SILNLP. It then renames these files from SILNLP's default naming convention (e.g., eng-KJV.txt or aai-aai.txt) to the project's final standard format, which is translationId.txt (e.g., KJV.txt, aai.txt). b. Update Status with Final Paths: The paths to these newly renamed text files, along with the date of renaming, are recorded in ebible_status.csv. This marks the successful extraction and finalization of the text data for a translation.

Ongoing Status Management and Error Tracking: Throughout all these stages, ebible_status.csv serves as the central ledger. It's continuously updated to reflect:

Dates of various operations (download, unzip, license check, settings file creation, text file renaming).
Paths to downloaded zip files, unzipped project folders, and final extracted text files.
Key extracted metadata like license details and inferred versification.
Any errors encountered during the processing of a specific translation, which helps in debugging and allows the script to skip previously failed items on subsequent runs. This status file is crucial for the script's ability to resume processing, avoid redundant work, and manage the complex workflow across many translations.
Special Operational Mode - Update Settings (--update-settings): ebible.py includes a special mode triggered by the --update-settings command-line flag. When run in this mode:

It bypasses the download, unzip, and full processing pipeline.
Instead, it iterates through all existing project folders (both public and private).
For each project, it regenerates the Settings.xml file (and the associated project-specific .vrs file if it's missing or needs an update). This is useful for applying new logic for versification scoring or other settings changes across all previously processed translations.
It updates ebible_status.csv with the new settings file date and any changed versification information.
A report detailing the changes made to settings files (settings_update.csv) is generated.
The script then prints the SILNLP commands and exits.
Regarding README.md:

This detailed list can indeed help you check and update your README.md. Here are a few thoughts:

"Regenerating the corpus" section: Your current README describes a multi-step process involving two runs of ebible.py sandwiching the SILNLP step. The list above provides a more granular breakdown of what happens in each of those ebible.py passes. You could integrate this more detailed sequence there.

"What the script does" section: This section in your README is currently quite high-level:

downloads a translations.csv file which outlines the currently available translations (in metadata dir)
downloads zip files for each translation (in downloads dir)
unpacks those zip files into paratext projects (in projects dir)
constructs a licence file (in metadata dir)
The list above expands significantly on these points.
Correction needed: The last point, "constructs a licence file (in metadata dir)," is slightly misleading. The script extracts license information from copr.htm within each project and stores this data within the ebible_status.csv file (which itself resides in the metadata directory). It doesn't create a separate, standalone "licence file" in the metadata directory for all licenses. Step 4d in the list above clarifies this. You might want to update this in the README for accuracy.
Clarity on File Naming: The README correctly explains the initial SILNLP naming (<languageCode>-<translationId>.txt) and the final naming (<translationId>.txt). Step 6a in the list above reiterates this renaming step performed by ebible.py.

By incorporating these details, you can provide users with a clearer and more comprehensive understanding of the entire data generation pipeline.

Rate this answer: 
Context Sources (3)

David Baines
I would like to have the ebible code manage the extraction itself. This will greatly ease the use of the script and will also allow the script to know where it is in the processing of each translation.

I have full access to all the code necessary and the machine library is already available to the code. You can see the code that is used to extract a single project in extract_corpora.py This is copied from the SILNLP code. I hope this gives information that would allow us to effectively reproduce that within the ebible.py code, and call it for each project that needs to be extracted into a corpus text.

Can you identifiy concerns or difficulties with this idea? Are there more scripts from SILNLP or the machine library that would be helpful to examine in order to accomplish this goal? Are there other questions or ambiguites that should be resolved before starting on changes to ebible.py ?


Gemini
That's an excellent goal! Integrating the extraction directly into ebible.py will indeed streamline the workflow significantly and give you better control and insight into the process for each translation.

Based on the provided ebible.py and assets/extract_corpora.py, here's an analysis of your idea:

Concerns and Difficulties with Integration:
Dependency on assets/paratext.py:

The provided assets/extract_corpora.py heavily relies on functions imported from an assets/paratext.py file (e.g., extract_project, check_versification, extract_term_renderings). The logic within assets/paratext.py will be central to the success of this integration. ebible.py will effectively be calling functions from this module.
Error Handling and Propagation:

The extract_project function (from assets/paratext.py) needs a robust way to signal success or failure. Currently, assets/extract_corpora.py logs errors but doesn't have a clear return mechanism for ebible.py to catch specific errors from the extraction of a single project.
ebible.py will need to catch these errors, log them in ebible_status.csv (e.g., in status_last_error), and potentially mark the translation to be skipped in future runs, similar to how download/unzip errors are handled.
Configuration and Parameter Passing:

assets/extract_corpora.py takes several command-line arguments (--include, --exclude, --markers, --lemmas, --project-vrefs) that are passed to extract_project. ebible.py will need to:
Decide on default values for these (e.g., extract all books, no special markers for a general parallel corpus).
Potentially add new command-line arguments to ebible.py if you want to control these extraction options per run.
The extract_project function will need to accept the target output directory (e.g., corpus_folder or private_corpus_folder from ebible.py) instead of relying on SIL_NLP_ENV.mt_scripture_dir.
Path Management (SIL_NLP_ENV vs. ebible.py's system):

assets/extract_corpora.py uses SIL_NLP_ENV for paths like SIL_NLP_ENV.assets_dir / "vref.txt" and output directories. ebible.py has its own path management based on EBIBLE_DATA_DIR.
The functions from assets/paratext.py will need to be adapted or called in a way that uses ebible.py's path conventions. For instance, the path to vref.txt (if get_expected_verse_count is used) will need to be resolved correctly.
Output Filename Control:

extract_project returns a corpus_filename. ebible.py currently has a rename_extracted_files function that expects SILNLP's bulk_extract_corpora.py to produce files named like <languageCode>-<translationId>.txt.
Ideally, the integrated extract_project function should either:
Allow ebible.py to specify the output filename directly (e.g., to <languageCode>-<translationId>.txt or even the final <translationId>.txt).
Or, its output naming convention must be clearly understood by ebible.py so that status_extract_original_filename can be correctly populated and rename_extracted_files can work.
Role of check_versification:

ebible.py already goes to great lengths to determine the correct versification and write it into Settings.xml (via settings_file.py).
The check_versification(project_dir) call (from assets/paratext.py) in assets/extract_corpora.py needs clarification. It should ideally use the Settings.xml created by ebible.py as the source of truth for versification during extraction, rather than trying to re-determine or validate it in a conflicting way. If it's purely a validation step, its outcome needs to be handled.
Necessity of Term Renderings:

assets/extract_corpora.py also calls extract_term_renderings. Your main aim is a "parallel dataset" of verses. If these term renderings are not essential for that goal, omitting this step during integration would simplify things.
Validation of Extracted Content:

assets/extract_corpora.py includes get_expected_verse_count and compares the actual extracted verse count. This is a valuable validation. ebible.py could incorporate this, logging warnings or errors if counts don't match expectations.
More Scripts/Modules to Examine:
assets/paratext.py (Crucial):

This is the most important piece of code to understand. The implementations of extract_project, check_versification, and extract_term_renderings will dictate how they are called and what they return.
extract_project(...):
How does it iterate through the USFM files in a project? (Likely using machine.corpora.ParatextTextCorpus).
How does it handle different versifications (it should use the Settings.xml)?
How does it format the output text (one verse per line)?
What are its exact error conditions?
machine.corpora.ParatextTextCorpus:

Both ebible.py (via settings_file.py for generating project-specific .vrs files) and assets/extract_corpora.py (likely via assets/paratext.py) will use this or similar logic to read Paratext projects. Ensuring consistent usage and understanding its behavior with Settings.xml is important.
machine.scripture module (VerseRef, Versification, Canon):

These are fundamental for handling Bible references. ebible.py and the extraction logic must use them consistently.
Other Questions and Ambiguities to Resolve Before Starting:
Deep Dive into assets/paratext.py:

What are the precise signatures (parameters, return types) of extract_project, check_versification, and extract_term_renderings?
How does extract_project determine the output filename if not explicitly told?
What specific exceptions can extract_project raise that ebible.py should handle?
Extraction Options (--include, --exclude, etc.):

For the primary goal of a parallel dataset, what should be the default behavior for book inclusion/exclusion, inclusion of markers, lemmas, or project-specific vrefs?
Will these options need to be configurable via ebible.py's command line, or can they be hardcoded to sensible defaults for this project? (e.g., include all canonical books, no markers, no lemmas).
Definition of "Parallel Dataset" Scope:

Are term renderings strictly necessary? (Likely no, for a verse-aligned parallel corpus).
Are lemmas or USFM markers desired in the final text output?
Integration of check_versification:

If ebible.py has already created Settings.xml with a chosen versification, what is the exact purpose of check_versification from assets/paratext.py? Is it a pre-flight check, a validation, or does it modify settings? It should not conflict with ebible.py's settings generation.
Workflow Modification in ebible.py:

The current multi-pass approach (prepare, then manual SILNLP, then rename) will change.
A likely new flow per translation inside ebible.py's main loop:
Download/Unzip (as is).
Prepare project folder (rename USFM, generate .vrs, write Settings.xml, extract license) (as is).
Call integrated extraction function (e.g., a wrapper around extract_project).
If extraction successful, get the output path.
Rename the extracted file if the integrated function doesn't output to the final desired name (<translationId>.txt).
Update ebible_status.csv with extraction path, date, and clear errors.
If extraction fails, update ebible_status.csv with the error.
Idempotency of Extraction:

If ebible.py is run again, and a translation's text file already exists and is up-to-date (based on status_extract_renamed_date and max_age_days), the extraction step should be skipped, similar to downloads and unzips. The determine_actions logic will need to be extended to include an action_needed_extract.
In summary:

Yes, this is a very good idea and achievable. The main challenge will be understanding and cleanly integrating the logic from assets/paratext.py (especially extract_project) into ebible.py's existing framework, paying close attention to path management, error handling, and configuration. The assets/extract_corpora.py script provides a good template for how extract_project is used.

You would essentially be creating a new function in ebible.py, say extract_text_for_project(project_row, project_path, output_corpus_dir), which would internally call the necessary functions (primarily extract_project from assets/paratext.py after setting up paths and parameters).

Rate this answer: 
Context Sources (15)

David Baines
Please note that the code in the assets folder is given so that you can see how it works.

ebible.py should not refer to code that is in the assets folder, but will import functions from the machine library. Perhaps the extract_project code from the paratext.py will need to tbe reproduced in ebible.py.

Yes the error handling will need to be able to signal success or failure. That could be a check of whether the expected extract file exists and contains data. Those should be stored in ebible_status.csv

For our purposes we can ignore most of the command-line arguments that can be passed to extract_corpora. We will extract all books so --include and --exclude are not required by our code. neither are --lemmas or --project-vrefs. The corpus folder will be the target folder for all the redistributable projects and the private_corpus folder will be the target for all those in the private_projects folder. These can be listed explicitly in the .env file perhaps to keep as many constants as possible in one place. The SIL_NLP_ENV are used by SILNLP, but that is just provided as code to make use of. We will not be using SIL_NLP_ENV.

Yes, it would be ideal if the extract_project could specify the output_filename and avoid the need for any renaming. I'm not sure how the current renaming is working, but keeping flexibility within the code to make it easy to adjust the naming of the corpus files will be very helpful.

If the filenames no longer need renaming, then the columns in ebible_status.csv related to that will no longer be required. The output filename returned by extract_project should be very useful to the ebible.py code. It means that the "status_extract_path" and "status_extract_date" can be saved in the ebible_status.csv in a column without any reliance on the human operator. Since the status_extract_path contains the original extracted filename we can omit the status_extract_original_filename column from the ebible_status.csv. It is duplicated data.

The renaming of the corpus file can also be done by the code immediately after extraction, and that information saved in the ebible_status.csv


Gemini
Okay, this is a significant and beneficial change to streamline your data processing pipeline! Integrating the text extraction directly into ebible.py will make it much more self-contained and easier to manage.

Here's a breakdown of the plan and the suggested code changes:

Core Idea:

Internal Extraction Function: We'll create a new helper function within ebible.py (e.g., _perform_text_extraction_for_project) that replicates the core logic of assets/paratext.py extract_project using components from the machine.corpora library. This function will take a prepared project directory and an output path, and write the extracted text directly to the final desired filename (e.g., KJV.txt).
Workflow Integration: This new function will be called from ebible.py's main processing loop for each translation that requires extraction.
Status File Simplification: The ebible_status.csv columns related to the intermediate SILNLP output and subsequent renaming will be simplified.
Removal of External Steps: The manual step of running bulk_extract_corpora.py and the ebible.py logic for renaming its output will no longer be necessary.
Benefits:

Simplified Workflow: No more multi-step execution involving external scripts for extraction.
Better State Tracking: ebible.py will have direct knowledge of the extraction status for each translation.
Reduced Complexity: Fewer scripts and intermediate file-naming conventions to manage.
Let's go through the suggested code modifications.

1. New Internal Extraction Function
We'll add this helper function to ebible.py. It leverages machine.corpora utilities.

python
# Add this function to ebible.py, for example, after the scanning functions.
# Ensure necessary imports are at the top of ebible.py:
# from machine.corpora import ParatextTextCorpus, create_versification_ref_corpus, extract_scripture_corpus
# from contextlib import ExitStack (if not already imported for other purposes)

def _perform_text_extraction_for_project(
    project_dir_path: Path,
    output_file_path: Path,
    translation_id: str # For logging
) -> Tuple[bool, Optional[str], int]: # Returns: Success_flag, error_message, segment_count
    """
    Extracts text from a Paratext project directory to a specified output file.
    Assumes Settings.xml is already present in project_dir_path.
    """
    logger.info(f"Starting text extraction for {translation_id} from {project_dir_path} to {output_file_path}")
    try:
        # ParatextTextCorpus uses Settings.xml within project_dir_path for versification.
        # include_markers=False is the default for typical parallel corpus text.
        project_corpus = ParatextTextCorpus(str(project_dir_path), include_markers=False)
        
        # ref_corpus is based on the standard vref.txt that machine.corpora knows.
        # This ensures alignment to a common verse reference scheme.
        ref_corpus = create_versification_ref_corpus()

        # The user wants all books, so no filtering logic for include_books/exclude_books is applied here.

        segment_count = 0
        # extract_scripture_corpus is a context manager.
        with output_file_path.open("w", encoding="utf-8", newline="\n") as output_stream, \
             extract_scripture_corpus(project_corpus, ref_corpus) as output_lines_iterator:
            
            for line_content, _ref_vref, _project_vref in output_lines_iterator:
                output_stream.write(line_content + "\n")
                segment_count += 1
        
        if segment_count == 0:
            # This could be an error or just an empty source.
            # For now, treat as success but log a warning. The file will be empty.
            logging.warning(f"Extraction for {translation_id} resulted in 0 segments. Output file {output_file_path} created but is empty.")
            # If an empty file is considered an error, you could return:
            # return False, "No segments extracted from project", 0

        logger.info(f"Successfully extracted {segment_count} segments for {translation_id} to {output_file_path}")
        return True, None, segment_count

    except Exception as e:
        logger.error(f"Error during text extraction for {translation_id} from {project_dir_path} to {output_file_path}: {e}", exc_info=True)
        # Clean up potentially incomplete output file
        if output_file_path.exists():
            try:
                output_file_path.unlink()
                logging.info(f"Removed incomplete output file: {output_file_path}")
            except OSError as unlink_e:
                logging.error(f"Error removing incomplete extraction output {output_file_path}: {unlink_e}")
        return False, str(e), 0

2. Modifications to ebible.py
ebible.py
-245
+211
4. Perform downloads, updating status.
5. Perform unzipping, renaming, settings file creation, and licence extraction,
   updating status.
6. Save the updated ebible_status.csv.
7. Print commands for bulk extraction using SILNLP.
6. Perform internal text extraction for projects, updating status.
7. Save the updated ebible_status.csv.
"""

import argparse
import shutil
import logging # Import logging
import sys
from contextlib import ExitStack # Potentially needed for extract_scripture_corpus if used directly

# --- CONFIGURE LOGGING before importing from settings_file ---
log_format = '%(name)s - %(levelname)s - %(message)s'
import regex
import requests
from bs4 import BeautifulSoup
from machine.corpora import ParatextTextCorpus, create_versification_ref_corpus, extract_scripture_corpus
from dotenv import load_dotenv
from rename_usfm import get_destination_file_from_book
from settings_file import generate_vrs_from_project, write_settings_file 

STATUS_COLUMNS = [
    "status_download_path", "status_download_date", "status_unzip_path",
    "status_unzip_date", "status_settings_xml_date",
    "status_extract_path", "status_extract_date", # For SILNLP output
    "status_extract_renamed_date", "status_extract_original_filename", # For renamed SILNLP output
    "status_unzip_date", "status_settings_xml_date", # Settings.xml creation date
    "status_extract_path", "status_extract_date", # For internally extracted and finalized text
    "status_last_error", "status_inferred_versification"
]

            status_df = pd.read_csv(status_path, keep_default_na=False, na_values=['']) # Treat empty strings as NA
            # Verify essential columns exist
            if not 'translationId' in status_df.columns:
                 raise ValueError("Status file missing 'translationId' column.")
                raise ValueError("Status file missing 'translationId' column.")

            # Handle migration from old column names for extraction
            if 'status_extract_renamed_date' in status_df.columns and 'status_extract_date' in STATUS_COLUMNS:
                # If new 'status_extract_date' is empty and old 'status_extract_renamed_date' has data, migrate it.
                # This assumes 'status_extract_path' from old system pointed to the SILNLP output.
                # The new 'status_extract_path' will be for the final file.
                # This migration is a bit tricky as paths also change.
                # For simplicity, we'll prioritize new columns. Users might need to reprocess for full path accuracy if migrating.
                logging.info("Detected 'status_extract_renamed_date'. Will prioritize new 'status_extract_date' if populated during scan/processing.")

            # Add any missing columns with default NaN values
            for col in ALL_STATUS_COLUMNS:
                if col not in status_df.columns:
                    logging.info(f"Adding missing column '{col}' to status DataFrame.")
                    status_df[col] = np.nan
            
            # Ensure correct order
            status_df = status_df[ALL_STATUS_COLUMNS]
            # Filter ALL_STATUS_COLUMNS to only include those actually present in status_df after additions, then reorder
            status_df = status_df[[col for col in ALL_STATUS_COLUMNS if col in status_df.columns]]

        except Exception as e:
            logging.error(f"Error loading status file {status_path}: {e}. Attempting to rebuild.")
    else:
        logging.info(f"Status file not found. Creating new one: {status_path}")
        if not translations_path.exists():
             logging.critical(f"Error: translations file missing at {translations_path}. Cannot create status file.")
             sys.exit(1)
            logging.critical(f"Error: translations file missing at {translations_path}. Cannot create status file.")
            sys.exit(1)
        try:
            # Read translations, ensuring 'translationId' is string
            trans_df = pd.read_csv(translations_path, dtype={'translationId': str}, keep_default_na=False, na_values=[''])
            if 'translationId' not in trans_df.columns:
                 raise ValueError("Translations file missing 'translationId' column.")
                raise ValueError("Translations file missing 'translationId' column.")

            # Create status DataFrame with all columns
            status_df = pd.DataFrame(columns=ALL_STATUS_COLUMNS)

            # Copy data from translations_df for matching columns
            for col in ORIGINAL_COLUMNS:
                 if col in trans_df.columns:
                      status_df[col] = trans_df[col]
                 else:
                      logging.warning(f"Column '{col}' not found in {translations_path}")
                      status_df[col] = np.nan # Add as empty column if missing
                if col in trans_df.columns:
                    status_df[col] = trans_df[col]
                else:
                    logging.warning(f"Column '{col}' not found in {translations_path}")
                    status_df[col] = np.nan # Add as empty column if missing

            # Initialize new status/licence columns with NaN
            for col in STATUS_COLUMNS + LICENCE_COLUMNS:
    try:
        trans_df = pd.read_csv(translations_path, dtype={'translationId': str}, keep_default_na=False, na_values=[''])
        if 'translationId' not in trans_df.columns:
             raise ValueError("Translations file missing 'translationId' column during merge check.")
            raise ValueError("Translations file missing 'translationId' column during merge check.")

        existing_ids = set(status_df['translationId'].astype(str))
        upstream_ids = set(trans_df['translationId'].astype(str))
            # Prepare new rows with all status columns, initializing non-original ones
            full_new_rows = pd.DataFrame(columns=ALL_STATUS_COLUMNS)
            for col in ORIGINAL_COLUMNS:
                 if col in new_rows_df.columns:
                      full_new_rows[col] = new_rows_df[col]
                 else:
                      full_new_rows[col] = np.nan
                if col in new_rows_df.columns:
                    full_new_rows[col] = new_rows_df[col]
                else:
                    full_new_rows[col] = np.nan
            for col in STATUS_COLUMNS + LICENCE_COLUMNS:
                 full_new_rows[col] = np.nan
                full_new_rows[col] = np.nan

            status_df = pd.concat([status_df, full_new_rows], ignore_index=True)
            # Consider saving immediately after adding new rows?

        removed_ids = list(existing_ids - upstream_ids)
        if removed_ids:
             logging.warning(f"{len(removed_ids)} translations exist in status but not in upstream {translations_path}. They will be kept but may be outdated.")
             # Optionally, mark them as inactive or remove them:
             # status_df = status_df[~status_df['translationId'].isin(removed_ids)]
            logging.warning(f"{len(removed_ids)} translations exist in status but not in upstream {translations_path}. They will be kept but may be outdated.")
            # Optionally, mark them as inactive or remove them:
            # status_df = status_df[~status_df['translationId'].isin(removed_ids)]

    except Exception as e:
        logging.error(f"Error merging upstream changes from {translations_path}: {e}")
                status_df.loc[index, 'status_unzip_date'] = scan_result[1]
                updated_count += 1

        # Scan Corpus (Extracted) - Note: Extract path is less certain, depends on SILNLP output format
        # This part might need adjustment if SILNLP naming changes. Assuming standard {lang}-{proj_name}.txt
        # Also, status_extract_path/date might be less critical now, but we can scan for it.
        # Scan Corpus (Extracted) - now looks for final <translationId>.txt
        if pd.isna(row['status_extract_date']):
            proj_name = translation_id
            corpus_base = corpus_folder if is_redist else private_corpus_folder
            # Construct expected extract filename - THIS IS AN ASSUMPTION
            expected_extract_filename = f"{lang_code}-{proj_name}.txt"
            # Construct final expected extract filename
            expected_extract_filename = f"{proj_name}.txt"
            extract_path = corpus_base / expected_extract_filename
            scan_result = scan_corpus_file(extract_path)
            if scan_result:
    logging.info("Ensuring status_extract_path is populated...")
    for index, row in status_df.iterrows():
        if pd.isna(row['status_extract_path']):
            lang_code = row['languageCode']
            translation_id = row['translationId']
            is_redist = row['Redistributable'] # Assumes bool
            proj_name = translation_id
            corpus_base = corpus_folder if is_redist else private_corpus_folder
            expected_extract_filename = f"{lang_code}-{proj_name}.txt"
            status_df.loc[index, 'status_extract_path'] = str((corpus_base / expected_extract_filename).resolve())
            final_extract_filename = f"{translation_id}.txt" # Final name
            status_df.loc[index, 'status_extract_path'] = str((corpus_base / final_extract_filename).resolve())
    return status_df


    df['action_needed_download'] = False
    df['action_needed_unzip'] = False
    df['action_needed_licence'] = False
    df["pipeline_needs_settings_update"] = False
    df["pipeline_needs_rename"] = False
    df["pipeline_needs_hash"] = False
    df['action_needed_extract'] = False # New action for internal extraction

    for index, row in df.iterrows():
        # --- Download Check ---
        elif is_date_older_than(row['status_download_date'], max_age_days):
            needs_download = True
        elif pd.isna(row['status_download_path']) or not Path(row['status_download_path']).exists():
             # Check if file exists only if date is recent
             needs_download = True
            # Check if file exists only if date is recent
            needs_download = True

        df.loc[index, 'action_needed_download'] = needs_download

        elif is_date_older_than(row['status_unzip_date'], max_age_days):
            needs_unzip = True
        elif pd.isna(row['status_unzip_path']) or not Path(row['status_unzip_path']).exists():
             # Check if dir exists only if date is recent
             needs_unzip = True
            # Check if dir exists only if date is recent
            needs_unzip = True

        df.loc[index, 'action_needed_unzip'] = needs_unzip

        # No path check needed here, as licence data is in the status file itself

        df.loc[index, 'action_needed_licence'] = needs_licence

        # --- Text Extraction Check ---
        needs_extract = False
        if needs_unzip: # If re-unzipping, must re-extract
            needs_extract = True
        elif force_download: # Force implies re-extract too
            needs_extract = True
        elif is_date_older_than(row['status_extract_date'], max_age_days): # Using the new combined date
            needs_extract = True
        elif pd.isna(row['status_extract_path']) or not Path(row['status_extract_path']).exists():
            # Check if final extracted file exists
            needs_extract = True
        
        df.loc[index, 'action_needed_extract'] = needs_extract

    return df


        download_path = Path(download_path_str)
        if not download_path.exists():
             logging.warning(f"Skipping unzip for {translation_id}: Download path {download_path} not found.")
             df.loc[index, 'status_last_error'] = f"Unzip skipped: Download not found at {download_path}"
             continue
            logging.warning(f"Skipping unzip for {translation_id}: Download path {download_path} not found.")
            df.loc[index, 'status_last_error'] = f"Unzip skipped: Download not found at {download_path}"
            continue

        unzip_base_dir = projects_folder if is_redist else private_projects_folder
        proj_name = translation_id
                # Continue to attempt settings file write; scoring will use fallback or default.

            # Write Settings.xml
             # Unpack all return values, even if old/new dicts aren't used here yet
            # Unpack all return values, even if old/new dicts aren't used here yet
            settings_path, vrs_num, _, _ = write_settings_file(project_dir, lang_code)
            df.loc[index, 'status_inferred_versification'] = vrs_num # Store the inferred versification number
            if settings_path: df.loc[index, 'status_settings_xml_date'] = TODAY_STR

            # Extract Licence Details (only if needed or forced)
            if row['action_needed_licence']:
                 df = get_and_update_licence_details(df, index, project_dir)
                df = get_and_update_licence_details(df, index, project_dir)

            processed_count += 1

        except (shutil.ReadError, FileNotFoundError, OSError, Exception) as e:
        except (shutil.ReadError, FileNotFoundError, OSError) as e: # Catch more specific IO/OS errors
            logging.error(f"Error processing {translation_id} at {project_dir}: {e}")
            df.loc[index, 'status_unzip_path'] = np.nan
            df.loc[index, 'status_unzip_date'] = np.nan
            df.loc[index, 'status_last_error'] = f"Processing error: {e}"
            # Clean up potentially corrupted unzip dir
            if project_dir.exists():
                 try:
                      shutil.rmtree(project_dir)
                 except OSError as rm_e:
                      logging.warning(f"Could not remove failed unzip dir {project_dir}: {rm_e}")
                try:
                    shutil.rmtree(project_dir)
                except OSError as rm_e:
                    logging.warning(f"Could not remove failed unzip dir {project_dir}: {rm_e}")

    logging.info(f"Finished processing. Successfully processed {processed_count}/{count} projects.")
    return df
            old_usfm_path.rename(new_sfm_path)
            renamed_count += 1
        if renamed_count > 0:
             logging.info(f"Renamed {renamed_count} USFM files.")
            logging.info(f"Renamed {renamed_count} USFM files.")
    except Exception as e:
        logging.error(f"Error renaming USFM files in {project_dir}: {e}")


    # Clear previous licence data for this row first
    for col in LICENCE_COLUMNS:
        if col != 'licence_date_read': # Keep date read until success
             df.loc[index, col] = np.nan
        if col != 'licence_date_read': # Keep date read until success or new attempt
            df.loc[index, col] = np.nan

    if not copyright_path.exists():
        logging.warning(f"Unable to find {copyright_path}")
                    entry["licence_Licence_Type"] = cc_match.group(1)
                    entry["licence_Licence_Version"] = cc_match.group(2)
                else: # Handle simpler cases like /by/4.0/
                     cc_match_simple = regex.search(r"/licenses/([a-z\-]+)/?", ref)
                     if cc_match_simple:
                          entry["licence_Licence_Type"] = cc_match_simple.group(1)
                          # Try to find version elsewhere if needed
                    cc_match_simple = regex.search(r"/licenses/([a-z\-]+)/?", ref)
                    if cc_match_simple:
                        entry["licence_Licence_Type"] = cc_match_simple.group(1)
                        # Try to find version elsewhere if needed

        titlelink = soup.find(href=regex.compile(f"https://ebible.org/{entry['licence_ID']}"))
        if titlelink and titlelink.string:
            entry["licence_Vernacular_Title"] = titlelink.string.strip()

        # Extract text, handle potential missing <p> or body
        body_tag = soup.body
        if body_tag and body_tag.p:
             copy_strings = [s.strip() for s in body_tag.p.stripped_strings if s.strip()]
            copy_strings = [s.strip() for s in body_tag.p.stripped_strings if s.strip()]
        elif body_tag:
             copy_strings = [s.strip() for s in body_tag.stripped_strings if s.strip()]
            copy_strings = [s.strip() for s in body_tag.stripped_strings if s.strip()]
        else:
             copy_strings = []
             logging.warning(f"Warning: No body or paragraph tag found in {copyright_path}")


            copy_strings = []
            logging.warning(f"Warning: No body or paragraph tag found in {copyright_path}")


        # Simpler text parsing logic
        is_public_domain = False
        for i, text in enumerate(copy_strings):
                is_public_domain = True
                break # Assume PD overrides other info
            elif "copyright ©" in text.lower():
                 entry["licence_Copyright_Years"] = text # Keep full string for now
                 if i + 1 < len(copy_strings):
                      entry["licence_Copyright_Holder"] = copy_strings[i+1]
                entry["licence_Copyright_Years"] = text # Keep full string for now
                if i + 1 < len(copy_strings):
                    entry["licence_Copyright_Holder"] = copy_strings[i+1]
            elif text.lower().startswith("language:"):
                 if i + 1 < len(copy_strings):
                      entry["licence_Language"] = copy_strings[i+1]
                if i + 1 < len(copy_strings):
                    entry["licence_Language"] = copy_strings[i+1]
            elif text.lower().startswith("dialect"): # Handles "Dialect:" or "Dialect (if applicable):"
                 # Take rest of string after colon, or the next string if current is just "Dialect:"
                 parts = text.split(":", 1)
                 if len(parts) > 1 and parts[1].strip():
                      entry["licence_Dialect"] = parts[1].strip()
                 elif i + 1 < len(copy_strings):
                      entry["licence_Dialect"] = copy_strings[i+1]
                # Take rest of string after colon, or the next string if current is just "Dialect:"
                parts = text.split(":", 1)
                if len(parts) > 1 and parts[1].strip():
                    entry["licence_Dialect"] = parts[1].strip()
                elif i + 1 < len(copy_strings):
                    entry["licence_Dialect"] = copy_strings[i+1]
            elif "translation by" in text.lower():
                 entry["licence_Translation_by"] = text # Keep full string
                entry["licence_Translation_by"] = text # Keep full string

        if is_public_domain:
            entry["licence_Copyright_Holder"] = "Public Domain"

        # --- Data Cleaning/Defaults ---
        if pd.isna(entry.get("licence_Licence_Type")):
             if "Public Domain" == entry.get("licence_Copyright_Holder"):
                  entry["licence_Licence_Type"] = "Public Domain"
             elif entry.get("licence_CC_Licence_Link"):
                  entry["licence_Licence_Type"] = "CC (Unknown Version)" # Indicate CC link exists but type/version parse failed
             else:
                  entry["licence_Licence_Type"] = "Unknown" # Default if no other info
            if "Public Domain" == entry.get("licence_Copyright_Holder"):
                entry["licence_Licence_Type"] = "Public Domain"
            elif entry.get("licence_CC_Licence_Link"):
                entry["licence_Licence_Type"] = "CC (Unknown Version)" # Indicate CC link exists but type/version parse failed
            else:
                entry["licence_Licence_Type"] = "Unknown" # Default if no other info

        # Apply specific known fixes (example)
        if entry["licence_ID"] in ["engwmb", "engwmbb"]:
             entry["licence_Copyright_Holder"] = "Public Domain"
             entry["licence_Licence_Type"] = "Public Domain"
            entry["licence_Copyright_Holder"] = "Public Domain"
            entry["licence_Licence_Type"] = "Public Domain"

        # Update DataFrame row
        for col_suffix, value in entry.items():
             # col_name = f"licence_{col_suffix}" # Prefix already included in entry keys
             if col_suffix in df.columns:
                  df.loc[index, col_suffix] = value
             else:
                  logging.warning(f"Licence key '{col_suffix}' not a column in DataFrame.")
            # col_name = f"licence_{col_suffix}" # Prefix already included in entry keys
            if col_suffix in df.columns:
                df.loc[index, col_suffix] = value
            else:
                logging.warning(f"Licence key '{col_suffix}' not a column in DataFrame.")

        df.loc[index, 'licence_date_read'] = TODAY_STR
        df.loc[index, 'status_last_error'] = np.nan # Clear error on success

    if count == 0:
        logging.info(f"No existing projects require a separate licence check.")
        logging.info("No existing projects require a separate licence check.")
        return df

    logging.info(f"Performing licence check for {count} existing projects...")
    logging.info(f"Finished separate licence check. Updated {checked_count}/{count} projects.")
    return df

def rename_extracted_files(
    status_df: pd.DataFrame,  # The full, unfiltered status DataFrame

def _perform_text_extraction_for_project(
    project_dir_path: Path,
    output_file_path: Path,
    translation_id: str # For logging
) -> Tuple[bool, Optional[str], int]: # Returns: Success_flag, error_message, segment_count
    """
    Extracts text from a Paratext project directory to a specified output file.
    Assumes Settings.xml is already present in project_dir_path.
    """
    logger.info(f"Starting text extraction for {translation_id} from {project_dir_path} to {output_file_path}")
    try:
        # ParatextTextCorpus uses Settings.xml within project_dir_path for versification.
        # include_markers=False is the default for typical parallel corpus text.
        project_corpus = ParatextTextCorpus(str(project_dir_path), include_markers=False)
        
        # ref_corpus is based on the standard vref.txt that machine.corpora knows.
        # This ensures alignment to a common verse reference scheme.
        ref_corpus = create_versification_ref_corpus()

        # The user wants all books, so no filtering logic for include_books/exclude_books is applied here.

        segment_count = 0
        # extract_scripture_corpus is a context manager.
        with output_file_path.open("w", encoding="utf-8", newline="\n") as output_stream, \
             extract_scripture_corpus(project_corpus, ref_corpus) as output_lines_iterator:
            
            for line_content, _ref_vref, _project_vref in output_lines_iterator:
                output_stream.write(line_content + "\n")
                segment_count += 1
        
        if segment_count == 0:
            logging.warning(f"Extraction for {translation_id} resulted in 0 segments. Output file {output_file_path} created but is empty.")

        logger.info(f"Successfully extracted {segment_count} segments for {translation_id} to {output_file_path}")
        return True, None, segment_count

    except Exception as e:
        logger.error(f"Error during text extraction for {translation_id} from {project_dir_path} to {output_file_path}: {e}", exc_info=True)
        if output_file_path.exists():
            try:
                output_file_path.unlink()
                logging.info(f"Removed incomplete output file: {output_file_path}")
            except OSError as unlink_e:
                logging.error(f"Error removing incomplete extraction output {output_file_path}: {unlink_e}")
        return False, str(e), 0


def extract_and_finalize_texts(
    df: pd.DataFrame,
    corpus_folder: Path,
    private_corpus_folder: Path
) -> pd.DataFrame:
    """
    Scans corpus folders for .txt files, renames them from SILNLP output format
    (lang_code-translation_id.txt) to translation_id.txt.
    Updates status_df with rename status and correct paths.
    Reports on files processed and any unexpected files found.
    """
    logging.info("Starting scan of corpus folders to rename extracted files...")

    renamed_files_log = []
    target_exists_skipped_log = []
    already_correct_name_log = []
    unknown_txt_files_log = []
    non_txt_files_log = []
    failed_to_rename_log = []

    files_processed_count = 0

    for folder_to_scan in [corpus_folder, private_corpus_folder]:
        if not folder_to_scan.is_dir():
            logging.warning(f"Corpus folder not found, skipping: {folder_to_scan}")
            continue
        
        logging.info(f"Scanning folder: {folder_to_scan}")
        # Sort for consistent processing order, helpful for debugging/review
        discovered_files = sorted(list(folder_to_scan.glob('*'))) 
        
        for filepath in discovered_files:
            files_processed_count += 1
            filename = filepath.name
            filestem = filepath.stem

            if not filepath.is_file(): # Skip directories
                continue

            if filepath.suffix.lower() != '.txt':
                non_txt_files_log.append(str(filepath.resolve()))
                continue

            # Try to parse as <lang_code>-<translation_id_part>.txt
            parts = filestem.split('-', 1)
            processed_this_file = False

            if len(parts) == 2: # Potential lang-id.txt
                potential_lang_code, potential_id_from_file = parts[0], parts[1]
                
                matching_rows = status_df[
                    (status_df['languageCode'] == potential_lang_code) &
                    (status_df['translationId'] == potential_id_from_file)
                ]

                if not matching_rows.empty:
                    original_df_idx = matching_rows.index[0]
                    target_name = f"{potential_id_from_file}.txt"
                    target_path = filepath.with_name(target_name)

                    if filepath.name != target_name: # Only proceed if current name is different from target
                        if target_path.exists():
                            target_exists_skipped_log.append(
                                f"{filename} (target {target_name} already exists in {folder_to_scan})"
                            )
                            if pd.isna(status_df.loc[original_df_idx, 'status_extract_renamed_date']):
                                status_df.loc[original_df_idx, 'status_extract_renamed_date'] = TODAY_STR
                            status_df.loc[original_df_idx, 'status_extract_path'] = str(target_path.resolve())
                            status_df.loc[original_df_idx, 'status_last_error'] = f"Rename skipped; target {target_name} exists."
                            processed_this_file = True
                        else: # Target does not exist, and current file is different
                            try:
                                filepath.rename(target_path)
                                renamed_files_log.append(f"{filename} -> {target_name} in {folder_to_scan}")
                                status_df.loc[original_df_idx, 'status_extract_renamed_date'] = TODAY_STR
                                status_df.loc[original_df_idx, 'status_extract_path'] = str(target_path.resolve())
                                status_df.loc[original_df_idx, 'status_last_error'] = np.nan
                            except OSError as e:
                                logging.error(f"Error renaming {filepath} to {target_path}: {e}")
                                failed_to_rename_log.append(f"{filename} (in {folder_to_scan}, error: {e})")
                                status_df.loc[original_df_idx, 'status_last_error'] = f"Extract rename failed: {e}"
                            processed_this_file = True
            
            if processed_this_file:
                continue

            # If not processed as lang-id.txt, check if it's a correctly named id.txt or unknown
            matching_rows_direct = status_df[status_df['translationId'] == filestem]

            if not matching_rows_direct.empty:
                original_df_idx_direct = matching_rows_direct.index[0]
                already_correct_name_log.append(f"{filename} (in {folder_to_scan})")
                
                if pd.isna(status_df.loc[original_df_idx_direct, 'status_extract_renamed_date']):
                    status_df.loc[original_df_idx_direct, 'status_extract_renamed_date'] = TODAY_STR
                status_df.loc[original_df_idx_direct, 'status_extract_path'] = str(filepath.resolve())
                if "Extract rename failed" in str(status_df.loc[original_df_idx_direct, 'status_last_error']) or \
                   "Rename skipped" in str(status_df.loc[original_df_idx_direct, 'status_last_error']):
                    status_df.loc[original_df_idx_direct, 'status_last_error'] = np.nan
                processed_this_file = True
            
            if not processed_this_file:
                unknown_txt_files_log.append(f"{filename} (in {folder_to_scan})")

    logging.info(f"--- Corpus File Renaming Summary (Processed {files_processed_count} items) ---")
    if renamed_files_log:
        logging.info(f"Successfully renamed {len(renamed_files_log)} files:")
        for item in renamed_files_log: logging.info(f"  - {item}")
    if target_exists_skipped_log:
        logging.info(f"Skipped renaming for {len(target_exists_skipped_log)} files (target already existed):")
        for item in target_exists_skipped_log: logging.info(f"  - {item}")
    if already_correct_name_log:
        logging.info(f"Found {len(already_correct_name_log)} files already correctly named (status updated if needed):")
        for item in already_correct_name_log: logging.info(f"  - {item}")
    if failed_to_rename_log:
        logging.error(f"Failed to rename {len(failed_to_rename_log)} files due to errors:")
        for item in failed_to_rename_log: logging.error(f"  - {item}")
    
    if not unknown_txt_files_log and not non_txt_files_log:
        logging.info("\nNo unexpected files found in corpus folders.")
    else:
        logging.info("\n--- Unexpected Files Report ---")
        if unknown_txt_files_log:
            logging.warning(f"Found {len(unknown_txt_files_log)} unknown .txt files (not matching known translation IDs or patterns):")
            for item in unknown_txt_files_log: logging.warning(f"  - {item}")
        if non_txt_files_log:
            logging.warning(f"Found {len(non_txt_files_log)} non-.txt files in corpus folders:")
            for item in non_txt_files_log: logging.warning(f"  - {item}")
    
    if not (renamed_files_log or target_exists_skipped_log or already_correct_name_log or failed_to_rename_log or unknown_txt_files_log or non_txt_files_log):
        logging.info("No files required renaming and no unexpected files found in corpus folders.")

    logging.info("Finished renaming extracted files.")
    return status_df
    Performs internal text extraction for projects marked with 'action_needed_extract'.
    """
    translations_to_extract = df[df['action_needed_extract']]
    count = len(translations_to_extract)
    logging.info(f"Attempting to extract text for {count} projects...")

    extracted_count = 0
    for index, row in tqdm(translations_to_extract.iterrows(), total=count, desc="Extracting Text"):
        translation_id = row['translationId']
        project_unzip_path_str = row['status_unzip_path']
        is_redist = row['Redistributable']

        if pd.isna(project_unzip_path_str):
            logging.warning(f"Skipping extraction for {translation_id}: No valid unzip path.")
            df.loc[index, 'status_last_error'] = "Extraction skipped: Missing unzip path"
            continue
        
        project_dir_path = Path(project_unzip_path_str)
        if not project_dir_path.is_dir():
            logging.warning(f"Skipping extraction for {translation_id}: Project directory not found at {project_dir_path}.")
            df.loc[index, 'status_last_error'] = f"Extraction skipped: Project dir not found at {project_dir_path}"
            continue

        target_corpus_folder = corpus_folder if is_redist else private_corpus_folder
        # Output filename is now directly the final name, e.g., KJV.txt
        output_file_path = target_corpus_folder / f"{translation_id}.txt"

        success, error_msg, _segment_count = _perform_text_extraction_for_project(
            project_dir_path, output_file_path, translation_id
        )

        if success:
            df.loc[index, 'status_extract_path'] = str(output_file_path.resolve())
            df.loc[index, 'status_extract_date'] = TODAY_STR
            df.loc[index, 'status_last_error'] = np.nan # Clear error on success
            extracted_count +=1
        else:
            df.loc[index, 'status_extract_path'] = np.nan
            df.loc[index, 'status_extract_date'] = np.nan
            df.loc[index, 'status_last_error'] = f"Extraction failed: {error_msg}"

    logging.info(f"Finished text extraction. Successfully extracted {extracted_count}/{count} projects.")
    return df


# --- Function for --update-settings mode ---
    return status_df


def print_silnlp_commands(logs_folder, log_suffix, private_projects_folder, private_corpus_folder, projects_folder, corpus_folder):
    # Define extract log paths using the same suffix
    public_extract_log: Path = logs_folder / ("extract_public" + log_suffix)
    private_extract_log: Path = logs_folder / ("extract_private" + log_suffix)

    # Log the commands as separate info messages
    logging.info("\n--- Next Step: Bulk Extraction ---")
    logging.info("Use SILNLP's bulk_extract_corpora tool.")
    logging.info("Ensure you have SILNLP installed and configured (e.g., via poetry).")
    logging.info("\nCommand for PRIVATE projects:")
    logging.info(f"\n\npoetry run python -m silnlp.common.bulk_extract_corpora --input \"{private_projects_folder}\" --output \"{private_corpus_folder}\" --error-log \"{private_extract_log}\"")
    logging.info("\nCommand for PUBLIC projects:")
    logging.info(f"\n\npoetry run python -m silnlp.common.bulk_extract_corpora --input \"{projects_folder}\" --output \"{corpus_folder}\" --error-log \"{public_extract_log}\"")
    logging.info("\n---------------------------------")



def main() -> None:
    load_dotenv()
        except Exception as e:
            logging.error(f"Error saving status file {status_path} after settings update: {e}")
        # Print SILNLP commands and exit
        print_silnlp_commands(logs_folder, log_suffix, private_projects_folder, private_corpus_folder, projects_folder, corpus_folder)
        logging.info("Settings update mode finished. No further extraction commands needed as extraction is internal.")
        sys.exit(0)

    # --- Scan existing folders to update status if necessary---
    # Perform licence checks for existing projects if needed
    actions_df = check_and_update_licences(actions_df)

    # --- Perform post-extraction renaming ---
    # Pass the full status_df to scan all corpus files and update status directly
    status_df = rename_extracted_files(status_df, corpus_folder, private_corpus_folder)
    
    # --- Perform Internal Text Extraction ---
    actions_df = extract_and_finalize_texts(
        actions_df,
        corpus_folder,
        private_corpus_folder
    )

    # --- Update Main Status DataFrame and Save ---
    # Use update() which aligns on index (translationId if set, otherwise row number)
    # Ensure index is set correctly if needed, or update based on 'translationId' column
    except Exception as e:
        logging.error(f"Error saving status file {status_path}: {e}")

    # --- Perform post-extraction renaming (Run again after save? Maybe not needed if run before save) ---
    # Renaming is now done before saving the main status_df update.

    # --- Report Missing Extracts ---
    # Re-scan folders to update status one last time before reporting
    status_df = scan_and_update_status(status_df, downloads_folder, projects_folder, private_projects_folder, corpus_folder, private_corpus_folder)
    licence_counts = actions_df['licence_Licence_Type'].value_counts(dropna=False)
    logging.info(f"\n{licence_counts.to_string()}")

    # --- Print SILNLP Commands ---
-    print_silnlp_commands(logs_folder, log_suffix, private_projects_folder, private_corpus_folder, projects_folder, corpus_folder)
+    logging.info("\n--- ebible.py processing finished ---")
 
 
 if __name__ == "__main__":

3. Review README.md
Your README.md will need updates to reflect these changes:

Remove references to the manual SILNLP bulk_extract_corpora.py step. The extraction is now internal.
Update the "What the ebible script does" / Workflow section:
The step "Text Extraction using SILNLP (Manual Intermediary Step)" should be removed/replaced.
The "Finalize Extracted Files and Update Status (Second Major Pass/Run of ebible.py)" is now integrated. The renaming part is gone if the internal extraction writes to the final filename directly.
The description of file naming by SILNLP (<languageCode>-<translationId>.txt) is no longer relevant as an intermediate step visible to the user. The script will directly aim for <translationId>.txt.
File Naming Convention: Clarify that ebible.py now directly produces <translationId>.txt.
ebible_status.csv columns: If you update the README to list columns, reflect the changes (removal of status_extract_renamed_date and status_extract_original_filename, and the new meaning of status_extract_date).
Example README Snippet Change (Conceptual):

Old (Conceptual from your README):

Text Extraction using SILNLP (Manual Intermediary Step): After the project folders are prepared, ebible.py prints out the necessary commands for the user to run the bulk_extract_corpora.py script... ... Finalize Extracted Files and Update Status (Second Major Pass/Run of ebible.py): Once the SILNLP bulk_extract_corpora.py script has completed, ebible.py is run again... This pass focuses on the files generated by SILNLP: a. Rename Extracted Text Files...

New (Conceptual):

Integrated Text Extraction: After preparing the project folders (USFM renaming, Settings.xml creation, license extraction), ebible.py internally extracts the verse text from the USFM files. It uses functionalities similar to those in SILNLP, leveraging the machine library. The extracted text is saved directly into plain text files (one verse per line) in the corpus/ (for public data) or private_corpus/ (for private data) directories. The files are named using the final convention: <translationId>.txt (e.g., KJV.txt, aai.txt). The status of this extraction, including the path to the final text file and the date of extraction, is recorded in ebible_status.csv.

This is a substantial refactoring, but it significantly improves the robustness and usability of your script. Remember to test thoroughly, especially with existing ebible_status.csv files to ensure the column migration/handling works as expected.

Rate this answer: 
Context Sources (16)

David Baines
Can we call the new function extract_project as in paratext.py - I think that will help me to see the connection between the two, though of course they are not identical.


Gemini
Okay, that's a good idea for clarity! Renaming _perform_text_extraction_for_project to extract_project in ebible.py will indeed make the connection to the logic in assets/paratext.py more apparent, even though the implementation details differ.

Here's the diff for c:\ebible_code\ebible_code\ebible.py with that change:

ebible.py
-3
+3
    return df


def _perform_text_extraction_for_project(
def extract_project( # Renamed from _perform_text_extraction_for_project
    project_dir_path: Path,
    output_file_path: Path,
    translation_id: str # For logging
) -> Tuple[bool, Optional[str], int]: # Returns: Success_flag, error_message, segment_count
) -> tuple[bool, Optional[str], int]: # Returns: Success_flag, error_message, segment_count
    """
    Extracts text from a Paratext project directory to a specified output file.
    Assumes Settings.xml is already present in project_dir_path.
        # Output filename is now directly the final name, e.g., KJV.txt
        output_file_path = target_corpus_folder / f"{translation_id}.txt"

        success, error_msg, _segment_count = _perform_text_extraction_for_project(
        success, error_msg, _segment_count = extract_project( # Call the renamed function
            project_dir_path, output_file_path, translation_id
        )


This change simply renames the function and updates its call site within extract_and_finalize_texts. The logic remains the same. This should help you trace the functionality more easily!

Rate this answer: 
Context Sources (17)

David Baines
In the checking licences stage, the output looks like this: root - INFO - Successfully extracted licence info for gfks root - INFO - Extracting licence info for ghn from C:\ebible_data_pipeline\projects\ghn\copr.htm root - INFO - Successfully extracted licence info for ghn root - INFO - Extracting licence info for ghs from C:\ebible_data_pipeline\projects\ghs\copr.htm root - INFO - Successfully extracted licence info for ghs Checking Licences: 32%|████████████████████████████████████▍ | 374/1181 [00:08<00:19, 42.11it/s]root - INFO - Extracting licence info for glk from C:\ebible_data_pipeline\projects\glk\copr.htm root - INFO - Successfully extracted licence info for glk root - INFO - Extracting licence info for glwl from C:\ebible_data_pipeline\projects\glwl\copr.htm root - INFO - Successfully extracted licence info for glwl root - INFO - Extracting licence info for gmve from C:\ebible_data_pipeline\projects\gmve\copr.htm root - INFO - Successfully extracted licence info for gmve root - INFO - Extracting licence info for gmvggm from C:\ebible_data_pipeline\projects\gmvggm\copr.htm Checking Licences: 32%|████████████████████████████████████▋

Can we clean that up a bit so that the detailled information like this is only shown if the log level is set to DEBUG?


Gemini
Okay, I can help you clean up that logging output!

You're right, those detailed "Extracting licence info..." and "Successfully extracted..." messages for each file can make the INFO level logs quite verbose when processing many translations. We can change those specific messages to logging.debug so they only appear when you explicitly set the log level to DEBUG. The overall progress and summary messages will remain at the INFO level.

Here's how we can adjust the get_and_update_licence_details function in your ebible.py script:

ebible.py
-2
+2
def get_and_update_licence_details(df: pd.DataFrame, index, project_dir: Path) -> pd.DataFrame:
    """Extracts licence details from copr.htm and updates the DataFrame row."""
    copyright_path = project_dir / "copr.htm"
    logging.info(f"Extracting licence info for {project_dir.name} from {copyright_path}")
    logging.debug(f"Extracting licence info for {project_dir.name} from {copyright_path}")

    # Clear previous licence data for this row first
    for col in LICENCE_COLUMNS:

        df.loc[index, 'licence_date_read'] = TODAY_STR
        df.loc[index, 'status_last_error'] = np.nan # Clear error on success
        logging.info(f"Successfully extracted licence info for {project_dir.name}")
        logging.debug(f"Successfully extracted licence info for {project_dir.name}")

    except Exception as e:
        logging.error(f"Error parsing licence file {copyright_path}: {e}")

Explanation of Changes:

logging.info(f"Extracting licence info for {project_dir.name} from {copyright_path}") has been changed to: logging.debug(f"Extracting licence info for {project_dir.name} from {copyright_path}")
logging.info(f"Successfully extracted licence info for {project_dir.name}") has been changed to: logging.debug(f"Successfully extracted licence info for {project_dir.name}")
With these changes:

When your script runs with the default INFO logging level, you will no longer see the individual "Extracting..." and "Successfully extracted..." lines for each license file.
You will still see the tqdm progress bar and the summary messages like "Performing licence check for X existing projects..." and "Finished separate licence check...".
If you need to see the detailed license extraction messages (for example, during debugging), you can change the root logger's level to logging.DEBUG (e.g., by modifying root_logger.setLevel(logging.INFO) to root_logger.setLevel(logging.DEBUG) temporarily, or by adding a command-line argument to control the log level).
This should make your console output much cleaner during normal operation while still providing the detailed information when needed!

Rate this answer: 
Context Sources (4)

David Baines
Wow- so much better! Thanks.

Oh - I see an error later on:

root - INFO - Performing licence check for 1181 existing projects... Checking Licences: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1181/1181 [00:19<00:00, 60.25it/s] root - INFO - Finished separate licence check. Updated 1181/1181 projects. root - INFO - Attempting to extract text for 1181 projects... Extracting Text: 0%| | 0/1181 [00:00<?, ?it/s] Traceback (most recent call last): File "C:\ebible_code\ebible_code\ebible.py", line 1218, in <module> main() File "C:\ebible_code\ebible_code\ebible.py", line 1174, in main actions_df = extract_and_finalize_texts( File "C:\ebible_code\ebible_code\ebible.py", line 887, in extract_and_finalize_texts success, error_msg, _segment_count = extract_project( # Call the renamed function File "C:\ebible_code\ebible_code\ebible.py", line 816, in extract_project logger.info(f"Starting text extraction for {translation_id} from {project_dir_path} to {output_file_path}") NameError: name 'logger' is not defined


Gemini
You're welcome! I'm glad the previous change helped clean up the logging.

Ah, that NameError is a classic scope issue! It seems when we created or refactored the extract_project function, we used logger.info and logger.error instead of the standard logging.info and logging.error that are used elsewhere in ebible.py.

The logger variable itself isn't defined within the local scope of the extract_project function, but the logging module (which is imported at the top of the script) is globally available.

Here's the fix to correct those calls:

ebible.py
-3
+3
    Extracts text from a Paratext project directory to a specified output file.
    Assumes Settings.xml is already present in project_dir_path.
    """
    logger.info(f"Starting text extraction for {translation_id} from {project_dir_path} to {output_file_path}")
    logging.info(f"Starting text extraction for {translation_id} from {project_dir_path} to {output_file_path}")
    try:
        # ParatextTextCorpus uses Settings.xml within project_dir_path for versification.
        # include_markers=False is the default for typical parallel corpus text.
        if segment_count == 0:
            logging.warning(f"Extraction for {translation_id} resulted in 0 segments. Output file {output_file_path} created but is empty.")

        logger.info(f"Successfully extracted {segment_count} segments for {translation_id} to {output_file_path}")
        logging.info(f"Successfully extracted {segment_count} segments for {translation_id} to {output_file_path}")
        return True, None, segment_count

    except Exception as e:
        logger.error(f"Error during text extraction for {translation_id} from {project_dir_path} to {output_file_path}: {e}", exc_info=True)
        logging.error(f"Error during text extraction for {translation_id} from {project_dir_path} to {output_file_path}: {e}", exc_info=True)
        if output_file_path.exists():
            try:
                output_file_path.unlink()

By changing logger.info to logging.info and logger.error to logging.error within the extract_project function, it will correctly use the imported logging module, resolving the NameError.

Rate this answer: 
Context Sources (17)

David Baines
For the logging during the extration part of the code, the current output looks like this:

root - INFO - Finished separate licence check. Updated 1181/1181 projects. root - INFO - Attempting to extract text for 1174 projects... Extracting Text: 0%| | 0/1174 [00:00<?, ?it/s]root - INFO - Starting text extraction for abx from C:\ebible_data_pipeline\projects\abx to C:\ebible_data_pipeline\corpus\abx.txt root - INFO - Successfully extracted 41899 segments for abx to C:\ebible_data_pipeline\corpus\abx.txt Extracting Text: 0%|▏ | 1/1174 [00:04<1:35:23, 4.88s/it]root - INFO - Starting text extraction for aby from C:\ebible_data_pipeline\projects\aby to C:\ebible_data_pipeline\corpus\aby.txt root - INFO - Successfully extracted 41899 segments for aby to C:\ebible_data_pipeline\corpus\aby.txt Extracting Text: 0%|▎ | 2/1174 [00:09<1:30:15, 4.62s/it]root - INFO - Starting text extraction for acfNT from C:\ebible_data_pipeline\projects\acfNT to C:\ebible_data_pipeline\corpus\acfNT.txt root - INFO - Successfully extracted 41899 segments for acfNT to C:\ebible_data_pipeline\corpus\acfNT.txt Extracting Text: 0%|▍ | 3/1174 [00:13<1:29:46, 4.60s/it]root - INFO - Starting text extraction for acr-acc from C:\ebible_data_pipeline\projects\acr-acc to C:\ebible_data_pipeline\corpus\acr-acc.txt root - INFO - Successfully extracted 41899 segments for acr-acc to C:\ebible_data_pipeline\corpus\acr-acc.txt Extracting Text: 0%|▌ | 4/1174 [00:18<1:31:37, 4.70s/it]root - INFO - Starting text extraction for acrNNT from C:\ebible_data_pipeline\projects\acrNNT to C:\ebible_data_pipeline\corpus\acrNNT.txt root - INFO - Successfully extracted 41899 segments for acrNNT to C:\ebible_data_pipeline\corpus\acrNNT.txt Extracting Text: 0%|▋

Could we tell tqdm to print the name of the project being extracted, along with the extract output file name, something like "Extracting project <project_folder> to file <extract_output_filepath>" along with the progress bar?


Gemini
Certainly! We can update the tqdm progress bar to display the name of the project and the output file it's currently working on. This will give you more context directly in the progress bar line.

We'll modify the extract_and_finalize_texts function in ebible.py. The tqdm instance will be assigned to a variable, and then we'll use its set_description_str() method to update the descriptive text dynamically within the loop.

Here's the suggested change:

ebible.py
-1
+16
    count = len(translations_to_extract)
    logging.info(f"Attempting to extract text for {count} projects...")

    extracted_count = 0
    for index, row in tqdm(translations_to_extract.iterrows(), total=count, desc="Extracting Text"):
    # Assign tqdm iterator to a variable to allow dynamic description updates
    pbar = tqdm(translations_to_extract.iterrows(), total=count, desc="Initializing extraction...")
    for index, row in pbar:
        translation_id = row['translationId']
        project_unzip_path_str = row['status_unzip_path']
        is_redist = row['Redistributable']

        # Default display names for tqdm, in case of early skip
        project_display_name = translation_id
        output_display_name = f"{translation_id}.txt (target)"

        if pd.isna(project_unzip_path_str):
            # Update description for skipped item
            pbar.set_description_str(f"Skipping {translation_id} (no unzip_path)")
            logging.warning(f"Skipping extraction for {translation_id}: No valid unzip path.")
            df.loc[index, 'status_last_error'] = "Extraction skipped: Missing unzip path"
            continue
        
        project_dir_path = Path(project_unzip_path_str)
        project_display_name = project_dir_path.name # Actual project folder name

        if not project_dir_path.is_dir():
            pbar.set_description_str(f"Skipping {project_display_name} (project dir not found)")
            logging.warning(f"Skipping extraction for {translation_id}: Project directory not found at {project_dir_path}.")
            df.loc[index, 'status_last_error'] = f"Extraction skipped: Project dir not found at {project_dir_path}"
            continue
        target_corpus_folder = corpus_folder if is_redist else private_corpus_folder
        # Output filename is now directly the final name, e.g., KJV.txt
        output_file_path = target_corpus_folder / f"{translation_id}.txt"
        output_display_name = output_file_path.name # Actual output file name

        # Set the description for the current item being actively processed
        pbar.set_description_str(f"Extracting {project_display_name} -> {output_display_name}")
        
        success, error_msg, _segment_count = extract_project( # Call the renamed function
            project_dir_path, output_file_path, translation_id
        )

Explanation of Changes:

pbar = tqdm(translations_to_extract.iterrows(), total=count, desc="Initializing extraction..."):

The tqdm object (which is an iterator) is now assigned to the variable pbar.
The initial description is set to "Initializing extraction...".
for index, row in pbar::

We now iterate over pbar.
project_display_name = ... and output_display_name = ...:

These variables are determined early in the loop to be used in the tqdm description.
pbar.set_description_str(f"Extracting {project_display_name} -> {output_display_name}"):

Before processing each item, the description of the progress bar is updated to show the current project folder name and the target output file name.
If an item is skipped (e.g., due to a missing unzip path or project directory), the description is also updated to reflect that.
With these changes, your tqdm progress bar line will dynamically update to something like: Extracting myproject -> myproject.txt: 10%|█ | 117/1174 [00:10<01:30, 11.2it/s]

The INFO level logs from within the extract_project function (like "Starting text extraction..." and "Successfully extracted...") will still print on their own lines, interspersed with the tqdm updates, which matches the behavior in your example. This should give you a clearer view of what's happening directly on the progress bar.

Rate this answer: 
Context Sources (17)

David Baines
This is printing the output as follows:

root - INFO - Attempting to extract text for 1167 projects... Extracting adz -> adz.txt: 0%| | 0/1167 [00:00<?, ?it/s]root - INFO - Starting text extraction for adz frotraction for adz from C:\ebible_data_pipeline\projects\adz to C:\ebible_data_pipeline\corpus\adz.txt root - INFO - Successfully extracted 41899 segments for adz to C:\ebible_data_pipeline\corpus\adz.txt Extracting aer -> aer.txt: 0%| | 1/1167 [00:03<1:00:29, 3.11s/it]root - INFO - Starting text extraction for aer frotraction for aer from C:\ebible_data_pipeline\projects\aer to C:\ebible_data_pipeline\corpus\aer.txt root - INFO - Successfully extracted 41899 segments for aer to C:\ebible_data_pipeline\corpus\aer.txt Extracting aey -> aey.txt: 0%|▏

So that the progress bar doesn't progress, but is output repeatedly. I think that info logging within root that produces: root - INFO - Successfully extracted 41899 segments for adz to C:\ebible_data_pipeline\corpus\adz.txt Could be changed to be printed only if the logging is set to DEBUG. That will hopefully avoid interruptions to the tqdm output.


Gemini
You've got it! That's a common issue when mixing tqdm with frequent logging. The tqdm progress bar tries to control the current line of the terminal, and any other print or log to standard output/error can push it down, causing it to redraw.

Changing those specific log messages within the extract_project function to logging.debug is the right approach. This way, during normal operation (with logging at INFO level), these detailed messages won't print and interfere with tqdm. If you need to see them for debugging, you can set your root logger level to DEBUG.

Here's the diff to make that change in c:\ebible_code\ebible_code\ebible.py:

ebible.py
-2
+2
    Extracts text from a Paratext project directory to a specified output file.
    Assumes Settings.xml is already present in project_dir_path.
    """
    logging.info(f"Starting text extraction for {translation_id} from {project_dir_path} to {output_file_path}")
    logging.debug(f"Starting text extraction for {translation_id} from {project_dir_path} to {output_file_path}")
    try:
        # ParatextTextCorpus uses Settings.xml within project_dir_path for versification.
        # include_markers=False is the default for typical parallel corpus text.
        if segment_count == 0:
            logging.warning(f"Extraction for {translation_id} resulted in 0 segments. Output file {output_file_path} created but is empty.")

        logging.info(f"Successfully extracted {segment_count} segments for {translation_id} to {output_file_path}")
        logging.debug(f"Successfully extracted {segment_count} segments for {translation_id} to {output_file_path}")
        return True, None, segment_count

    except Exception as e:

With this change, the "Starting text extraction..." and "Successfully extracted..." messages will only appear if you set your script's logging level to DEBUG. The tqdm progress bar should now update smoothly on a single line when the logging level is INFO.

Rate this answer: 
Context Sources (17)
5 context items
